[
    {
        "id": "http://arxiv.org/abs/2508.21816v1",
        "title": "The Demon is in Ambiguity: Revisiting Situation Recognition with Single\n  Positive Multi-Label Learning",
        "abstract": "Context recognition (SR) is a fundamental task in computer vision that aims\nto extract structured semantic summaries from images by identifying key events\nand their associated entities. Specifically, given an input image, the model\nmust first classify the main visual events (verb classification), then identify\nthe participating entities and their semantic roles (semantic role labeling),\nand finally localize these entities in the image (semantic role localization).\nExisting methods treat verb classification as a single-label problem, but we\nshow through a comprehensive analysis that this formulation fails to address\nthe inherent ambiguity in visual event recognition, as multiple verb categories\nmay reasonably describe the same image. This paper makes three key\ncontributions: First, we reveal through empirical analysis that verb\nclassification is inherently a multi-label problem due to the ubiquitous\nsemantic overlap between verb categories. Second, given the impracticality of\nfully annotating large-scale datasets with multiple labels, we propose to\nreformulate verb classification as a single positive multi-label learning\n(SPMLL) problem - a novel perspective in SR research. Third, we design a\ncomprehensive multi-label evaluation benchmark for SR that is carefully\ndesigned to fairly evaluate model performance in a multi-label setting. To\naddress the challenges of SPMLL, we futher develop the Graph Enhanced Verb\nMultilayer Perceptron (GE-VerbMLP), which combines graph neural networks to\ncapture label correlations and adversarial training to optimize decision\nboundaries. Extensive experiments on real-world datasets show that our approach\nachieves more than 3\\% MAP improvement while remaining competitive on\ntraditional top-1 and top-5 accuracy metrics.",
        "published": "2025-08-29T17:51:55Z",
        "updated": "2025-08-29T17:51:55Z",
        "authors": [
            "Yiming Lin",
            "Yuchen Niu",
            "Shang Wang",
            "Kaizhu Huang",
            "Qiufeng Wang",
            "Xiao-Bo Jin"
        ],
        "link": "http://arxiv.org/abs/2508.21816v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21803v1",
        "title": "Automated Clinical Problem Detection from SOAP Notes using a\n  Collaborative Multi-Agent LLM Architecture",
        "abstract": "Accurate interpretation of clinical narratives is critical for patient care,\nbut the complexity of these notes makes automation challenging. While Large\nLanguage Models (LLMs) show promise, single-model approaches can lack the\nrobustness required for high-stakes clinical tasks. We introduce a\ncollaborative multi-agent system (MAS) that models a clinical consultation team\nto address this gap. The system is tasked with identifying clinical problems by\nanalyzing only the Subjective (S) and Objective (O) sections of SOAP notes,\nsimulating the diagnostic reasoning process of synthesizing raw data into an\nassessment. A Manager agent orchestrates a dynamically assigned team of\nspecialist agents who engage in a hierarchical, iterative debate to reach a\nconsensus. We evaluated our MAS against a single-agent baseline on a curated\ndataset of 420 MIMIC-III notes. The dynamic multi-agent configuration\ndemonstrated consistently improved performance in identifying congestive heart\nfailure, acute kidney injury, and sepsis. Qualitative analysis of the agent\ndebates reveals that this structure effectively surfaces and weighs conflicting\nevidence, though it can occasionally be susceptible to groupthink. By modeling\na clinical team's reasoning process, our system offers a promising path toward\nmore accurate, robust, and interpretable clinical decision support tools.",
        "published": "2025-08-29T17:31:24Z",
        "updated": "2025-08-29T17:31:24Z",
        "authors": [
            "Yeawon Lee",
            "Xiaoyang Wang",
            "Christopher C. Yang"
        ],
        "link": "http://arxiv.org/abs/2508.21803v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21800v1",
        "title": "Tree-Guided Diffusion Planner",
        "abstract": "Planning with pretrained diffusion models has emerged as a promising approach\nfor solving test-time guided control problems. However, standard gradient\nguidance typically performs optimally under convex and differentiable reward\nlandscapes, showing substantially reduced effectiveness in real-world scenarios\ninvolving non-convex objectives, non-differentiable constraints, and\nmulti-reward structures. Furthermore, recent supervised planning approaches\nrequire task-specific training or value estimators, which limits test-time\nflexibility and zero-shot generalization. We propose a Tree-guided Diffusion\nPlanner (TDP), a zero-shot test-time planning framework that balances\nexploration and exploitation through structured trajectory generation. We frame\ntest-time planning as a tree search problem using a bi-level sampling process:\n(1) diverse parent trajectories are produced via training-free particle\nguidance to encourage broad exploration, and (2) sub-trajectories are refined\nthrough fast conditional denoising guided by task objectives. TDP addresses the\nlimitations of gradient guidance by exploring diverse trajectory regions and\nharnessing gradient information across this expanded solution space using only\npretrained models and test-time reward signals. We evaluate TDP on three\ndiverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze\nmulti-goal exploration. TDP consistently outperforms state-of-the-art\napproaches on all tasks. The project page can be found at:\ntree-diffusion-planner.github.io.",
        "published": "2025-08-29T17:27:44Z",
        "updated": "2025-08-29T17:27:44Z",
        "authors": [
            "Hyeonseong Jeon",
            "Cheolhong Min",
            "Jaesik Park"
        ],
        "link": "http://arxiv.org/abs/2508.21800v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21797v1",
        "title": "DynaMark: A Reinforcement Learning Framework for Dynamic Watermarking in\n  Industrial Machine Tool Controllers",
        "abstract": "Industry 4.0's highly networked Machine Tool Controllers (MTCs) are prime\ntargets for replay attacks that use outdated sensor data to manipulate\nactuators. Dynamic watermarking can reveal such tampering, but current schemes\nassume linear-Gaussian dynamics and use constant watermark statistics, making\nthem vulnerable to the time-varying, partly proprietary behavior of MTCs. We\nclose this gap with DynaMark, a reinforcement learning framework that models\ndynamic watermarking as a Markov decision process (MDP). It learns an adaptive\npolicy online that dynamically adapts the covariance of a zero-mean Gaussian\nwatermark using available measurements and detector feedback, without needing\nsystem knowledge. DynaMark maximizes a unique reward function balancing control\nperformance, energy consumption, and detection confidence dynamically. We\ndevelop a Bayesian belief updating mechanism for real-time detection confidence\nin linear systems. This approach, independent of specific system assumptions,\nunderpins the MDP for systems with linear dynamics. On a Siemens Sinumerik 828D\ncontroller digital twin, DynaMark achieves a reduction in watermark energy by\n70% while preserving the nominal trajectory, compared to constant variance\nbaselines. It also maintains an average detection delay equivalent to one\nsampling interval. A physical stepper-motor testbed validates these findings,\nrapidly triggering alarms with less control performance decline and exceeding\nexisting benchmarks.",
        "published": "2025-08-29T17:24:00Z",
        "updated": "2025-08-29T17:24:00Z",
        "authors": [
            "Navid Aftabi",
            "Abhishek Hanchate",
            "Satish Bukkapatnam",
            "Dan Li"
        ],
        "link": "http://arxiv.org/abs/2508.21797v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21795v1",
        "title": "TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection\n  Models with a Text Memory Bank",
        "abstract": "Anomaly detection, which aims to identify anomalies deviating from normal\npatterns, is challenging due to the limited amount of normal data available.\nUnlike most existing unified methods that rely on carefully designed image\nfeature extractors and memory banks to capture logical relationships between\nobjects, we introduce a text memory bank to enhance the detection of logical\nanomalies. Specifically, we propose a Three-Memory framework for Unified\nstructural and logical Anomaly Detection (TMUAD). First, we build a class-level\ntext memory bank for logical anomaly detection by the proposed logic-aware text\nextractor, which can capture rich logical descriptions of objects from input\nimages. Second, we construct an object-level image memory bank that preserves\ncomplete object contours by extracting features from segmented objects. Third,\nwe employ visual encoders to extract patch-level image features for\nconstructing a patch-level memory bank for structural anomaly detection. These\nthree complementary memory banks are used to retrieve and compare normal images\nthat are most similar to the query image, compute anomaly scores at multiple\nlevels, and fuse them into a final anomaly score. By unifying structural and\nlogical anomaly detection through collaborative memory banks, TMUAD achieves\nstate-of-the-art performance across seven publicly available datasets involving\nindustrial and medical domains. The model and code are available at\nhttps://github.com/SIA-IDE/TMUAD.",
        "published": "2025-08-29T17:22:13Z",
        "updated": "2025-08-29T17:22:13Z",
        "authors": [
            "Jiawei Liu",
            "Jiahe Hou",
            "Wei Wang",
            "Jinsong Du",
            "Yang Cong",
            "Huijie Fan"
        ],
        "link": "http://arxiv.org/abs/2508.21795v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21793v1",
        "title": "MoE-Health: A Mixture of Experts Framework for Robust Multimodal\n  Healthcare Prediction",
        "abstract": "Healthcare systems generate diverse multimodal data, including Electronic\nHealth Records (EHR), clinical notes, and medical images. Effectively\nleveraging this data for clinical prediction is challenging, particularly as\nreal-world samples often present with varied or incomplete modalities. Existing\napproaches typically require complete modality data or rely on manual selection\nstrategies, limiting their applicability in real-world clinical settings where\ndata availability varies across patients and institutions. To address these\nlimitations, we propose MoE-Health, a novel Mixture of Experts framework\ndesigned for robust multimodal fusion in healthcare prediction. MoE-Health\narchitecture is specifically developed to handle samples with differing\nmodalities and improve performance on critical clinical tasks. By leveraging\nspecialized expert networks and a dynamic gating mechanism, our approach\ndynamically selects and combines relevant experts based on available data\nmodalities, enabling flexible adaptation to varying data availability\nscenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical\nclinical prediction tasks: in-hospital mortality prediction, long length of\nstay, and hospital readmission prediction. Experimental results demonstrate\nthat MoE-Health achieves superior performance compared to existing multimodal\nfusion methods while maintaining robustness across different modality\navailability patterns. The framework effectively integrates multimodal\ninformation, offering improved predictive performance and robustness in\nhandling heterogeneous and incomplete healthcare data, making it particularly\nsuitable for deployment in diverse healthcare environments with heterogeneous\ndata availability.",
        "published": "2025-08-29T17:17:11Z",
        "updated": "2025-08-29T17:17:11Z",
        "authors": [
            "Xiaoyang Wang",
            "Christopher C. Yang"
        ],
        "link": "http://arxiv.org/abs/2508.21793v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21788v1",
        "title": "Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing\n  Fine Web for Problematic Content Search and Retrieval",
        "abstract": "Large language models (LLMs) rely heavily on web-scale datasets like Common\nCrawl, which provides over 80\\% of training data for some modern models.\nHowever, the indiscriminate nature of web crawling raises challenges in data\nquality, safety, and ethics. Despite the critical importance of training data\nquality, prior research on harmful content has been limited to small samples\ndue to computational constraints. This project presents a framework for\nindexing and analyzing LLM training datasets using an ElasticSearch-based\npipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),\nachieving fast query performance--most searches in milliseconds, all under 2\nseconds. Our work demonstrates real-time dataset analysis, offering practical\ntools for safer, more accountable AI systems.",
        "published": "2025-08-29T17:04:20Z",
        "updated": "2025-08-29T17:04:20Z",
        "authors": [
            "Inés Altemir Marinas",
            "Anastasiia Kucherenko",
            "Andrei Kucharavy"
        ],
        "link": "http://arxiv.org/abs/2508.21788v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21787v1",
        "title": "PiCSAR: Probabilistic Confidence Selection And Ranking",
        "abstract": "Best-of-n sampling improves the accuracy of large language models (LLMs) and\nlarge reasoning models (LRMs) by generating multiple candidate solutions and\nselecting the one with the highest reward. The key challenge for reasoning\ntasks is designing a scoring function that can identify correct reasoning\nchains without access to ground-truth answers. We propose Probabilistic\nConfidence Selection And Ranking (PiCSAR): a simple, training-free method that\nscores each candidate generation using the joint log-likelihood of the\nreasoning and final answer. The joint log-likelihood of the reasoning and final\nanswer naturally decomposes into reasoning confidence and answer confidence.\nPiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,\n+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in\n16 out of 20 comparisons. Our analysis reveals that correct reasoning chains\nexhibit significantly higher reasoning and answer confidence, justifying the\neffectiveness of PiCSAR.",
        "published": "2025-08-29T17:03:47Z",
        "updated": "2025-08-29T17:03:47Z",
        "authors": [
            "Joshua Ong Jun Leang",
            "Zheng Zhao",
            "Aryo Pradipta Gema",
            "Sohee Yang",
            "Wai-Chung Kwan",
            "Xuanli He",
            "Wenda Li",
            "Pasquale Minervini",
            "Eleonora Giunchiglia",
            "Shay B. Cohen"
        ],
        "link": "http://arxiv.org/abs/2508.21787v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21777v1",
        "title": "Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but\n  Persistent Need for Expert Oversight",
        "abstract": "Introduction: Large language models (LLM) have shown great potential in\nclinical decision support. GPT-5 is a novel LLM system that has been\nspecifically marketed towards oncology use.\n  Methods: Performance was assessed using two complementary benchmarks: (i) the\nACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300\nmultiple-choice items, and (ii) a curated set of 60 authentic radiation\noncologic vignettes representing diverse disease sites and treatment\nindications. For the vignette evaluation, GPT-5 was instructed to generate\nconcise therapeutic plans. Four board-certified radiation oncologists rated\ncorrectness, comprehensiveness, and hallucinations. Inter-rater reliability was\nquantified using Fleiss' \\k{appa}.\n  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,\noutperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were\nmost pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's\ntreatment recommendations were rated highly for correctness (mean 3.24/4, 95%\nCI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).\nHallucinations were rare with no case reaching majority consensus for their\npresence. Inter-rater agreement was low (Fleiss' \\k{appa} 0.083 for\ncorrectness), reflecting inherent variability in clinical judgment. Errors\nclustered in complex scenarios requiring precise trial knowledge or detailed\nclinical adaptation.\n  Discussion: GPT-5 clearly outperformed prior model variants on the radiation\noncology multiple-choice benchmark. Although GPT-5 exhibited favorable\nperformance in generating real-world radiation oncology treatment\nrecommendations, correctness ratings indicate room for further improvement.\nWhile hallucinations were infrequent, the presence of substantive errors\nunderscores that GPT-5-generated recommendations require rigorous expert\noversight before clinical implementation.",
        "published": "2025-08-29T16:55:25Z",
        "updated": "2025-08-29T16:55:25Z",
        "authors": [
            "Ugur Dinc",
            "Jibak Sarkar",
            "Philipp Schubert",
            "Sabine Semrau",
            "Thomas Weissmann",
            "Andre Karius",
            "Johann Brand",
            "Bernd-Niklas Axer",
            "Ahmed Gomaa",
            "Pluvio Stephan",
            "Ishita Sheth",
            "Sogand Beirami",
            "Annette Schwarz",
            "Udo Gaipl",
            "Benjamin Frey",
            "Christoph Bert",
            "Stefanie Corradini",
            "Rainer Fietkau",
            "Florian Putz"
        ],
        "link": "http://arxiv.org/abs/2508.21777v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21773v1",
        "title": "Unsupervised Video Continual Learning via Non-Parametric Deep Embedded\n  Clustering",
        "abstract": "We propose a realistic scenario for the unsupervised video learning where\nneither task boundaries nor labels are provided when learning a succession of\ntasks. We also provide a non-parametric learning solution for the\nunder-explored problem of unsupervised video continual learning. Videos\nrepresent a complex and rich spatio-temporal media information, widely used in\nmany applications, but which have not been sufficiently explored in\nunsupervised continual learning. Prior studies have only focused on supervised\ncontinual learning, relying on the knowledge of labels and task boundaries,\nwhile having labeled data is costly and not practical. To address this gap, we\nstudy the unsupervised video continual learning (uVCL). uVCL raises more\nchallenges due to the additional computational and memory requirements of\nprocessing videos when compared to images. We introduce a general benchmark\nexperimental protocol for uVCL by considering the learning of unstructured\nvideo data categories during each task. We propose to use the Kernel Density\nEstimation (KDE) of deep embedded video features extracted by unsupervised\nvideo transformer networks as a non-parametric probabilistic representation of\nthe data. We introduce a novelty detection criterion for the incoming new task\ndata, dynamically enabling the expansion of memory clusters, aiming to capture\nnew knowledge when learning a succession of tasks. We leverage the use of\ntransfer learning from the previous tasks as an initial state for the knowledge\ntransfer to the current learning task. We found that the proposed methodology\nsubstantially enhances the performance of the model when successively learning\nmany tasks. We perform in-depth evaluations on three standard video action\nrecognition datasets, including UCF101, HMDB51, and Something-to-Something V2,\nwithout using any labels or class boundaries.",
        "published": "2025-08-29T16:49:03Z",
        "updated": "2025-08-29T16:49:03Z",
        "authors": [
            "Nattapong Kurpukdee",
            "Adrian G. Bors"
        ],
        "link": "http://arxiv.org/abs/2508.21773v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21762v1",
        "title": "Reasoning-Intensive Regression",
        "abstract": "AI researchers and practitioners increasingly apply large language models\n(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing\nsubtle numerical properties from text. Unlike standard language regression\ntasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc\nproblems like rubric-based scoring or domain-specific retrieval, where much\ndeeper analysis of text is required while only limited task-specific training\ndata and computation are available. We cast three realistic problems as RiR\ntasks to establish an initial benchmark, and use that to test our hypothesis\nthat prompting frozen LLMs and finetuning Transformer encoders via gradient\ndescent will both often struggle in RiR. We then propose MENTAT, a simple and\nlightweight method that combines batch-reflective prompt optimization with\nneural ensemble learning. MENTAT achieves up to 65% improvement over both\nbaselines, though substantial room remains for future advances in RiR.",
        "published": "2025-08-29T16:37:42Z",
        "updated": "2025-08-29T16:37:42Z",
        "authors": [
            "Diane Tchuindjo",
            "Omar Khattab"
        ],
        "link": "http://arxiv.org/abs/2508.21762v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21742v1",
        "title": "Orientability of Causal Relations in Time Series using Summary Causal\n  Graphs and Faithful Distributions",
        "abstract": "Understanding causal relations between temporal variables is a central\nchallenge in time series analysis, particularly when the full causal structure\nis unknown. Even when the full causal structure cannot be fully specified,\nexperts often succeed in providing a high-level abstraction of the causal\ngraph, known as a summary causal graph, which captures the main causal\nrelations between different time series while abstracting away micro-level\ndetails. In this work, we present conditions that guarantee the orientability\nof micro-level edges between temporal variables given the background knowledge\nencoded in a summary causal graph and assuming having access to a faithful and\ncausally sufficient distribution with respect to the true unknown graph. Our\nresults provide theoretical guarantees for edge orientation at the micro-level,\neven in the presence of cycles or bidirected edges at the macro-level. These\nfindings offer practical guidance for leveraging SCGs to inform causal\ndiscovery in complex temporal systems and highlight the value of incorporating\nexpert knowledge to improve causal inference from observational time series\ndata.",
        "published": "2025-08-29T16:08:35Z",
        "updated": "2025-08-29T16:08:35Z",
        "authors": [
            "Timothée Loranchet",
            "Charles K. Assaad"
        ],
        "link": "http://arxiv.org/abs/2508.21742v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21739v1",
        "title": "Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL,\n  Rogue Software and Auto-SNL",
        "abstract": "The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline\nexperiments at rates of up to 1~MHz, with detectors producing data throughputs\nexceeding 1 TB/s. Managing such massive data streams presents significant\nchallenges, as transmission and storage infrastructures become prohibitively\nexpensive. Machine learning (ML) offers a promising solution for real-time data\nreduction, but conventional implementations introduce excessive latency, making\nthem unsuitable for high-speed experimental environments. To address these\nchallenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized\nframework designed to deploy real-time ML inference models on\nField-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to\ndynamically update model weights without requiring FPGA resynthesis, enhancing\nflexibility for adaptive learning applications. To further enhance usability\nand accessibility, we introduce Auto-SNL, a Python extension that streamlines\nthe process of converting Python-based neural network models into\nSNL-compatible high-level synthesis code. This paper presents a benchmark\ncomparison against hls4ml, the current state-of-the-art tool, across multiple\nneural network architectures, fixed-point precisions, and synthesis\nconfigurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL\nachieves competitive or superior latency in most tested architectures, while in\nsome cases also offering FPGA resource savings. This adaptation demonstrates\nSNL's versatility, opening new opportunities for researchers and academics in\nfields such as high-energy physics, medical imaging, robotics, and many more.",
        "published": "2025-08-29T16:04:15Z",
        "updated": "2025-08-29T16:04:15Z",
        "authors": [
            "Hamza Ezzaoui Rahali",
            "Abhilasha Dave",
            "Larry Ruckman",
            "Mohammad Mehdi Rahimifar",
            "Audrey C. Therrien",
            "James J. Russel",
            "Ryan T. Herbst"
        ],
        "link": "http://arxiv.org/abs/2508.21739v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21733v1",
        "title": "Developer Insights into Designing AI-Based Computer Perception Tools",
        "abstract": "Artificial intelligence (AI)-based computer perception (CP) technologies use\nmobile sensors to collect behavioral and physiological data for clinical\ndecision-making. These tools can reshape how clinical knowledge is generated\nand interpreted. However, effective integration of these tools into clinical\nworkflows depends on how developers balance clinical utility with user\nacceptability and trustworthiness. Our study presents findings from 20 in-depth\ninterviews with developers of AI-based CP tools. Interviews were transcribed\nand inductive, thematic analysis was performed to identify 4 key design\npriorities: 1) to account for context and ensure explainability for both\npatients and clinicians; 2) align tools with existing clinical workflows; 3)\nappropriately customize to relevant stakeholders for usability and\nacceptability; and 4) push the boundaries of innovation while aligning with\nestablished paradigms. Our findings highlight that developers view themselves\nas not merely technical architects but also ethical stewards, designing tools\nthat are both acceptable by users and epistemically responsible (prioritizing\nobjectivity and pushing clinical knowledge forward). We offer the following\nsuggestions to help achieve this balance: documenting how design choices around\ncustomization are made, defining limits for customization choices,\ntransparently conveying information about outputs, and investing in user\ntraining. Achieving these goals will require interdisciplinary collaboration\nbetween developers, clinicians, and ethicists.",
        "published": "2025-08-29T16:01:02Z",
        "updated": "2025-08-29T16:01:02Z",
        "authors": [
            "Maya Guhan",
            "Meghan E. Hurley",
            "Eric A. Storch",
            "John Herrington",
            "Casey Zampella",
            "Julia Parish-Morris",
            "Gabriel Lázaro-Muñoz",
            "Kristin Kostick-Quenet"
        ],
        "link": "http://arxiv.org/abs/2508.21733v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21732v1",
        "title": "CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD\n  Model Datasets for fine-tuning Large Vision-Language Models",
        "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nimpressive capabilities across various multimodal tasks. They continue,\nhowever, to struggle with trivial scenarios such as reading values from Digital\nMeasurement Devices (DMDs), particularly in real-world conditions involving\nclutter, occlusions, extreme viewpoints, and motion blur; common in\nhead-mounted cameras and Augmented Reality (AR) applications. Motivated by\nthese limitations, this work introduces CAD2DMD-SET, a synthetic data\ngeneration tool designed to support visual question answering (VQA) tasks\ninvolving DMDs. By leveraging 3D CAD models, advanced rendering, and\nhigh-fidelity image composition, our tool produces diverse, VQA-labelled\nsynthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present\nDMDBench, a curated validation set of 1,000 annotated real-world images\ndesigned to evaluate model performance under practical constraints.\nBenchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein\nSimilarity (ANLS) and further fine-tuning LoRA's of these models with\nCAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL\nshowcasing a score increase of 200% without degrading on other tasks. This\ndemonstrates that the CAD2DMD-SET training dataset substantially improves the\nrobustness and performance of LVLMs when operating under the previously stated\nchallenging conditions. The CAD2DMD-SET tool is expected to be released as\nopen-source once the final version of this manuscript is prepared, allowing the\ncommunity to add different measurement devices and generate their own datasets.",
        "published": "2025-08-29T15:57:43Z",
        "updated": "2025-08-29T15:57:43Z",
        "authors": [
            "João Valente",
            "Atabak Dehban",
            "Rodrigo Ventura"
        ],
        "link": "http://arxiv.org/abs/2508.21732v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21730v1",
        "title": "Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman\n  Problem",
        "abstract": "In this paper we present a variational algorithm for the Traveling Salesman\nProblem (TSP) that combines (i) a compact encoding of permutations, which\nreduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy:\nwhere the circuit topology (``Ansatz'') is first optimized on a training\ninstance by Simulated Annealing (SA), then ``frozen'' and re-used on novel\ninstances, limited to a rapid re-optimization of only the circuit parameters.\nThis pipeline eliminates costly structural research in testing, making the\nprocedure immediately implementable on NISQ hardware.\n  On a set of $40$ randomly generated symmetric instances that span $4 - 7$\ncities, the resulting Ansatz achieves an average optimal trip sampling\nprobability of $100\\%$ for 4 city cases, $90\\%$ for 5 city cases and $80\\%$ for\n6 city cases. With 7 cities the success rate drops markedly to an average of\n$\\sim 20\\%$, revealing the onset of scalability limitations of the proposed\nmethod.\n  The results show robust generalization ability for moderate problem sizes and\nindicate how freezing the Ansatz can dramatically reduce time-to-solution\nwithout degrading solution quality. The paper also discusses scalability\nlimitations, the impact of ``warm-start'' initialization of parameters, and\nprospects for extension to more complex problems, such as Vehicle Routing and\nJob-Shop Scheduling.",
        "published": "2025-08-29T15:56:16Z",
        "updated": "2025-08-29T15:56:16Z",
        "authors": [
            "Fabrizio Fagiolo",
            "Nicolo' Vescera"
        ],
        "link": "http://arxiv.org/abs/2508.21730v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21727v1",
        "title": "OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time\n  Optimization",
        "abstract": "Watermarking diffusion-generated images is crucial for copyright protection\nand user tracking. However, current diffusion watermarking methods face\nsignificant limitations: zero-bit watermarking systems lack the capacity for\nlarge-scale user tracking, while multi-bit methods are highly sensitive to\ncertain image transformations or generative attacks, resulting in a lack of\ncomprehensive robustness. In this paper, we propose OptMark, an\noptimization-based approach that embeds a robust multi-bit watermark into the\nintermediate latents of the diffusion denoising process. OptMark strategically\ninserts a structural watermark early to resist generative attacks and a detail\nwatermark late to withstand image transformations, with tailored regularization\nterms to preserve image quality and ensure imperceptibility. To address the\nchallenge of memory consumption growing linearly with the number of denoising\nsteps during optimization, OptMark incorporates adjoint gradient methods,\nreducing memory usage from O(N) to O(1). Experimental results demonstrate that\nOptMark achieves invisible multi-bit watermarking while ensuring robust\nresilience against valuemetric transformations, geometric transformations,\nediting, and regeneration attacks.",
        "published": "2025-08-29T15:50:59Z",
        "updated": "2025-08-29T15:50:59Z",
        "authors": [
            "Jiazheng Xing",
            "Hai Ci",
            "Hongbin Xu",
            "Hangjie Yuan",
            "Yong Liu",
            "Mike Zheng Shou"
        ],
        "link": "http://arxiv.org/abs/2508.21727v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21720v1",
        "title": "PosterForest: Hierarchical Multi-Agent Collaboration for Scientific\n  Poster Generation",
        "abstract": "We present a novel training-free framework, \\textit{PosterForest}, for\nautomated scientific poster generation. Unlike prior approaches, which largely\nneglect the hierarchical structure of scientific documents and the semantic\nintegration of textual and visual elements, our method addresses both\nchallenges directly. We introduce the \\textit{Poster Tree}, a hierarchical\nintermediate representation that jointly encodes document structure and\nvisual-textual relationships at multiple levels. Our framework employs a\nmulti-agent collaboration strategy, where agents specializing in content\nsummarization and layout planning iteratively coordinate and provide mutual\nfeedback. This approach enables the joint optimization of logical consistency,\ncontent fidelity, and visual coherence. Extensive experiments on multiple\nacademic domains show that our method outperforms existing baselines in both\nqualitative and quantitative evaluations. The resulting posters achieve quality\nclosest to expert-designed ground truth and deliver superior information\npreservation, structural clarity, and user preference.",
        "published": "2025-08-29T15:36:06Z",
        "updated": "2025-08-29T15:36:06Z",
        "authors": [
            "Jiho Choi",
            "Seojeong Park",
            "Seongjong Song",
            "Hyunjung Shim"
        ],
        "link": "http://arxiv.org/abs/2508.21720v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21715v1",
        "title": "Entropy-Based Non-Invasive Reliability Monitoring of Convolutional\n  Neural Networks",
        "abstract": "Convolutional Neural Networks (CNNs) have become the foundation of modern\ncomputer vision, achieving unprecedented accuracy across diverse image\nrecognition tasks. While these networks excel on in-distribution data, they\nremain vulnerable to adversarial perturbations imperceptible input\nmodifications that cause misclassification with high confidence. However,\nexisting detection methods either require expensive retraining, modify network\narchitecture, or degrade performance on clean inputs. Here we show that\nadversarial perturbations create immediate, detectable entropy signatures in\nCNN activations that can be monitored without any model modification. Using\nparallel entropy monitoring on VGG-16, we demonstrate that adversarial inputs\nconsistently shift activation entropy by 7% in early convolutional layers,\nenabling 90% detection accuracy with false positives and false negative rates\nbelow 20%. The complete separation between clean and adversarial entropy\ndistributions reveals that CNNs inherently encode distribution shifts in their\nactivation patterns. This work establishes that CNN reliability can be assessed\nthrough activation entropy alone, enabling practical deployment of\nself-diagnostic vision systems that detect adversarial inputs in real-time\nwithout compromising original model performance.",
        "published": "2025-08-29T15:33:45Z",
        "updated": "2025-08-29T15:33:45Z",
        "authors": [
            "Amirhossein Nazeri",
            "Wael Hafez"
        ],
        "link": "http://arxiv.org/abs/2508.21715v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21693v1",
        "title": "Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR",
        "abstract": "Conventional optical character recognition (OCR) techniques segmented each\ncharacter and then recognized. This made them prone to error in character\nsegmentation, and devoid of context to exploit language models. Advances in\nsequence to sequence translation in last decade led to modern techniques first\ndetecting words and then inputting one word at a time to a model to directly\noutput full words as sequence of characters. This allowed better utilization of\nlanguage models and bypass error-prone character segmentation step. We observe\nthat the above transition in style has moved the bottleneck in accuracy to word\nsegmentation. Hence, in this paper, we propose a natural and logical\nprogression from word level OCR to line-level OCR. The proposal allows to\nbypass errors in word detection, and provides larger sentence context for\nbetter utilization of language models. We show that the proposed technique not\nonly improves the accuracy but also efficiency of OCR. Despite our thorough\nliterature survey, we did not find any public dataset to train and benchmark\nsuch shift from word to line-level OCR. Hence, we also contribute a\nmeticulously curated dataset of 251 English page images with line-level\nannotations. Our experimentation revealed a notable end-to-end accuracy\nimprovement of 5.4%, underscoring the potential benefits of transitioning\ntowards line-level OCR, especially for document images. We also report a 4\ntimes improvement in efficiency compared to word-based pipelines. With\ncontinuous improvements in large language models, our methodology also holds\npotential to exploit such advances. Project Website:\nhttps://nishitanand.github.io/line-level-ocr-website",
        "published": "2025-08-29T15:02:11Z",
        "updated": "2025-08-29T15:02:11Z",
        "authors": [
            "Shashank Vempati",
            "Nishit Anand",
            "Gaurav Talebailkar",
            "Arpan Garai",
            "Chetan Arora"
        ],
        "link": "http://arxiv.org/abs/2508.21693v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21666v1",
        "title": "Harnessing IoT and Generative AI for Weather-Adaptive Learning in\n  Climate Resilience Education",
        "abstract": "This paper introduces the Future Atmospheric Conditions Training System\n(FACTS), a novel platform that advances climate resilience education through\nplace-based, adaptive learning experiences. FACTS combines real-time\natmospheric data collected by IoT sensors with curated resources from a\nKnowledge Base to dynamically generate localized learning challenges. Learner\nresponses are analyzed by a Generative AI powered server, which delivers\npersonalized feedback and adaptive support. Results from a user evaluation\nindicate that participants found the system both easy to use and effective for\nbuilding knowledge related to climate resilience. These findings suggest that\nintegrating IoT and Generative AI into atmospherically adaptive learning\ntechnologies holds significant promise for enhancing educational engagement and\nfostering climate awareness.",
        "published": "2025-08-29T14:30:06Z",
        "updated": "2025-08-29T14:30:06Z",
        "authors": [
            "Imran S. A. Khan",
            "Emmanuel G. Blanchard",
            "Sébastien George"
        ],
        "link": "http://arxiv.org/abs/2508.21666v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21648v1",
        "title": "Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing\n  Bias in Medical AI",
        "abstract": "Bias in medical artificial intelligence is conventionally viewed as a defect\nrequiring elimination. However, human reasoning inherently incorporates biases\nshaped by education, culture, and experience, suggesting their presence may be\ninevitable and potentially valuable. We propose MEDLEY (Medical Ensemble\nDiagnostic system with Leveraged diversitY), a conceptual framework that\norchestrates multiple AI models while preserving their diverse outputs rather\nthan collapsing them into a consensus. Unlike traditional approaches that\nsuppress disagreement, MEDLEY documents model-specific biases as potential\nstrengths and treats hallucinations as provisional hypotheses for clinician\nverification. A proof-of-concept demonstrator was developed using over 30 large\nlanguage models, creating a minimum viable product that preserved both\nconsensus and minority views in synthetic cases, making diagnostic uncertainty\nand latent biases transparent for clinical oversight. While not yet a validated\nclinical tool, the demonstration illustrates how structured diversity can\nenhance medical reasoning under clinician supervision. By reframing AI\nimperfection as a resource, MEDLEY offers a paradigm shift that opens new\nregulatory, ethical, and innovation pathways for developing trustworthy medical\nAI systems.",
        "published": "2025-08-29T14:12:03Z",
        "updated": "2025-08-29T14:12:03Z",
        "authors": [
            "Farhad Abtahi",
            "Mehdi Astaraki",
            "Fernando Seoane"
        ],
        "link": "http://arxiv.org/abs/2508.21648v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21637v1",
        "title": "A-MHA*: Anytime Multi-Heuristic A*",
        "abstract": "Designing good heuristic functions for graph search requires adequate domain\nknowledge. It is often easy to design heuristics that perform well and\ncorrelate with the underlying true cost-to-go values in certain parts of the\nsearch space but these may not be admissible throughout the domain thereby\naffecting the optimality guarantees of the search. Bounded suboptimal search\nusing several such partially good but inadmissible heuristics was developed in\nMulti-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible\nheuristics to potentially generate a faster suboptimal solution, the original\nversion does not improve the solution over time. It is a one shot algorithm\nthat requires careful setting of inflation factors to obtain a desired one time\nsolution. In this work, we tackle this issue by extending MHA* to an anytime\nversion that finds a feasible suboptimal solution quickly and continually\nimproves it until time runs out. Our work is inspired from the Anytime\nRepairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*\nconcepts in the MHA* framework preserves the original suboptimal and\ncompleteness guarantees and enhances MHA* to perform in an anytime fashion.\nFurthermore, we report the performance of A-MHA* in 3-D path planning domain\nand sliding tiles puzzle and compare against MHA* and other anytime algorithms.",
        "published": "2025-08-29T14:00:45Z",
        "updated": "2025-08-29T14:00:45Z",
        "authors": [
            "Ramkumar Natarajan",
            "Muhammad Suhail Saleem",
            "William Xiao",
            "Sandip Aine",
            "Howie Choset",
            "Maxim Likhachev"
        ],
        "link": "http://arxiv.org/abs/2508.21637v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21632v1",
        "title": "QZhou-Embedding Technical Report",
        "abstract": "We present QZhou-Embedding, a general-purpose contextual text embedding model\nwith exceptional text representation capabilities. Built upon the\nQwen2.5-7B-Instruct foundation model, we designed a unified multi-task\nframework comprising specialized data transformation and training strategies.\nThe data transformation scheme enables the incorporation of more diverse\ntextual training datasets, while the task-specific training strategies enhance\nmodel learning efficiency. We developed a data synthesis pipeline leveraging\nLLM API, incorporating techniques such as paraphrasing, augmentation, and hard\nnegative example generation to improve the semantic richness and sample\ndifficulty of the training set. Additionally, we employ a two-stage training\nstrategy, comprising initial retrieval-focused pretraining followed by\nfull-task fine-tuning, enabling the embedding model to extend its capabilities\nbased on robust retrieval performance. Our model achieves state-of-the-art\nresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards\n(August 27 2025), and simultaneously achieves state-of-the-art performance on\ntasks including reranking, clustering, etc. Our findings demonstrate that\nhigher-quality, more diverse data is crucial for advancing retrieval model\nperformance, and that leveraging LLMs generative capabilities can further\noptimize data quality for embedding model breakthroughs. Our model weights are\nreleased on HuggingFace under Apache 2.0 license. For reproducibility, we\nprovide evaluation code and instructions on GitHub.",
        "published": "2025-08-29T13:47:22Z",
        "updated": "2025-08-29T13:47:22Z",
        "authors": [
            "Peng Yu",
            "En Xu",
            "Bin Chen",
            "Haibiao Chen",
            "Yinfei Xu"
        ],
        "link": "http://arxiv.org/abs/2508.21632v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21622v1",
        "title": "Integrating Large Language Models with Network Optimization for\n  Interactive and Explainable Supply Chain Planning: A Real-World Case Study",
        "abstract": "This paper presents an integrated framework that combines traditional network\noptimization models with large language models (LLMs) to deliver interactive,\nexplainable, and role-aware decision support for supply chain planning. The\nproposed system bridges the gap between complex operations research outputs and\nbusiness stakeholder understanding by generating natural language summaries,\ncontextual visualizations, and tailored key performance indicators (KPIs). The\ncore optimization model addresses tactical inventory redistribution across a\nnetwork of distribution centers for multi-period and multi-item, using a\nmixed-integer formulation. The technical architecture incorporates AI agents,\nRESTful APIs, and a dynamic user interface to support real-time interaction,\nconfiguration updates, and simulation-based insights. A case study demonstrates\nhow the system improves planning outcomes by preventing stockouts, reducing\ncosts, and maintaining service levels. Future extensions include integrating\nprivate LLMs, transfer learning, reinforcement learning, and Bayesian neural\nnetworks to enhance explainability, adaptability, and real-time\ndecision-making.",
        "published": "2025-08-29T13:34:55Z",
        "updated": "2025-08-29T13:34:55Z",
        "authors": [
            "Saravanan Venkatachalam"
        ],
        "link": "http://arxiv.org/abs/2508.21622v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21618v1",
        "title": "Physics-Informed Spectral Modeling for Hyperspectral Imaging",
        "abstract": "We present PhISM, a physics-informed deep learning architecture that learns\nwithout supervision to explicitly disentangle hyperspectral observations and\nmodel them with continuous basis functions. \\mname outperforms prior methods on\nseveral classification and regression benchmarks, requires limited labeled\ndata, and provides additional insights thanks to interpretable latent\nrepresentation.",
        "published": "2025-08-29T13:32:07Z",
        "updated": "2025-08-29T13:32:07Z",
        "authors": [
            "Zuzanna Gawrysiak",
            "Krzysztof Krawiec"
        ],
        "link": "http://arxiv.org/abs/2508.21618v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21595v1",
        "title": "Scalable Solution Methods for Dec-POMDPs with Deterministic Dynamics",
        "abstract": "Many high-level multi-agent planning problems, including multi-robot\nnavigation and path planning, can be effectively modeled using deterministic\nactions and observations.\n  In this work, we focus on such domains and introduce the class of\nDeterministic Decentralized POMDPs (Det-Dec-POMDPs). This is a subclass of\nDec-POMDPs characterized by deterministic transitions and observations\nconditioned on the state and joint actions.\n  We then propose a practical solver called Iterative Deterministic POMDP\nPlanning (IDPP). This method builds on the classic Joint Equilibrium Search for\nPolicies framework and is specifically optimized to handle large-scale\nDet-Dec-POMDPs that current Dec-POMDP solvers are unable to address\nefficiently.",
        "published": "2025-08-29T12:50:10Z",
        "updated": "2025-08-29T12:50:10Z",
        "authors": [
            "Yang You",
            "Alex Schutz",
            "Zhikun Li",
            "Bruno Lacerda",
            "Robert Skilton",
            "Nick Hawes"
        ],
        "link": "http://arxiv.org/abs/2508.21595v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21589v1",
        "title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning",
        "abstract": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our \\method consistently enhances the quality of\nseed data and boosts LLM's performance with improving accuracy by 7.15% on\naverage while maintaining the original dataset scale. This work establishes a\nnew paradigm for sustainable LLM training through dynamic human-AI co-evolution\nof data and models. Our datasets, models, and code are coming soon.",
        "published": "2025-08-29T12:47:27Z",
        "updated": "2025-08-29T12:47:27Z",
        "authors": [
            "Zinan Tang",
            "Xin Gao",
            "Qizhi Pei",
            "Zhuoshi Pan",
            "Mengzhang Cai",
            "Jiang Wu",
            "Conghui He",
            "Lijun Wu"
        ],
        "link": "http://arxiv.org/abs/2508.21589v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21587v1",
        "title": "A Survey on Current Trends and Recent Advances in Text Anonymization",
        "abstract": "The proliferation of textual data containing sensitive personal information\nacross various domains requires robust anonymization techniques to protect\nprivacy and comply with regulations, while preserving data usability for\ndiverse and crucial downstream tasks. This survey provides a comprehensive\noverview of current trends and recent advances in text anonymization\ntechniques. We begin by discussing foundational approaches, primarily centered\non Named Entity Recognition, before examining the transformative impact of\nLarge Language Models, detailing their dual role as sophisticated anonymizers\nand potent de-anonymization threats. The survey further explores\ndomain-specific challenges and tailored solutions in critical sectors such as\nhealthcare, law, finance, and education. We investigate advanced methodologies\nincorporating formal privacy models and risk-aware frameworks, and address the\nspecialized subfield of authorship anonymization. Additionally, we review\nevaluation frameworks, comprehensive metrics, benchmarks, and practical\ntoolkits for real-world deployment of anonymization solutions. This review\nconsolidates current knowledge, identifies emerging trends and persistent\nchallenges, including the evolving privacy-utility trade-off, the need to\naddress quasi-identifiers, and the implications of LLM capabilities, and aims\nto guide future research directions for both academics and practitioners in\nthis field.",
        "published": "2025-08-29T12:43:06Z",
        "updated": "2025-08-29T12:43:06Z",
        "authors": [
            "Tobias Deußer",
            "Lorenz Sparrenberg",
            "Armin Berger",
            "Max Hahnbück",
            "Christian Bauckhage",
            "Rafet Sifa"
        ],
        "link": "http://arxiv.org/abs/2508.21587v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21566v1",
        "title": "NSPDI-SNN: An efficient lightweight SNN based on nonlinear synaptic\n  pruning and dendritic integration",
        "abstract": "Spiking neural networks (SNNs) are artificial neural networks based on\nsimulated biological neurons and have attracted much attention in recent\nartificial intelligence technology studies. The dendrites in biological neurons\nhave efficient information processing ability and computational power; however,\nthe neurons of SNNs rarely match the complex structure of the dendrites.\nInspired by the nonlinear structure and highly sparse properties of neuronal\ndendrites, in this study, we propose an efficient, lightweight SNN method with\nnonlinear pruning and dendritic integration (NSPDI-SNN). In this method, we\nintroduce nonlinear dendritic integration (NDI) to improve the representation\nof the spatiotemporal information of neurons. We implement heterogeneous state\ntransition ratios of dendritic spines and construct a new and flexible\nnonlinear synaptic pruning (NSP) method to achieve the high sparsity of SNN. We\nconducted systematic experiments on three benchmark datasets (DVS128 Gesture,\nCIFAR10-DVS, and CIFAR10) and extended the evaluation to two complex tasks\n(speech recognition and reinforcement learning-based maze navigation task).\nAcross all tasks, NSPDI-SNN consistently achieved high sparsity with minimal\nperformance degradation. In particular, our method achieved the best\nexperimental results on all three event stream datasets. Further analysis\nshowed that NSPDI significantly improved the efficiency of synaptic information\ntransfer as sparsity increased. In conclusion, our results indicate that the\ncomplex structure and nonlinear computation of neuronal dendrites provide a\npromising approach for developing efficient SNN methods.",
        "published": "2025-08-29T12:22:00Z",
        "updated": "2025-08-29T12:22:00Z",
        "authors": [
            "Wuque Cai",
            "Hongze Sun",
            "Jiayi He",
            "Qianqian Liao",
            "Yunliang Zang",
            "Duo Chen",
            "Dezhong Yao",
            "Daqing Guo"
        ],
        "link": "http://arxiv.org/abs/2508.21566v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21564v1",
        "title": "Revisiting Landmarks: Learning from Previous Plans to Generalize over\n  Problem Instances",
        "abstract": "We propose a new framework for discovering landmarks that automatically\ngeneralize across a domain. These generalized landmarks are learned from a set\nof solved instances and describe intermediate goals for planning problems where\ntraditional landmark extraction algorithms fall short. Our generalized\nlandmarks extend beyond the predicates of a domain by using state functions\nthat are independent of the objects of a specific problem and apply to all\nsimilar objects, thus capturing repetition. Based on these functions, we\nconstruct a directed generalized landmark graph that defines the landmark\nprogression, including loop possibilities for repetitive subplans. We show how\nto use this graph in a heuristic to solve new problem instances of the same\ndomain. Our results show that the generalized landmark graphs learned from a\nfew small instances are also effective for larger instances in the same domain.\nIf a loop that indicates repetition is identified, we see a significant\nimprovement in heuristic performance over the baseline. Generalized landmarks\ncapture domain information that is interpretable and useful to an automated\nplanner. This information can be discovered from a small set of plans for the\nsame domain.",
        "published": "2025-08-29T12:21:44Z",
        "updated": "2025-08-29T12:21:44Z",
        "authors": [
            "Issa Hanou",
            "Sebastijan Dumančić",
            "Mathijs de Weerdt"
        ],
        "link": "http://arxiv.org/abs/2508.21564v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21559v1",
        "title": "Limitations of Physics-Informed Neural Networks: a Study on Smart Grid\n  Surrogation",
        "abstract": "Physics-Informed Neural Networks (PINNs) present a transformative approach\nfor smart grid modeling by integrating physical laws directly into learning\nframeworks, addressing critical challenges of data scarcity and physical\nconsistency in conventional data-driven methods. This paper evaluates PINNs'\ncapabilities as surrogate models for smart grid dynamics, comparing their\nperformance against XGBoost, Random Forest, and Linear Regression across three\nkey experiments: interpolation, cross-validation, and episodic trajectory\nprediction. By training PINNs exclusively through physics-based loss functions\n(enforcing power balance, operational constraints, and grid stability) we\ndemonstrate their superior generalization, outperforming data-driven models in\nerror reduction. Notably, PINNs maintain comparatively lower MAE in dynamic\ngrid operations, reliably capturing state transitions in both random and\nexpert-driven control scenarios, while traditional models exhibit erratic\nperformance. Despite slight degradation in extreme operational regimes, PINNs\nconsistently enforce physical feasibility, proving vital for safety-critical\napplications. Our results contribute to establishing PINNs as a\nparadigm-shifting tool for smart grid surrogation, bridging data-driven\nflexibility with first-principles rigor. This work advances real-time grid\ncontrol and scalable digital twins, emphasizing the necessity of physics-aware\narchitectures in mission-critical energy systems.",
        "published": "2025-08-29T12:15:32Z",
        "updated": "2025-08-29T12:15:32Z",
        "authors": [
            "Julen Cestero",
            "Carmine Delle Femine",
            "Kenji S. Muro",
            "Marco Quartulli",
            "Marcello Restelli"
        ],
        "link": "http://arxiv.org/abs/2508.21559v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21550v1",
        "title": "EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based\n  Pre-Ordering and Human-in-the-Loop Sorting",
        "abstract": "Pairwise comparison is often favored over absolute rating or ordinal\nclassification in subjective or difficult annotation tasks due to its improved\nreliability. However, exhaustive comparisons require a massive number of\nannotations (O(n^2)). Recent work has greatly reduced the annotation burden\n(O(n log n)) by actively sampling pairwise comparisons using a sorting\nalgorithm. We further improve annotation efficiency by (1) roughly pre-ordering\nitems using the Contrastive Language-Image Pre-training (CLIP) model\nhierarchically without training, and (2) replacing easy, obvious human\ncomparisons with automated comparisons. The proposed EZ-Sort first produces a\nCLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores,\nand finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation\nwas conducted using various datasets: face-age estimation (FGNET), historical\nimage chronology (DHCI), and retinal image quality assessment (EyePACS). It\nshowed that EZ-Sort reduced human annotation cost by 90.5% compared to\nexhaustive pairwise comparisons and by 19.8% compared to prior work (when n =\n100), while improving or maintaining inter-rater reliability. These results\ndemonstrate that combining CLIP-based priors with uncertainty-aware sampling\nyields an efficient and scalable solution for pairwise ranking.",
        "published": "2025-08-29T12:06:49Z",
        "updated": "2025-08-29T12:06:49Z",
        "authors": [
            "Yujin Park",
            "Haejun Chung",
            "Ikbeom Jang"
        ],
        "link": "http://arxiv.org/abs/2508.21550v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21547v1",
        "title": "What Data is Really Necessary? A Feasibility Study of Inference Data\n  Minimization for Recommender Systems",
        "abstract": "Data minimization is a legal principle requiring personal data processing to\nbe limited to what is necessary for a specified purpose. Operationalizing this\nprinciple for recommender systems, which rely on extensive personal data,\nremains a significant challenge. This paper conducts a feasibility study on\nminimizing implicit feedback inference data for such systems. We propose a\nnovel problem formulation, analyze various minimization techniques, and\ninvestigate key factors influencing their effectiveness. We demonstrate that\nsubstantial inference data reduction is technically feasible without\nsignificant performance loss. However, its practicality is critically\ndetermined by two factors: the technical setting (e.g., performance targets,\nchoice of model) and user characteristics (e.g., history size, preference\ncomplexity). Thus, while we establish its technical feasibility, we conclude\nthat data minimization remains practically challenging and its dependence on\nthe technical and user context makes a universal standard for data `necessity'\ndifficult to implement.",
        "published": "2025-08-29T12:01:17Z",
        "updated": "2025-08-29T12:01:17Z",
        "authors": [
            "Jens Leysen",
            "Marco Favier",
            "Bart Goethals"
        ],
        "link": "http://arxiv.org/abs/2508.21547v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21542v1",
        "title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion\n  Models",
        "abstract": "Gaussian splatting typically requires dense observations of the scene and can\nfail to reconstruct occluded and unobserved areas. We propose a latent\ndiffusion model to reconstruct a complete 3D scene with Gaussian splats,\nincluding the occluded parts, from only a single image during inference.\nCompleting the unobserved surfaces of a scene is challenging due to the\nambiguity of the plausible surfaces. Conventional methods use a\nregression-based formulation to predict a single \"mode\" for occluded and\nout-of-frustum surfaces, leading to blurriness, implausibility, and failure to\ncapture multiple possible explanations. Thus, they often address this problem\npartially, focusing either on objects isolated from the background,\nreconstructing only visible surfaces, or failing to extrapolate far from the\ninput views. In contrast, we propose a generative formulation to learn a\ndistribution of 3D representations of Gaussian splats conditioned on a single\ninput image. To address the lack of ground-truth training data, we propose a\nVariational AutoReconstructor to learn a latent space only from 2D images in a\nself-supervised manner, over which a diffusion model is trained. Our method\ngenerates faithful reconstructions and diverse samples with the ability to\ncomplete the occluded surfaces for high-quality 360-degree renderings.",
        "published": "2025-08-29T11:55:47Z",
        "updated": "2025-08-29T11:55:47Z",
        "authors": [
            "Ziwei Liao",
            "Mohamed Sayed",
            "Steven L. Waslander",
            "Sara Vicente",
            "Daniyar Turmukhambetov",
            "Michael Firman"
        ],
        "link": "http://arxiv.org/abs/2508.21542v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21540v1",
        "title": "HealthProcessAI: A Technical Framework and Proof-of-Concept for\n  LLM-Enhanced Healthcare Process Mining",
        "abstract": "Process mining has emerged as a powerful analytical technique for\nunderstanding complex healthcare workflows. However, its application faces\nsignificant barriers, including technical complexity, a lack of standardized\napproaches, and limited access to practical training resources. We introduce\nHealthProcessAI, a GenAI framework designed to simplify process mining\napplications in healthcare and epidemiology by providing a comprehensive\nwrapper around existing Python (PM4PY) and R (bupaR) libraries. To address\nunfamiliarity and improve accessibility, the framework integrates multiple\nLarge Language Models (LLMs) for automated process map interpretation and\nreport generation, helping translate technical analyses into outputs that\ndiverse users can readily understand. We validated the framework using sepsis\nprogression data as a proof-of-concept example and compared the outputs of five\nstate-of-the-art LLM models through the OpenRouter platform. To test its\nfunctionality, the framework successfully processed sepsis data across four\nproof-of-concept scenarios, demonstrating robust technical performance and its\ncapability to generate reports through automated LLM analysis. LLM evaluation\nusing five independent LLMs as automated evaluators revealed distinct model\nstrengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency\nscores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By\nintegrating multiple Large Language Models (LLMs) for automated interpretation\nand report generation, the framework addresses widespread unfamiliarity with\nprocess mining outputs, making them more accessible to clinicians, data\nscientists, and researchers. This structured analytics and AI-driven\ninterpretation combination represents a novel methodological advance in\ntranslating complex process mining results into potentially actionable insights\nfor healthcare applications.",
        "published": "2025-08-29T11:53:16Z",
        "updated": "2025-08-29T11:53:16Z",
        "authors": [
            "Eduardo Illueca-Fernandez",
            "Kaile Chen",
            "Fernando Seoane",
            "Farhad Abtahi"
        ],
        "link": "http://arxiv.org/abs/2508.21540v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21521v1",
        "title": "Counterfactual Scenarios for Automated Planning",
        "abstract": "Counterfactual Explanations (CEs) are a powerful technique used to explain\nMachine Learning models by showing how the input to a model should be minimally\nchanged for the model to produce a different output. Similar proposals have\nbeen made in the context of Automated Planning, where CEs have been\ncharacterised in terms of minimal modifications to an existing plan that would\nresult in the satisfaction of a different goal. While such explanations may\nhelp diagnose faults and reason about the characteristics of a plan, they fail\nto capture higher-level properties of the problem being solved. To address this\nlimitation, we propose a novel explanation paradigm that is based on\ncounterfactual scenarios. In particular, given a planning problem $P$ and an\n\\ltlf formula $\\psi$ defining desired properties of a plan, counterfactual\nscenarios identify minimal modifications to $P$ such that it admits plans that\ncomply with $\\psi$. In this paper, we present two qualitative instantiations of\ncounterfactual scenarios based on an explicit quantification over plans that\nmust satisfy $\\psi$. We then characterise the computational complexity of\ngenerating such counterfactual scenarios when different types of changes are\nallowed on $P$. We show that producing counterfactual scenarios is often only\nas expensive as computing a plan for $P$, thus demonstrating the practical\nviability of our proposal and ultimately providing a framework to construct\npractical algorithms in this area.",
        "published": "2025-08-29T11:16:17Z",
        "updated": "2025-08-29T11:16:17Z",
        "authors": [
            "Nicola Gigante",
            "Francesco Leofante",
            "Andrea Micheli"
        ],
        "link": "http://arxiv.org/abs/2508.21521v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21517v1",
        "title": "Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by\n  Phronesis",
        "abstract": "Background: Wisdom is a superordinate construct that embraces perspective\ntaking, reflectiveness, prosocial orientation, reflective empathetic action,\nand intellectual humility. Unlike conventional models of reasoning that are\nrigidly bound by binary thinking, wisdom unfolds in shades of ambiguity,\nrequiring both graded evaluation and self-reflective humility. Current measures\ndepend on self-reports and seldom reflect the humility and uncertainty inherent\nin wise reasoning. A computational framework that takes into account both\nmultidimensionality and confidence has the potential to improve psychological\nscience and allow humane AI. Method: We present a fuzzy inference system with Z\nnumbers, each of the decisions being expressed in terms of a wisdom score\n(restriction) and confidence score (certainty). As part of this study,\nparticipants (N = 100) were exposed to culturally neutral pictorial moral\ndilemma tasks to which they generated think-aloud linguistic responses, which\nwere mapped into five theoretically based components of wisdom. The scores of\neach individual component were combined using a base of 21 rules, with\nmembership functions tuned via Gaussian kernel density estimation. Results: In\na proof of concept study, the system produced dual attribute wisdom\nrepresentations that correlated modestly but significantly with established\nscales while showing negligible relations with unrelated traits, supporting\nconvergent and divergent validity. Contribution: The contribution is to\nformalize wisdom as a multidimensional, uncertainty-conscious construct,\noperationalized in the form of Z-numbers. In addition to progressing\nmeasurement in psychology, it calculates how fuzzy Z numbers can provide AI\nsystems with interpretable, confidence-sensitive reasoning that affords a safe,\nmiddle ground between rigorous computation and human-like judgment.",
        "published": "2025-08-29T11:03:44Z",
        "updated": "2025-08-29T11:03:44Z",
        "authors": [
            "Sweta Kaman",
            "Ankita Sharma",
            "Romi Banerjee"
        ],
        "link": "http://arxiv.org/abs/2508.21517v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21513v1",
        "title": "On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph\n  Ricci Curvature",
        "abstract": "Graph Neural Networks (GNNs) have recently shown promise as solvers for\nBoolean Satisfiability Problems (SATs) by operating on graph representations of\nlogical formulas. However, their performance degrades sharply on harder\ninstances, raising the question of whether this reflects fundamental\narchitectural limitations. In this work, we provide a geometric explanation\nthrough the lens of graph Ricci Curvature (RC), which quantifies local\nconnectivity bottlenecks. We prove that bipartite graphs derived from random\nk-SAT formulas are inherently negatively curved, and that this curvature\ndecreases with instance difficulty. Building on this, we show that GNN-based\nSAT solvers are affected by oversquashing, a phenomenon where long-range\ndependencies become impossible to compress into fixed-length representations.\nWe validate our claims empirically across different SAT benchmarks and confirm\nthat curvature is both a strong indicator of problem complexity and can be used\nto predict performance. Finally, we connect our findings to design principles\nof existing solvers and outline promising directions for future work.",
        "published": "2025-08-29T10:54:19Z",
        "updated": "2025-08-29T10:54:19Z",
        "authors": [
            "Geri Skenderi"
        ],
        "link": "http://arxiv.org/abs/2508.21513v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21496v1",
        "title": "ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long\n  Video Understanding",
        "abstract": "Video multimodal large language models (Video-MLLMs) have achieved remarkable\nprogress in video understanding. However, they remain vulnerable to\nhallucination-producing content inconsistent with or unrelated to video inputs.\nPrevious video hallucination benchmarks primarily focus on short-videos. They\nattribute hallucinations to factors such as strong language priors, missing\nframes, or vision-language biases introduced by the visual encoder. While these\ncauses indeed account for most hallucinations in short videos, they still\noversimplify the cause of hallucinations. Sometimes, models generate incorrect\noutputs but with correct frame-level semantics. We refer to this type of\nhallucination as Semantic Aggregation Hallucination (SAH), which arises during\nthe process of aggregating frame-level semantics into event-level semantic\ngroups. Given that SAH becomes particularly critical in long videos due to\nincreased semantic complexity across multiple events, it is essential to\nseparate and thoroughly investigate the causes of this type of hallucination.\nTo address the above issues, we introduce ELV-Halluc, the first benchmark\ndedicated to long-video hallucination, enabling a systematic investigation of\nSAH. Our experiments confirm the existence of SAH and show that it increases\nwith semantic complexity. Additionally, we find that models are more prone to\nSAH on rapidly changing semantics. Moreover, we discuss potential approaches to\nmitigate SAH. We demonstrate that positional encoding strategy contributes to\nalleviating SAH, and further adopt DPO strategy to enhance the model's ability\nto distinguish semantics within and across events. To support this, we curate a\ndataset of 8K adversarial data pairs and achieve improvements on both\nELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.",
        "published": "2025-08-29T10:25:03Z",
        "updated": "2025-08-29T10:25:03Z",
        "authors": [
            "Hao Lu",
            "Jiahao Wang",
            "Yaolun Zhang",
            "Ruohui Wang",
            "Xuanyu Zheng",
            "Yepeng Tang",
            "Dahua Lin",
            "Lewei Lu"
        ],
        "link": "http://arxiv.org/abs/2508.21496v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21488v1",
        "title": "Priors Matter: Addressing Misspecification in Bayesian Deep Q-Learning",
        "abstract": "Uncertainty quantification in reinforcement learning can greatly improve\nexploration and robustness. Approximate Bayesian approaches have recently been\npopularized to quantify uncertainty in model-free algorithms. However, so far\nthe focus has been on improving the accuracy of the posterior approximation,\ninstead of studying the accuracy of the prior and likelihood assumptions\nunderlying the posterior. In this work, we demonstrate that there is a cold\nposterior effect in Bayesian deep Q-learning, where contrary to theory,\nperformance increases when reducing the temperature of the posterior. To\nidentify and overcome likely causes, we challenge common assumptions made on\nthe likelihood and priors in Bayesian model-free algorithms. We empirically\nstudy prior distributions and show through statistical tests that the common\nGaussian likelihood assumption is frequently violated. We argue that developing\nmore suitable likelihoods and priors should be a key focus in future Bayesian\nreinforcement learning research and we offer simple, implementable solutions\nfor better priors in deep Q-learning that lead to more performant Bayesian\nalgorithms.",
        "published": "2025-08-29T10:12:42Z",
        "updated": "2025-08-29T10:12:42Z",
        "authors": [
            "Pascal R. van der Vaart",
            "Neil Yorke-Smith",
            "Matthijs T. J. Spaan"
        ],
        "link": "http://arxiv.org/abs/2508.21488v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21482v1",
        "title": "HSFN: Hierarchical Selection for Fake News Detection building\n  Heterogeneous Ensemble",
        "abstract": "Psychological biases, such as confirmation bias, make individuals\nparticularly vulnerable to believing and spreading fake news on social media,\nleading to significant consequences in domains such as public health and\npolitics. Machine learning-based fact-checking systems have been widely studied\nto mitigate this problem. Among them, ensemble methods are particularly\neffective in combining multiple classifiers to improve robustness. However,\ntheir performance heavily depends on the diversity of the constituent\nclassifiers-selecting genuinely diverse models remains a key challenge,\nespecially when models tend to learn redundant patterns. In this work, we\npropose a novel automatic classifier selection approach that prioritizes\ndiversity, also extended by performance. The method first computes pairwise\ndiversity between classifiers and applies hierarchical clustering to organize\nthem into groups at different levels of granularity. A HierarchySelect then\nexplores these hierarchical levels to select one pool of classifiers per level,\neach representing a distinct intra-pool diversity. The most diverse pool is\nidentified and selected for ensemble construction from these. The selection\nprocess incorporates an evaluation metric reflecting each classifiers's\nperformance to ensure the ensemble also generalises well. We conduct\nexperiments with 40 heterogeneous classifiers across six datasets from\ndifferent application domains and with varying numbers of classes. Our method\nis compared against the Elbow heuristic and state-of-the-art baselines. Results\nshow that our approach achieves the highest accuracy on two of six datasets.\nThe implementation details are available on the project's repository:\nhttps://github.com/SaraBCoutinho/HSFN .",
        "published": "2025-08-29T10:09:20Z",
        "updated": "2025-08-29T10:09:20Z",
        "authors": [
            "Sara B. Coutinho",
            "Rafael M. O. Cruz",
            "Francimaria R. S. Nascimento",
            "George D. C. Cavalcanti"
        ],
        "link": "http://arxiv.org/abs/2508.21482v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21476v1",
        "title": "Igniting Creative Writing in Small Language Models: LLM-as-a-Judge\n  versus Multi-Agent Refined Rewards",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable creative writing\ncapabilities, yet their substantial computational demands hinder widespread\nuse. Enhancing Small Language Models (SLMs) offers a promising alternative, but\ncurrent methods like Supervised Fine-Tuning (SFT) struggle with novelty, and\nReinforcement Learning from Human Feedback (RLHF) is costly. This paper\nexplores two distinct AI-driven reward strategies within a Reinforcement\nLearning from AI Feedback (RLAIF) framework to ignite the creative writing of a\n7B-parameter SLM, specifically for generating Chinese greetings. The first\nstrategy employs a RM trained on high-quality preference data curated by a\nnovel multi-agent rejection sampling framework designed for creative tasks. The\nsecond, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose\nreward function is optimized via an adversarial training scheme with a\nreflection mechanism, to directly provide reward signals. Comprehensive\nexperiments reveal that while both approaches significantly enhance creative\noutput over baselines, the principle-guided LLM-as-a-Judge demonstrably yields\nsuperior generation quality. Furthermore, it offers notable advantages in\ntraining efficiency and reduced dependency on human-annotated data, presenting\na more scalable and effective path towards creative SLMs. Our automated\nevaluation methods also exhibit strong alignment with human judgments. Our code\nand data are publicly available at\nhttps://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.",
        "published": "2025-08-29T10:00:55Z",
        "updated": "2025-08-29T10:00:55Z",
        "authors": [
            "Xiaolong Wei",
            "Bo Lu",
            "Xingyu Zhang",
            "Zhejun Zhao",
            "Dongdong Shen",
            "Long Xia",
            "Dawei Yin"
        ],
        "link": "http://arxiv.org/abs/2508.21476v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21475v1",
        "title": "MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal\n  Browsing Agents",
        "abstract": "Large multimodal language models (MLLMs) are increasingly deployed as web\nagents, yet many multimodal browsing benchmarks can be solved by shallow, fixed\nworkflows that lean on high-recall image search and nearby text-masking the\ngenuinely multimodal challenges of fine-grained visual reasoning, provenance\nverification, and long-horizon tool use. We introduce MMSearch-Plus, a\nbenchmark of 311 tasks that highly demand multimodal understanding while\npreserving the difficulty profile of strong text-only browsing suites. Each\nitem is constructed to contain multiple weak, localized visual signals that\nmust be extracted, propagated through iterative text-image search, and\ncross-validated under retrieval noise before answering. Our curation procedure,\nSpatial-Temporal Extrapolation, seeds questions whose answers require\nextrapolating from spatial cues (micro-text, part-level appearance, layouts,\nsignage) and temporal traces (broadcast overlays, seasonal context) to\nout-of-image facts such as events, dates, and venues. We provide a\nmodel-agnostic agent framework with browsing tools and evaluate a range of\nclosed and open MLLMs. The strongest agent (o3) attains 15.1% without search\nand 36.0% accuracy with rollout under our framework, while a strong open-source\nmodel (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20\nrounds of search. Beyond answer accuracy, we assess bounding-box production and\ncropped-image search, and conduct an error analysis that surfaces failures in\nsource verification, part-based reasoning, and long-horizon planning.",
        "published": "2025-08-29T09:58:27Z",
        "updated": "2025-08-29T09:58:27Z",
        "authors": [
            "Xijia Tao",
            "Yihua Teng",
            "Xinxing Su",
            "Xinyu Fu",
            "Jihao Wu",
            "Chaofan Tao",
            "Ziru Liu",
            "Haoli Bai",
            "Rui Liu",
            "Lingpeng Kong"
        ],
        "link": "http://arxiv.org/abs/2508.21475v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21468v1",
        "title": "Controllable 3D Molecular Generation for Structure-Based Drug Design\n  Through Bayesian Flow Networks and Gradient Integration",
        "abstract": "Recent advances in Structure-based Drug Design (SBDD) have leveraged\ngenerative models for 3D molecular generation, predominantly evaluating model\nperformance by binding affinity to target proteins. However, practical drug\ndiscovery necessitates high binding affinity along with synthetic feasibility\nand selectivity, critical properties that were largely neglected in previous\nevaluations. To address this gap, we identify fundamental limitations of\nconventional diffusion-based generative models in effectively guiding molecule\ngeneration toward these diverse pharmacological properties. We propose CByG, a\nnovel framework extending Bayesian Flow Network into a gradient-based\nconditional generative model that robustly integrates property-specific\nguidance. Additionally, we introduce a comprehensive evaluation scheme\nincorporating practical benchmarks for binding affinity, synthetic feasibility,\nand selectivity, overcoming the limitations of conventional evaluation methods.\nExtensive experiments demonstrate that our proposed CByG framework\nsignificantly outperforms baseline models across multiple essential evaluation\ncriteria, highlighting its effectiveness and practicality for real-world drug\ndiscovery applications.",
        "published": "2025-08-29T09:49:15Z",
        "updated": "2025-08-29T09:49:15Z",
        "authors": [
            "Seungyeon Choi",
            "Hwanhee Kim",
            "Chihyun Park",
            "Dahyeon Lee",
            "Seungyong Lee",
            "Yoonju Kim",
            "Hyoungjoon Park",
            "Sein Kwon",
            "Youngwan Jo",
            "Sanghyun Park"
        ],
        "link": "http://arxiv.org/abs/2508.21468v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21460v1",
        "title": "Diffusion-based Multi-modal Synergy Interest Network for Click-through\n  Rate Prediction",
        "abstract": "In click-through rate prediction, click-through rate prediction is used to\nmodel users' interests. However, most of the existing CTR prediction methods\nare mainly based on the ID modality. As a result, they are unable to\ncomprehensively model users' multi-modal preferences. Therefore, it is\nnecessary to introduce multi-modal CTR prediction. Although it seems appealing\nto directly apply the existing multi-modal fusion methods to click-through rate\nprediction models, these methods (1) fail to effectively disentangle\ncommonalities and specificities across different modalities; (2) fail to\nconsider the synergistic effects between modalities and model the complex\ninteractions between modalities.\n  To address the above issues, this paper proposes the Diffusion-based\nMulti-modal Synergy Interest Network (Diff-MSIN) framework for click-through\nprediction. This framework introduces three innovative modules: the Multi-modal\nFeature Enhancement (MFE) Module Synergistic Relationship Capture (SRC) Module,\nand the Feature Dynamic Adaptive Fusion (FDAF) Module. The MFE Module and SRC\nModule extract synergistic, common, and special information among different\nmodalities. They effectively enhances the representation of the modalities,\nimproving the overall quality of the fusion. To encourage distinctiveness among\ndifferent features, we design a Knowledge Decoupling method. Additionally, the\nFDAF Module focuses on capturing user preferences and reducing fusion noise. To\nvalidate the effectiveness of the Diff-MSIN framework, we conducted extensive\nexperiments using the Rec-Tmall and three Amazon datasets. The results\ndemonstrate that our approach yields a significant improvement of at least\n1.67% compared to the baseline, highlighting its potential for enhancing\nmulti-modal recommendation systems. Our code is available at the following\nlink: https://github.com/Cxx-0/Diff-MSIN.",
        "published": "2025-08-29T09:46:16Z",
        "updated": "2025-08-29T09:46:16Z",
        "authors": [
            "Xiaoxi Cui",
            "Weihai Lu",
            "Yu Tong",
            "Yiheng Li",
            "Zhejun Zhao"
        ],
        "link": "http://arxiv.org/abs/2508.21460v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21449v1",
        "title": "Learning Lifted Action Models From Traces of Incomplete Actions and\n  States",
        "abstract": "Consider the problem of learning a lifted STRIPS model of the sliding-tile\npuzzle from random state-action traces where the states represent the location\nof the tiles only, and the actions are the labels up, down, left, and right,\nwith no arguments. Two challenges are involved in this problem. First, the\nstates are not full STRIPS states, as some predicates are missing, like the\natoms representing the position of the ``blank''. Second, the actions are not\nfull STRIPS either, as they do not reveal all the objects involved in the\nactions effects and preconditions. Previous approaches have addressed different\nversions of this model learning problem, but most assume that actions in the\ntraces are full STRIPS actions or that the domain predicates are all\nobservable. The new setting considered in this work is more ``realistic'', as\nthe atoms observed convey the state of the world but not full STRIPS states,\nand the actions reveal the arguments needed for selecting the action but not\nthe ones needed for modeling it in STRIPS. For formulating and addressing the\nlearning problem, we introduce a variant of STRIPS, which we call STRIPS+,\nwhere certain STRIPS action arguments can be left implicit in preconditions\nwhich can also involve a limited form of existential quantification. The\nlearning problem becomes the problem of learning STRIPS+ models from STRIPS+\nstate-action traces. For this, the proposed learning algorithm, called SYNTH,\nconstructs a stratified sequence (conjunction) of precondition expressions or\n``queries'' for each action, that denote unique objects in the state and ground\nthe implicit action arguments in STRIPS+. The correctness and completeness of\nSYNTH is established, and its scalability is tested on state-action traces\nobtained from STRIPS+ models derived from existing STRIPS domains.",
        "published": "2025-08-29T09:27:53Z",
        "updated": "2025-08-29T09:27:53Z",
        "authors": [
            "Niklas Jansen",
            "Jonas Gösgens",
            "Hector Geffner"
        ],
        "link": "http://arxiv.org/abs/2508.21449v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21441v1",
        "title": "A General Framework of Epistemic Forgetting and its Instantiation by\n  Ranking Functions",
        "abstract": "Forgetting as a knowledge management operation deliberately ignores parts of\nthe knowledge and beliefs of an agent, for various reasons. Forgetting has many\nfacets, one may want to forget parts of the syntax, a proposition, or a\nconditional. In the literature, two main operators suitable for performing\nforgetting have been proposed and investigated in depth: First, variable\nelimination is a syntactical method that blends out certain atomic variables to\nfocus on the rest of the language. It has been mainly used in the area of logic\nprogramming and answer set programming. Second, contraction in AGM belief\nrevision theory effectively removes propositions from belief sets under logical\ndeduction. Both operations rely mainly on classical logics. In this article, we\ntake an epistemic perspective and study forgetting operations in epistemic\nstates with richer semantic structures, but with clear links to propositional\nlogic. This allows us to investigate what forgetting in the epistemic\nbackground means, thereby lifting well-known and novel forgetting operations to\nthe epistemic level. We present five general types of epistemic forgetting and\ninstantiate them with seven concrete forgetting operations for Spohn's ranking\nfunctions. We take inspiration from postulates of forgetting both from logic\nprogramming and AGM theory to propose a rich landscape of axioms for evaluating\nforgetting operations. Finally, we evaluate all concrete forgetting operations\naccording to all postulates, leading to a novel comprehensive overview\nhighlighting differences and commonalities among the forgetting operators.",
        "published": "2025-08-29T09:08:54Z",
        "updated": "2025-08-29T09:08:54Z",
        "authors": [
            "Christoph Beierle",
            "Alexander Hahn",
            "Diana Howey",
            "Gabriele Kern-Isberner",
            "Kai Sauerwald"
        ],
        "link": "http://arxiv.org/abs/2508.21441v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21435v1",
        "title": "MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation",
        "abstract": "Synthetic medical data offers a scalable solution for training robust models,\nbut significant domain gaps limit its generalizability to real-world clinical\nsettings. This paper addresses the challenge of cross-domain translation\nbetween synthetic and real X-ray images of the head, focusing on bridging\ndiscrepancies in attenuation behavior, noise characteristics, and soft tissue\nrepresentation. We propose MedShift, a unified class-conditional generative\nmodel based on Flow Matching and Schrodinger Bridges, which enables\nhigh-fidelity, unpaired image translation across multiple domains. Unlike prior\napproaches that require domain-specific training or rely on paired data,\nMedShift learns a shared domain-agnostic latent space and supports seamless\ntranslation between any pair of domains seen during training. We introduce\nX-DigiSkull, a new dataset comprising aligned synthetic and real skull X-rays\nunder varying radiation doses, to benchmark domain translation models.\nExperimental results demonstrate that, despite its smaller model size compared\nto diffusion-based approaches, MedShift offers strong performance and remains\nflexible at inference time, as it can be tuned to prioritize either perceptual\nfidelity or structural consistency, making it a scalable and generalizable\nsolution for domain adaptation in medical imaging. The code and dataset are\navailable at https://caetas.github.io/medshift.html",
        "published": "2025-08-29T09:04:11Z",
        "updated": "2025-08-29T09:04:11Z",
        "authors": [
            "Francisco Caetano",
            "Christiaan Viviers",
            "Peter H. H. de With",
            "Fons van der Sommen"
        ],
        "link": "http://arxiv.org/abs/2508.21435v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21433v1",
        "title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM\n  Summarization for Agent Context Management",
        "abstract": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility",
        "published": "2025-08-29T09:02:35Z",
        "updated": "2025-08-29T09:02:35Z",
        "authors": [
            "Tobias Lindenbauer",
            "Igor Slinko",
            "Ludwig Felder",
            "Egor Bogomolov",
            "Yaroslav Zharov"
        ],
        "link": "http://arxiv.org/abs/2508.21433v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21815v1",
        "title": "Achieving Hilbert-Schmidt Independence Under Rényi Differential\n  Privacy for Fair and Private Data Generation",
        "abstract": "As privacy regulations such as the GDPR and HIPAA and responsibility\nframeworks for artificial intelligence such as the AI Act gain traction, the\nethical and responsible use of real-world data faces increasing constraints.\nSynthetic data generation has emerged as a promising solution to risk-aware\ndata sharing and model development, particularly for tabular datasets that are\nfoundational to sensitive domains such as healthcare. To address both privacy\nand fairness concerns in this setting, we propose FLIP (Fair Latent\nIntervention under Privacy guarantees), a transformer-based variational\nautoencoder augmented with latent diffusion to generate heterogeneous tabular\ndata. Unlike the typical setup in fairness-aware data generation, we assume a\ntask-agnostic setup, not reliant on a fixed, defined downstream task, thus\noffering broader applicability. To ensure privacy, FLIP employs R\\'enyi\ndifferential privacy (RDP) constraints during training and addresses fairness\nin the input space with RDP-compatible balanced sampling that accounts for\ngroup-specific noise levels across multiple sampling rates. In the latent\nspace, we promote fairness by aligning neuron activation patterns across\nprotected groups using Centered Kernel Alignment (CKA), a similarity measure\nextending the Hilbert-Schmidt Independence Criterion (HSIC). This alignment\nencourages statistical independence between latent representations and the\nprotected feature. Empirical results demonstrate that FLIP effectively provides\nsignificant fairness improvements for task-agnostic fairness and across diverse\ndownstream tasks under differential privacy constraints.",
        "published": "2025-08-29T17:51:42Z",
        "updated": "2025-08-29T17:51:42Z",
        "authors": [
            "Tobias Hyrup",
            "Emmanouil Panagiotou",
            "Arjun Roy",
            "Arthur Zimek",
            "Eirini Ntoutsi",
            "Peter Schneider-Kamp"
        ],
        "link": "http://arxiv.org/abs/2508.21815v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21810v1",
        "title": "QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large\n  Language Models",
        "abstract": "The growing scale of Large Language Models (LLMs) has necessitated the\ndevelopment of parameter-efficient fine-tuning techniques. Low-Rank Adaptation\n(LoRA) has emerged as a promising approach, reducing the number of trainable\nparameters by applying low-rank updates to pretrained weights. While standard\nLoRA learns both update factors directly, several recent variants first\ninitialize those matrices via an SVD of the pretrained weights -- an operation\nthat can be expensive on large models and yields singular vectors that are not\nalways easy to interpret. In this work, we extract an orthonormal basis from\nthe pretrained weight matrix using QR decomposition with column pivoting, and\nthen express the LoRA update as a linear combination of these basis vectors --\ntraining only the scalar coefficients, which imposes clear structure on\nadaptation and drastically reduces parameter count. Experiments across GLUE\ntasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,\nstandard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular\nvalue decomposition) with as few as 601 parameters -- a reduction of over 1000x\ncompared to full fine-tuning and 77x fewer than typical LoRA setups.",
        "published": "2025-08-29T17:47:27Z",
        "updated": "2025-08-29T17:47:27Z",
        "authors": [
            "Jessica Liang",
            "Anirudh Bharadwaj"
        ],
        "link": "http://arxiv.org/abs/2508.21810v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21804v1",
        "title": "Considerations for Estimating Causal Effects of Informatively Timed\n  Treatments",
        "abstract": "Epidemiological studies are often concerned with estimating causal effects of\na sequence of treatment decisions on survival outcomes. In many settings,\ntreatment decisions do not occur at fixed, pre-specified followup times.\nRather, timing varies across subjects in ways that may be informative of\nsubsequent treatment decisions and potential outcomes. Awareness of the issue\nand its potential solutions is lacking in the literature, which motivate this\nwork. Here, we formalize the issue of informative timing, problems associated\nwith ignoring it, and show how g-methods can be used to analyze sequential\ntreatments that are informatively timed. As we describe, in such settings, the\nwaiting times between successive treatment decisions may be properly viewed as\na time-varying confounders. Using synthetic examples, we illustrate how\ng-methods that do not adjust for these waiting times may be biased and how\nadjustment can be done in scenarios where patients may die or be censored in\nbetween treatments. We draw connections between adjustment and identification\nwith discrete-time versus continuous-time models. Finally, we provide\nimplementation guidance and examples using publicly available software. Our\nconcluding message is that 1) considering timing is important for valid\ninference and 2) correcting for informative timing can be done with g-methods\nthat adjust for waiting times between treatments as time-varying confounders.",
        "published": "2025-08-29T17:32:47Z",
        "updated": "2025-08-29T17:32:47Z",
        "authors": [
            "Arman Oganisian"
        ],
        "link": "http://arxiv.org/abs/2508.21804v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21797v1",
        "title": "DynaMark: A Reinforcement Learning Framework for Dynamic Watermarking in\n  Industrial Machine Tool Controllers",
        "abstract": "Industry 4.0's highly networked Machine Tool Controllers (MTCs) are prime\ntargets for replay attacks that use outdated sensor data to manipulate\nactuators. Dynamic watermarking can reveal such tampering, but current schemes\nassume linear-Gaussian dynamics and use constant watermark statistics, making\nthem vulnerable to the time-varying, partly proprietary behavior of MTCs. We\nclose this gap with DynaMark, a reinforcement learning framework that models\ndynamic watermarking as a Markov decision process (MDP). It learns an adaptive\npolicy online that dynamically adapts the covariance of a zero-mean Gaussian\nwatermark using available measurements and detector feedback, without needing\nsystem knowledge. DynaMark maximizes a unique reward function balancing control\nperformance, energy consumption, and detection confidence dynamically. We\ndevelop a Bayesian belief updating mechanism for real-time detection confidence\nin linear systems. This approach, independent of specific system assumptions,\nunderpins the MDP for systems with linear dynamics. On a Siemens Sinumerik 828D\ncontroller digital twin, DynaMark achieves a reduction in watermark energy by\n70% while preserving the nominal trajectory, compared to constant variance\nbaselines. It also maintains an average detection delay equivalent to one\nsampling interval. A physical stepper-motor testbed validates these findings,\nrapidly triggering alarms with less control performance decline and exceeding\nexisting benchmarks.",
        "published": "2025-08-29T17:24:00Z",
        "updated": "2025-08-29T17:24:00Z",
        "authors": [
            "Navid Aftabi",
            "Abhishek Hanchate",
            "Satish Bukkapatnam",
            "Dan Li"
        ],
        "link": "http://arxiv.org/abs/2508.21797v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21793v1",
        "title": "MoE-Health: A Mixture of Experts Framework for Robust Multimodal\n  Healthcare Prediction",
        "abstract": "Healthcare systems generate diverse multimodal data, including Electronic\nHealth Records (EHR), clinical notes, and medical images. Effectively\nleveraging this data for clinical prediction is challenging, particularly as\nreal-world samples often present with varied or incomplete modalities. Existing\napproaches typically require complete modality data or rely on manual selection\nstrategies, limiting their applicability in real-world clinical settings where\ndata availability varies across patients and institutions. To address these\nlimitations, we propose MoE-Health, a novel Mixture of Experts framework\ndesigned for robust multimodal fusion in healthcare prediction. MoE-Health\narchitecture is specifically developed to handle samples with differing\nmodalities and improve performance on critical clinical tasks. By leveraging\nspecialized expert networks and a dynamic gating mechanism, our approach\ndynamically selects and combines relevant experts based on available data\nmodalities, enabling flexible adaptation to varying data availability\nscenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical\nclinical prediction tasks: in-hospital mortality prediction, long length of\nstay, and hospital readmission prediction. Experimental results demonstrate\nthat MoE-Health achieves superior performance compared to existing multimodal\nfusion methods while maintaining robustness across different modality\navailability patterns. The framework effectively integrates multimodal\ninformation, offering improved predictive performance and robustness in\nhandling heterogeneous and incomplete healthcare data, making it particularly\nsuitable for deployment in diverse healthcare environments with heterogeneous\ndata availability.",
        "published": "2025-08-29T17:17:11Z",
        "updated": "2025-08-29T17:17:11Z",
        "authors": [
            "Xiaoyang Wang",
            "Christopher C. Yang"
        ],
        "link": "http://arxiv.org/abs/2508.21793v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21785v1",
        "title": "Learning Unified Representations from Heterogeneous Data for Robust\n  Heart Rate Modeling",
        "abstract": "Heart rate prediction is vital for personalized health monitoring and\nfitness, while it frequently faces a critical challenge when deploying in\nreal-world: data heterogeneity. We classify it in two key dimensions: source\nheterogeneity from fragmented device markets with varying feature sets, and\nuser heterogeneity reflecting distinct physiological patterns across\nindividuals and activities. Existing methods either discard device-specific\ninformation, or fail to model user-specific differences, limiting their\nreal-world performance. To address this, we propose a framework that learns\nlatent representations agnostic to both heterogeneity, enabling downstream\npredictors to work consistently under heterogeneous data patterns.\nSpecifically, we introduce a random feature dropout strategy to handle source\nheterogeneity, making the model robust to various feature sets. To manage user\nheterogeneity, we employ a time-aware attention module to capture long-term\nphysiological traits and use a contrastive learning objective to build a\ndiscriminative representation space. To reflect the heterogeneous nature of\nreal-world data, we created and publicly released a new benchmark dataset,\nParroTao. Evaluations on both ParroTao and the public FitRec dataset show that\nour model significantly outperforms existing baselines by 17% and 15%,\nrespectively. Furthermore, analysis of the learned representations demonstrates\ntheir strong discriminative power, and one downstream application task confirm\nthe practical value of our model.",
        "published": "2025-08-29T17:03:05Z",
        "updated": "2025-08-29T17:03:05Z",
        "authors": [
            "Peng Yang",
            "Zhengdong Huang",
            "Zicheng Xie",
            "Wentao Tian",
            "Jingyu Liu",
            "Lunhong Dong"
        ],
        "link": "http://arxiv.org/abs/2508.21785v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21777v1",
        "title": "Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but\n  Persistent Need for Expert Oversight",
        "abstract": "Introduction: Large language models (LLM) have shown great potential in\nclinical decision support. GPT-5 is a novel LLM system that has been\nspecifically marketed towards oncology use.\n  Methods: Performance was assessed using two complementary benchmarks: (i) the\nACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300\nmultiple-choice items, and (ii) a curated set of 60 authentic radiation\noncologic vignettes representing diverse disease sites and treatment\nindications. For the vignette evaluation, GPT-5 was instructed to generate\nconcise therapeutic plans. Four board-certified radiation oncologists rated\ncorrectness, comprehensiveness, and hallucinations. Inter-rater reliability was\nquantified using Fleiss' \\k{appa}.\n  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,\noutperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were\nmost pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's\ntreatment recommendations were rated highly for correctness (mean 3.24/4, 95%\nCI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).\nHallucinations were rare with no case reaching majority consensus for their\npresence. Inter-rater agreement was low (Fleiss' \\k{appa} 0.083 for\ncorrectness), reflecting inherent variability in clinical judgment. Errors\nclustered in complex scenarios requiring precise trial knowledge or detailed\nclinical adaptation.\n  Discussion: GPT-5 clearly outperformed prior model variants on the radiation\noncology multiple-choice benchmark. Although GPT-5 exhibited favorable\nperformance in generating real-world radiation oncology treatment\nrecommendations, correctness ratings indicate room for further improvement.\nWhile hallucinations were infrequent, the presence of substantive errors\nunderscores that GPT-5-generated recommendations require rigorous expert\noversight before clinical implementation.",
        "published": "2025-08-29T16:55:25Z",
        "updated": "2025-08-29T16:55:25Z",
        "authors": [
            "Ugur Dinc",
            "Jibak Sarkar",
            "Philipp Schubert",
            "Sabine Semrau",
            "Thomas Weissmann",
            "Andre Karius",
            "Johann Brand",
            "Bernd-Niklas Axer",
            "Ahmed Gomaa",
            "Pluvio Stephan",
            "Ishita Sheth",
            "Sogand Beirami",
            "Annette Schwarz",
            "Udo Gaipl",
            "Benjamin Frey",
            "Christoph Bert",
            "Stefanie Corradini",
            "Rainer Fietkau",
            "Florian Putz"
        ],
        "link": "http://arxiv.org/abs/2508.21777v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21773v1",
        "title": "Unsupervised Video Continual Learning via Non-Parametric Deep Embedded\n  Clustering",
        "abstract": "We propose a realistic scenario for the unsupervised video learning where\nneither task boundaries nor labels are provided when learning a succession of\ntasks. We also provide a non-parametric learning solution for the\nunder-explored problem of unsupervised video continual learning. Videos\nrepresent a complex and rich spatio-temporal media information, widely used in\nmany applications, but which have not been sufficiently explored in\nunsupervised continual learning. Prior studies have only focused on supervised\ncontinual learning, relying on the knowledge of labels and task boundaries,\nwhile having labeled data is costly and not practical. To address this gap, we\nstudy the unsupervised video continual learning (uVCL). uVCL raises more\nchallenges due to the additional computational and memory requirements of\nprocessing videos when compared to images. We introduce a general benchmark\nexperimental protocol for uVCL by considering the learning of unstructured\nvideo data categories during each task. We propose to use the Kernel Density\nEstimation (KDE) of deep embedded video features extracted by unsupervised\nvideo transformer networks as a non-parametric probabilistic representation of\nthe data. We introduce a novelty detection criterion for the incoming new task\ndata, dynamically enabling the expansion of memory clusters, aiming to capture\nnew knowledge when learning a succession of tasks. We leverage the use of\ntransfer learning from the previous tasks as an initial state for the knowledge\ntransfer to the current learning task. We found that the proposed methodology\nsubstantially enhances the performance of the model when successively learning\nmany tasks. We perform in-depth evaluations on three standard video action\nrecognition datasets, including UCF101, HMDB51, and Something-to-Something V2,\nwithout using any labels or class boundaries.",
        "published": "2025-08-29T16:49:03Z",
        "updated": "2025-08-29T16:49:03Z",
        "authors": [
            "Nattapong Kurpukdee",
            "Adrian G. Bors"
        ],
        "link": "http://arxiv.org/abs/2508.21773v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21772v1",
        "title": "UniMLR: Modeling Implicit Class Significance for Multi-Label Ranking",
        "abstract": "Existing multi-label ranking (MLR) frameworks only exploit information\ndeduced from the bipartition of labels into positive and negative sets.\nTherefore, they do not benefit from ranking among positive labels, which is the\nnovel MLR approach we introduce in this paper. We propose UniMLR, a new MLR\nparadigm that models implicit class relevance/significance values as\nprobability distributions using the ranking among positive labels, rather than\ntreating them as equally important. This approach unifies ranking and\nclassification tasks associated with MLR. Additionally, we address the\nchallenges of scarcity and annotation bias in MLR datasets by introducing eight\nsynthetic datasets (Ranked MNISTs) generated with varying\nsignificance-determining factors, providing an enriched and controllable\nexperimental environment. We statistically demonstrate that our method\naccurately learns a representation of the positive rank order, which is\nconsistent with the ground truth and proportional to the underlying\nsignificance values. Finally, we conduct comprehensive empirical experiments on\nboth real-world and synthetic datasets, demonstrating the value of our proposed\nframework.",
        "published": "2025-08-29T16:44:50Z",
        "updated": "2025-08-29T16:44:50Z",
        "authors": [
            "V. Bugra Yesilkaynak",
            "Emine Dari",
            "Alican Mertan",
            "Gozde Unal"
        ],
        "link": "http://arxiv.org/abs/2508.21772v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21769v1",
        "title": "Domain Generalization in-the-Wild: Disentangling Classification from\n  Domain-Aware Representations",
        "abstract": "Evaluating domain generalization (DG) for foundational models like CLIP is\nchallenging, as web-scale pretraining data potentially covers many existing\nbenchmarks. Consequently, current DG evaluation may neither be sufficiently\nchallenging nor adequately test genuinely unseen data scenarios. To better\nassess the performance of CLIP on DG in-the-wild, a scenario where CLIP\nencounters challenging unseen data, we consider two approaches: (1) evaluating\non 33 diverse datasets with quantified out-of-distribution (OOD) scores after\nfine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'\nsome domains as an approximation. We observe that CLIP's performance\ndeteriorates significantly on more OOD datasets. To address this, we present\nCLIP-DCA (Disentangling Classification from enhanced domain Aware\nrepresentations). Our approach is motivated by the observation that while\nstandard domain invariance losses aim to make representations domain-invariant,\nthis can be harmful to foundation models by forcing the discarding of\ndomain-aware representations beneficial for generalization. We instead\nhypothesize that enhancing domain awareness is a prerequisite for effective\ndomain-invariant classification in foundation models. CLIP-DCA identifies and\nenhances domain awareness within CLIP's encoders using a separate domain head\nand synthetically generated diverse domain data. Simultaneously, it encourages\ndomain-invariant classification through disentanglement from the domain\nfeatures. CLIP-DCA shows significant improvements within this challenging\nevaluation compared to existing methods, particularly on datasets that are more\nOOD.",
        "published": "2025-08-29T16:43:08Z",
        "updated": "2025-08-29T16:43:08Z",
        "authors": [
            "Ha Min Son",
            "Zhe Zhao",
            "Shahbaz Rezaei",
            "Xin Liu"
        ],
        "link": "http://arxiv.org/abs/2508.21769v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21739v1",
        "title": "Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL,\n  Rogue Software and Auto-SNL",
        "abstract": "The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline\nexperiments at rates of up to 1~MHz, with detectors producing data throughputs\nexceeding 1 TB/s. Managing such massive data streams presents significant\nchallenges, as transmission and storage infrastructures become prohibitively\nexpensive. Machine learning (ML) offers a promising solution for real-time data\nreduction, but conventional implementations introduce excessive latency, making\nthem unsuitable for high-speed experimental environments. To address these\nchallenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized\nframework designed to deploy real-time ML inference models on\nField-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to\ndynamically update model weights without requiring FPGA resynthesis, enhancing\nflexibility for adaptive learning applications. To further enhance usability\nand accessibility, we introduce Auto-SNL, a Python extension that streamlines\nthe process of converting Python-based neural network models into\nSNL-compatible high-level synthesis code. This paper presents a benchmark\ncomparison against hls4ml, the current state-of-the-art tool, across multiple\nneural network architectures, fixed-point precisions, and synthesis\nconfigurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL\nachieves competitive or superior latency in most tested architectures, while in\nsome cases also offering FPGA resource savings. This adaptation demonstrates\nSNL's versatility, opening new opportunities for researchers and academics in\nfields such as high-energy physics, medical imaging, robotics, and many more.",
        "published": "2025-08-29T16:04:15Z",
        "updated": "2025-08-29T16:04:15Z",
        "authors": [
            "Hamza Ezzaoui Rahali",
            "Abhilasha Dave",
            "Larry Ruckman",
            "Mohammad Mehdi Rahimifar",
            "Audrey C. Therrien",
            "James J. Russel",
            "Ryan T. Herbst"
        ],
        "link": "http://arxiv.org/abs/2508.21739v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21722v1",
        "title": "Inferring Effects of Major Events through Discontinuity Forecasting of\n  Population Anxiety",
        "abstract": "Estimating community-specific mental health effects of local events is vital\nfor public health policy. While forecasting mental health scores alone offers\nlimited insights into the impact of events on community well-being,\nquasi-experimental designs like the Longitudinal Regression Discontinuity\nDesign (LRDD) from econometrics help researchers derive more effects that are\nmore likely to be causal from observational data. LRDDs aim to extrapolate the\nsize of changes in an outcome (e.g. a discontinuity in running scores for\nanxiety) due to a time-specific event. Here, we propose adapting LRDDs beyond\ntraditional forecasting into a statistical learning framework whereby future\ndiscontinuities (i.e. time-specific shifts) and changes in slope (i.e. linear\ntrajectories) are estimated given a location's history of the score, dynamic\ncovariates (other running assessments), and exogenous variables (static\nrepresentations). Applying our framework to predict discontinuities in the\nanxiety of US counties from COVID-19 events, we found the task was difficult\nbut more achievable as the sophistication of models was increased, with the\nbest results coming from integrating exogenous and dynamic covariates. Our\napproach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$\nfor slope) over traditional static community representations. Discontinuity\nforecasting raises new possibilities for estimating the idiosyncratic effects\nof potential future or hypothetical events on specific communities.",
        "published": "2025-08-29T15:38:27Z",
        "updated": "2025-08-29T15:38:27Z",
        "authors": [
            "Siddharth Mangalik",
            "Ojas Deshpande",
            "Adithya V. Ganesan",
            "Sean A. P. Clouston",
            "H. Andrew Schwartz"
        ],
        "link": "http://arxiv.org/abs/2508.21722v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21695v1",
        "title": "Activation Subspaces for Out-of-Distribution Detection",
        "abstract": "To ensure the reliability of deep models in real-world applications,\nout-of-distribution (OOD) detection methods aim to distinguish samples close to\nthe training distribution (in-distribution, ID) from those farther away (OOD).\nIn this work, we propose a novel OOD detection method that utilizes singular\nvalue decomposition of the weight matrix of the classification head to\ndecompose the model's activations into decisive and insignificant components,\nwhich contribute maximally, respectively minimally, to the final classifier\noutput. We find that the subspace of insignificant components more effectively\ndistinguishes ID from OOD data than raw activations in regimes of large\ndistribution shifts (Far-OOD). This occurs because the classification objective\nleaves the insignificant subspace largely unaffected, yielding features that\nare ''untainted'' by the target classification task. Conversely, in regimes of\nsmaller distribution shifts (Near-OOD), we find that activation shaping methods\nprofit from only considering the decisive subspace, as the insignificant\ncomponent can cause interference in the activation space. By combining two\nfindings into a single approach, termed ActSub, we achieve state-of-the-art\nresults in various standard OOD benchmarks.",
        "published": "2025-08-29T15:03:36Z",
        "updated": "2025-08-29T15:03:36Z",
        "authors": [
            "Barış Zöngür",
            "Robin Hesse",
            "Stefan Roth"
        ],
        "link": "http://arxiv.org/abs/2508.21695v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21693v1",
        "title": "Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR",
        "abstract": "Conventional optical character recognition (OCR) techniques segmented each\ncharacter and then recognized. This made them prone to error in character\nsegmentation, and devoid of context to exploit language models. Advances in\nsequence to sequence translation in last decade led to modern techniques first\ndetecting words and then inputting one word at a time to a model to directly\noutput full words as sequence of characters. This allowed better utilization of\nlanguage models and bypass error-prone character segmentation step. We observe\nthat the above transition in style has moved the bottleneck in accuracy to word\nsegmentation. Hence, in this paper, we propose a natural and logical\nprogression from word level OCR to line-level OCR. The proposal allows to\nbypass errors in word detection, and provides larger sentence context for\nbetter utilization of language models. We show that the proposed technique not\nonly improves the accuracy but also efficiency of OCR. Despite our thorough\nliterature survey, we did not find any public dataset to train and benchmark\nsuch shift from word to line-level OCR. Hence, we also contribute a\nmeticulously curated dataset of 251 English page images with line-level\nannotations. Our experimentation revealed a notable end-to-end accuracy\nimprovement of 5.4%, underscoring the potential benefits of transitioning\ntowards line-level OCR, especially for document images. We also report a 4\ntimes improvement in efficiency compared to word-based pipelines. With\ncontinuous improvements in large language models, our methodology also holds\npotential to exploit such advances. Project Website:\nhttps://nishitanand.github.io/line-level-ocr-website",
        "published": "2025-08-29T15:02:11Z",
        "updated": "2025-08-29T15:02:11Z",
        "authors": [
            "Shashank Vempati",
            "Nishit Anand",
            "Gaurav Talebailkar",
            "Arpan Garai",
            "Chetan Arora"
        ],
        "link": "http://arxiv.org/abs/2508.21693v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21672v1",
        "title": "A Soft Inducement Framework for Incentive-Aided Steering of No-Regret\n  Players",
        "abstract": "In this work, we investigate a steering problem in a mediator-augmented\ntwo-player normal-form game, where the mediator aims to guide players toward a\nspecific action profile through information and incentive design. We first\ncharacterize the games for which successful steering is possible. Moreover, we\nestablish that steering players to any desired action profile is not always\nachievable with information design alone, nor when accompanied with sublinear\npayment schemes. Consequently, we derive a lower bound on the constant payments\nrequired per round to achieve this goal. To address these limitations incurred\nwith information design, we introduce an augmented approach that involves a\none-shot information design phase before the start of the repeated game,\ntransforming the prior interaction into a Stackelberg game. Finally, we\ntheoretically demonstrate that this approach improves the convergence rate of\nplayers' action profiles to the target point by a constant factor with high\nprobability, and support it with empirical results.",
        "published": "2025-08-29T14:34:57Z",
        "updated": "2025-08-29T14:34:57Z",
        "authors": [
            "Asrin Efe Yorulmaz",
            "Raj Kiriti Velicheti",
            "Melih Bastopcu",
            "Tamer Başar"
        ],
        "link": "http://arxiv.org/abs/2508.21672v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21666v1",
        "title": "Harnessing IoT and Generative AI for Weather-Adaptive Learning in\n  Climate Resilience Education",
        "abstract": "This paper introduces the Future Atmospheric Conditions Training System\n(FACTS), a novel platform that advances climate resilience education through\nplace-based, adaptive learning experiences. FACTS combines real-time\natmospheric data collected by IoT sensors with curated resources from a\nKnowledge Base to dynamically generate localized learning challenges. Learner\nresponses are analyzed by a Generative AI powered server, which delivers\npersonalized feedback and adaptive support. Results from a user evaluation\nindicate that participants found the system both easy to use and effective for\nbuilding knowledge related to climate resilience. These findings suggest that\nintegrating IoT and Generative AI into atmospherically adaptive learning\ntechnologies holds significant promise for enhancing educational engagement and\nfostering climate awareness.",
        "published": "2025-08-29T14:30:06Z",
        "updated": "2025-08-29T14:30:06Z",
        "authors": [
            "Imran S. A. Khan",
            "Emmanuel G. Blanchard",
            "Sébastien George"
        ],
        "link": "http://arxiv.org/abs/2508.21666v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21664v1",
        "title": "Trajectory learning for ensemble forecasts via the continuous ranked\n  probability score: a Lorenz '96 case study",
        "abstract": "This paper demonstrates the feasibility of trajectory learning for ensemble\nforecasts by employing the continuous ranked probability score (CRPS) as a loss\nfunction. Using the two-scale Lorenz '96 system as a case study, we develop and\ntrain both additive and multiplicative stochastic parametrizations to generate\nensemble predictions. Results indicate that CRPS-based trajectory learning\nproduces parametrizations that are both accurate and sharp. The resulting\nparametrizations are straightforward to calibrate and outperform\nderivative-fitting-based parametrizations in short-term forecasts. This\napproach is particularly promising for data assimilation applications due to\nits accuracy over short lead times.",
        "published": "2025-08-29T14:25:24Z",
        "updated": "2025-08-29T14:25:24Z",
        "authors": [
            "Sagy Ephrati",
            "James Woodfield"
        ],
        "link": "http://arxiv.org/abs/2508.21664v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21663v1",
        "title": "Surface Stability Modeling with Universal Machine Learning Interatomic\n  Potentials: A Comprehensive Cleavage Energy Benchmarking Study",
        "abstract": "Machine learning interatomic potentials (MLIPs) have revolutionized\ncomputational materials science by bridging the gap between quantum mechanical\naccuracy and classical simulation efficiency, enabling unprecedented\nexploration of materials properties across the periodic table. Despite their\nremarkable success in predicting bulk properties, no systematic evaluation has\nassessed how well these universal MLIPs (uMLIPs) can predict cleavage energies,\na critical property governing fracture, catalysis, surface stability, and\ninterfacial phenomena. Here, we present a comprehensive benchmark of 19\nstate-of-the-art uMLIPs for cleavage energy prediction using our previously\nestablished density functional theory (DFT) database of 36,718 slab structures\nspanning elemental, binary, and ternary metallic compounds. We evaluate diverse\narchitectural paradigms, analyzing their performance across chemical\ncompositions, crystal systems, thickness, and surface orientations. Our results\nreveal that training data composition dominates architectural sophistication:\nmodels trained on the Open Materials 2024 (OMat24) dataset, which emphasizes\nnon-equilibrium configurations, achieve mean absolute percentage errors below\n6% and correctly identify the thermodynamically most stable surface\nterminations in 87% of cases, without any explicit surface energy training. In\ncontrast, architecturally identical models trained on equilibrium-only datasets\nshow five-fold higher errors, while models trained on surface-adsorbate data\nfail catastrophically with a 17-fold degradation. Remarkably, simpler\narchitectures trained on appropriate data achieve comparable accuracy to\ncomplex transformers while offering 10-100x computational speedup. These\nfindings show that the community should focus on strategic training data\ngeneration that captures the relevant physical phenomena.",
        "published": "2025-08-29T14:24:47Z",
        "updated": "2025-08-29T14:24:47Z",
        "authors": [
            "Ardavan Mehdizadeh",
            "Peter Schindler"
        ],
        "link": "http://arxiv.org/abs/2508.21663v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21654v1",
        "title": "I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of\n  Model Stealing Attacks",
        "abstract": "Model stealing attacks endanger the confidentiality of machine learning\nmodels offered as a service. Although these models are kept secret, a malicious\nparty can query a model to label data samples and train their own substitute\nmodel, violating intellectual property. While novel attacks in the field are\ncontinually being published, their design and evaluations are not standardised,\nmaking it challenging to compare prior works and assess progress in the field.\nThis paper is the first to address this gap by providing recommendations for\ndesigning and evaluating model stealing attacks. To this end, we study the\nlargest group of attacks that rely on training a substitute model -- those\nattacking image classification models. We propose the first comprehensive\nthreat model and develop a framework for attack comparison. Further, we analyse\nattack setups from related works to understand which tasks and models have been\nstudied the most. Based on our findings, we present best practices for attack\ndevelopment before, during, and beyond experiments and derive an extensive list\nof open research questions regarding the evaluation of model stealing attacks.\nOur findings and recommendations also transfer to other problem domains, hence\nestablishing the first generic evaluation methodology for model stealing\nattacks.",
        "published": "2025-08-29T14:16:19Z",
        "updated": "2025-08-29T14:16:19Z",
        "authors": [
            "Daryna Oliynyk",
            "Rudolf Mayer",
            "Kathrin Grosse",
            "Andreas Rauber"
        ],
        "link": "http://arxiv.org/abs/2508.21654v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21652v1",
        "title": "Machine Intelligence on the Edge: Interpretable Cardiac Pattern\n  Localisation Using Reinforcement Learning",
        "abstract": "Matched filters are widely used to localise signal patterns due to their high\nefficiency and interpretability. However, their effectiveness deteriorates for\nlow signal-to-noise ratio (SNR) signals, such as those recorded on edge\ndevices, where prominent noise patterns can closely resemble the target within\nthe limited length of the filter. One example is the ear-electrocardiogram\n(ear-ECG), where the cardiac signal is attenuated and heavily corrupted by\nartefacts. To address this, we propose the Sequential Matched Filter (SMF), a\nparadigm that replaces the conventional single matched filter with a sequence\nof filters designed by a Reinforcement Learning agent. By formulating filter\ndesign as a sequential decision-making process, SMF adaptively design\nsignal-specific filter sequences that remain fully interpretable by revealing\nkey patterns driving the decision-making. The proposed SMF framework has strong\npotential for reliable and interpretable clinical decision support, as\ndemonstrated by its state-of-the-art R-peak detection and physiological state\nclassification performance on two challenging real-world ECG datasets. The\nproposed formulation can also be extended to a broad range of applications that\nrequire accurate pattern localisation from noise-corrupted signals.",
        "published": "2025-08-29T14:15:35Z",
        "updated": "2025-08-29T14:15:35Z",
        "authors": [
            "Haozhe Tian",
            "Qiyu Rao",
            "Nina Moutonnet",
            "Pietro Ferraro",
            "Danilo Mandic"
        ],
        "link": "http://arxiv.org/abs/2508.21652v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21650v1",
        "title": "Predicting Social Media Engagement from Emotional and Temporal Features",
        "abstract": "We present a machine learning approach for predicting social media engagement\n(comments and likes) from emotional and temporal features. The dataset contains\n600 songs with annotations for valence, arousal, and related sentiment metrics.\nA multi target regression model based on HistGradientBoostingRegressor is\ntrained on log transformed engagement ratios to address skewed targets.\nPerformance is evaluated with both a custom order of magnitude accuracy and\nstandard regression metrics, including the coefficient of determination (R^2).\nResults show that emotional and temporal metadata, together with existing view\ncounts, predict future engagement effectively. The model attains R^2 = 0.98 for\nlikes but only R^2 = 0.41 for comments. This gap indicates that likes are\nlargely driven by readily captured affective and exposure signals, whereas\ncomments depend on additional factors not represented in the current feature\nset.",
        "published": "2025-08-29T14:14:17Z",
        "updated": "2025-08-29T14:14:17Z",
        "authors": [
            "Yunwoo Kim",
            "Junhyuk Hwang"
        ],
        "link": "http://arxiv.org/abs/2508.21650v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21620v1",
        "title": "Introduction to the Analysis of Probabilistic Decision-Making Algorithms",
        "abstract": "Decision theories offer principled methods for making choices under various\ntypes of uncertainty. Algorithms that implement these theories have been\nsuccessfully applied to a wide range of real-world problems, including\nmaterials and drug discovery. Indeed, they are desirable since they can\nadaptively gather information to make better decisions in the future, resulting\nin data-efficient workflows. In scientific discovery, where experiments are\ncostly, these algorithms can thus significantly reduce the cost of\nexperimentation. Theoretical analyses of these algorithms are crucial for\nunderstanding their behavior and providing valuable insights for developing\nnext-generation algorithms. However, theoretical analyses in the literature are\noften inaccessible to non-experts. This monograph aims to provide an\naccessible, self-contained introduction to the theoretical analysis of commonly\nused probabilistic decision-making algorithms, including bandit algorithms,\nBayesian optimization, and tree search algorithms. Only basic knowledge of\nprobability theory and statistics, along with some elementary knowledge about\nGaussian processes, is assumed.",
        "published": "2025-08-29T13:33:23Z",
        "updated": "2025-08-29T13:33:23Z",
        "authors": [
            "Agustinus Kristiadi"
        ],
        "link": "http://arxiv.org/abs/2508.21620v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21618v1",
        "title": "Physics-Informed Spectral Modeling for Hyperspectral Imaging",
        "abstract": "We present PhISM, a physics-informed deep learning architecture that learns\nwithout supervision to explicitly disentangle hyperspectral observations and\nmodel them with continuous basis functions. \\mname outperforms prior methods on\nseveral classification and regression benchmarks, requires limited labeled\ndata, and provides additional insights thanks to interpretable latent\nrepresentation.",
        "published": "2025-08-29T13:32:07Z",
        "updated": "2025-08-29T13:32:07Z",
        "authors": [
            "Zuzanna Gawrysiak",
            "Krzysztof Krawiec"
        ],
        "link": "http://arxiv.org/abs/2508.21618v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21615v1",
        "title": "Adapting to Change: A Comparison of Continual and Transfer Learning for\n  Modeling Building Thermal Dynamics under Concept Drifts",
        "abstract": "Transfer Learning (TL) is currently the most effective approach for modeling\nbuilding thermal dynamics when only limited data are available. TL uses a\npretrained model that is fine-tuned to a specific target building. However, it\nremains unclear how to proceed after initial fine-tuning, as more operational\nmeasurement data are collected over time. This challenge becomes even more\ncomplex when the dynamics of the building change, for example, after a retrofit\nor a change in occupancy. In Machine Learning literature, Continual Learning\n(CL) methods are used to update models of changing systems. TL approaches can\nalso address this challenge by reusing the pretrained model at each update step\nand fine-tuning it with new measurement data. A comprehensive study on how to\nincorporate new measurement data over time to improve prediction accuracy and\naddress the challenges of concept drifts (changes in dynamics) for building\nthermal dynamics is still missing.\n  Therefore, this study compares several CL and TL strategies, as well as a\nmodel trained from scratch, for thermal dynamics modeling during building\noperation. The methods are evaluated using 5--7 years of simulated data\nrepresentative of single-family houses in Central Europe, including scenarios\nwith concept drifts from retrofits and changes in occupancy. We propose a CL\nstrategy (Seasonal Memory Learning) that provides greater accuracy improvements\nthan existing CL and TL methods, while maintaining low computational effort.\nSML outperformed the benchmark of initial fine-tuning by 28.1\\% without concept\ndrifts and 34.9\\% with concept drifts.",
        "published": "2025-08-29T13:29:54Z",
        "updated": "2025-08-29T13:29:54Z",
        "authors": [
            "Fabian Raisch",
            "Max Langtry",
            "Felix Koch",
            "Ruchi Choudhary",
            "Christoph Goebel",
            "Benjamin Tischler"
        ],
        "link": "http://arxiv.org/abs/2508.21615v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21571v1",
        "title": "Convergence of Stochastic Gradient Methods for Wide Two-Layer\n  Physics-Informed Neural Networks",
        "abstract": "Physics informed neural networks (PINNs) represent a very popular class of\nneural solvers for partial differential equations. In practice, one often\nemploys stochastic gradient descent type algorithms to train the neural\nnetwork. Therefore, the convergence guarantee of stochastic gradient descent is\nof fundamental importance. In this work, we establish the linear convergence of\nstochastic gradient descent / flow in training over-parameterized two layer\nPINNs for a general class of activation functions in the sense of high\nprobability. These results extend the existing result [18] in which gradient\ndescent was analyzed. The challenge of the analysis lies in handling the\ndynamic randomness introduced by stochastic optimization methods. The key of\nthe analysis lies in ensuring the positive definiteness of suitable Gram\nmatrices during the training. The analysis sheds insight into the dynamics of\nthe optimization process, and provides guarantees on the neural networks\ntrained by stochastic algorithms.",
        "published": "2025-08-29T12:25:51Z",
        "updated": "2025-08-29T12:25:51Z",
        "authors": [
            "Bangti Jin",
            "Longjun Wu"
        ],
        "link": "http://arxiv.org/abs/2508.21571v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21570v1",
        "title": "OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity\n  Imputation using Sparse Drifter Trajectories",
        "abstract": "Ocean salinity plays a vital role in circulation, climate, and marine\necosystems, yet its measurement is often sparse, irregular, and noisy,\nespecially in drifter-based datasets. Traditional approaches, such as remote\nsensing and optimal interpolation, rely on linearity and stationarity, and are\nlimited by cloud cover, sensor drift, and low satellite revisit rates. While\nmachine learning models offer flexibility, they often fail under severe\nsparsity and lack principled ways to incorporate physical covariates without\nspecialized sensors. In this paper, we introduce the OceAn Salinity Imputation\nSystem (OASIS), a novel diffusion adversarial framework designed to address\nthese challenges.",
        "published": "2025-08-29T12:25:26Z",
        "updated": "2025-08-29T12:25:26Z",
        "authors": [
            "Bo Li",
            "Yingqi Feng",
            "Ming Jin",
            "Xin Zheng",
            "Yufei Tang",
            "Laurent Cherubin",
            "Alan Wee-Chung Liew",
            "Can Wang",
            "Qinghua Lu",
            "Jingwei Yao",
            "Shirui Pan",
            "Hong Zhang",
            "Xingquan Zhu"
        ],
        "link": "http://arxiv.org/abs/2508.21570v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21569v1",
        "title": "L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models",
        "abstract": "We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)\ndataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT\nmodel optimized for regression-based similarity scoring. The MahaSTS dataset\nconsists of 16,860 Marathi sentence pairs labeled with continuous similarity\nscores in the range of 0-5. To ensure balanced supervision, the dataset is\nuniformly distributed across six score-based buckets spanning the full 0-5\nrange, thus reducing label bias and enhancing model stability. We fine-tune the\nMahaSBERT model on this dataset and benchmark its performance against other\nalternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments\ndemonstrate that MahaSTS enables effective training for sentence similarity\ntasks in Marathi, highlighting the impact of human-curated annotations,\ntargeted fine-tuning, and structured supervision in low-resource settings. The\ndataset and model are publicly shared at\nhttps://github.com/l3cube-pune/MarathiNLP",
        "published": "2025-08-29T12:24:31Z",
        "updated": "2025-08-29T12:24:31Z",
        "authors": [
            "Aishwarya Mirashi",
            "Ananya Joshi",
            "Raviraj Joshi"
        ],
        "link": "http://arxiv.org/abs/2508.21569v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21561v1",
        "title": "Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers\n  LLMs for Few-shot Tabular Classification",
        "abstract": "Recent studies show the promise of large language models (LLMs) for few-shot\ntabular classification but highlight challenges due to the variability in\nstructured data. To address this, we propose distilling data into actionable\ninsights to enable robust and effective classification by LLMs. Drawing\ninspiration from human learning processes, we introduce InsightTab, an insight\ndistillation framework guided by principles of divide-and-conquer, easy-first,\nand reflective learning. Our approach integrates rule summarization, strategic\nexemplification, and insight reflection through deep collaboration between LLMs\nand data modeling techniques. The obtained insights enable LLMs to better align\ntheir general knowledge and capabilities with the particular requirements of\nspecific tabular tasks. We extensively evaluate InsightTab on nine datasets.\nThe results demonstrate consistent improvement over state-of-the-art methods.\nAblation studies further validate the principle-guided distillation process,\nwhile analyses emphasize InsightTab's effectiveness in leveraging labeled data\nand managing bias.",
        "published": "2025-08-29T12:16:24Z",
        "updated": "2025-08-29T12:16:24Z",
        "authors": [
            "Yifei Yuan",
            "Jiatong Li",
            "Weijia Zhang",
            "Mohammad Aliannejadi",
            "Evangelos Kanoulas",
            "Renjun Hu"
        ],
        "link": "http://arxiv.org/abs/2508.21561v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21559v1",
        "title": "Limitations of Physics-Informed Neural Networks: a Study on Smart Grid\n  Surrogation",
        "abstract": "Physics-Informed Neural Networks (PINNs) present a transformative approach\nfor smart grid modeling by integrating physical laws directly into learning\nframeworks, addressing critical challenges of data scarcity and physical\nconsistency in conventional data-driven methods. This paper evaluates PINNs'\ncapabilities as surrogate models for smart grid dynamics, comparing their\nperformance against XGBoost, Random Forest, and Linear Regression across three\nkey experiments: interpolation, cross-validation, and episodic trajectory\nprediction. By training PINNs exclusively through physics-based loss functions\n(enforcing power balance, operational constraints, and grid stability) we\ndemonstrate their superior generalization, outperforming data-driven models in\nerror reduction. Notably, PINNs maintain comparatively lower MAE in dynamic\ngrid operations, reliably capturing state transitions in both random and\nexpert-driven control scenarios, while traditional models exhibit erratic\nperformance. Despite slight degradation in extreme operational regimes, PINNs\nconsistently enforce physical feasibility, proving vital for safety-critical\napplications. Our results contribute to establishing PINNs as a\nparadigm-shifting tool for smart grid surrogation, bridging data-driven\nflexibility with first-principles rigor. This work advances real-time grid\ncontrol and scalable digital twins, emphasizing the necessity of physics-aware\narchitectures in mission-critical energy systems.",
        "published": "2025-08-29T12:15:32Z",
        "updated": "2025-08-29T12:15:32Z",
        "authors": [
            "Julen Cestero",
            "Carmine Delle Femine",
            "Kenji S. Muro",
            "Marco Quartulli",
            "Marcello Restelli"
        ],
        "link": "http://arxiv.org/abs/2508.21559v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21554v1",
        "title": "Comprehensive Signal Quality Evaluation of a Wearable Textile ECG\n  Garment: A Sex-Balanced Study",
        "abstract": "We introduce a novel wearable textile-garment featuring an innovative\nelectrode placement aimed at minimizing noise and motion artifacts, thereby\nenhancing signal fidelity in Electrocardiography (ECG) recordings. We present a\ncomprehensive, sex-balanced evaluation involving 15 healthy males and 15\nhealthy female participants to ensure the device's suitability across\nanatomical and physiological variations. The assessment framework encompasses\ndistinct evaluation approaches: quantitative signal quality indices to\nobjectively benchmark device performance; rhythm-based analyzes of\nphysiological parameters such as heart rate and heart rate variability; machine\nlearning classification tasks to assess application-relevant predictive\nutility; morphological analysis of ECG features including amplitude and\ninterval parameters; and investigations of the effects of electrode projection\nangle given by the textile / body shape, with all analyzes stratified by sex to\nelucidate sex-specific influences. Evaluations were conducted across various\nactivity phases representing real-world conditions. The results demonstrate\nthat the textile system achieves signal quality highly concordant with\nreference devices in both rhythm and morphological analyses, exhibits robust\nclassification performance, and enables identification of key sex-specific\ndeterminants affecting signal acquisition. These findings underscore the\npractical viability of textile-based ECG garments for physiological monitoring\nas well as psychophysiological state detection. Moreover, we identify the\nimportance of incorporating sex-specific design considerations to ensure\nequitable and reliable cardiac diagnostics in wearable health technologies.",
        "published": "2025-08-29T12:10:59Z",
        "updated": "2025-08-29T12:10:59Z",
        "authors": [
            "Maximilian P. Oppelt",
            "Tobias S. Zech",
            "Sarah H. Lorenz",
            "Laurenz Ottmann",
            "Jan Steffan",
            "Bjoern M. Eskofier",
            "Nadine R. Lang-Richter",
            "Norman Pfeiffer"
        ],
        "link": "http://arxiv.org/abs/2508.21554v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21547v1",
        "title": "What Data is Really Necessary? A Feasibility Study of Inference Data\n  Minimization for Recommender Systems",
        "abstract": "Data minimization is a legal principle requiring personal data processing to\nbe limited to what is necessary for a specified purpose. Operationalizing this\nprinciple for recommender systems, which rely on extensive personal data,\nremains a significant challenge. This paper conducts a feasibility study on\nminimizing implicit feedback inference data for such systems. We propose a\nnovel problem formulation, analyze various minimization techniques, and\ninvestigate key factors influencing their effectiveness. We demonstrate that\nsubstantial inference data reduction is technically feasible without\nsignificant performance loss. However, its practicality is critically\ndetermined by two factors: the technical setting (e.g., performance targets,\nchoice of model) and user characteristics (e.g., history size, preference\ncomplexity). Thus, while we establish its technical feasibility, we conclude\nthat data minimization remains practically challenging and its dependence on\nthe technical and user context makes a universal standard for data `necessity'\ndifficult to implement.",
        "published": "2025-08-29T12:01:17Z",
        "updated": "2025-08-29T12:01:17Z",
        "authors": [
            "Jens Leysen",
            "Marco Favier",
            "Bart Goethals"
        ],
        "link": "http://arxiv.org/abs/2508.21547v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21531v1",
        "title": "Adaptive generative moment matching networks for improved learning of\n  dependence structures",
        "abstract": "An adaptive bandwidth selection procedure for the mixture kernel in the\nmaximum mean discrepancy (MMD) for fitting generative moment matching networks\n(GMMNs) is introduced, and its ability to improve the learning of copula random\nnumber generators is demonstrated. Based on the relative error of the training\nloss, the number of kernels is increased during training; additionally, the\nrelative error of the validation loss is used as an early stopping criterion.\nWhile training time of such adaptively trained GMMNs (AGMMNs) is similar to\nthat of GMMNs, training performance is increased significantly in comparison to\nGMMNs, which is assessed and shown based on validation MMD trajectories,\nsamples and validation MMD values. Superiority of AGMMNs over GMMNs, as well as\ntypical parametric copula models, is demonstrated in terms of three\napplications. First, convergence rates of quasi-random versus pseudo-random\nsamples from high-dimensional copulas are investigated for three functionals of\ninterest and in dimensions as large as 100 for the first time. Second,\nreplicated validation MMDs, as well as Monte Carlo and quasi-Monte Carlo\napplications based on the expected payoff of a basked call option and the risk\nmeasure expected shortfall as functionals are used to demonstrate the improved\ntraining of AGMMNs over GMMNs for a copula model fitted to the standardized\nresiduals of the 50 constituents of the S&P 500 index after deGARCHing. Last,\nboth the latter dataset and 50 constituents of the FTSE~100 are used to\ndemonstrate that the improved training of AGMMNs over GMMNs and in comparison\nto the fitting of classical parametric copula models indeed also translates to\nan improved model prediction.",
        "published": "2025-08-29T11:38:25Z",
        "updated": "2025-08-29T11:38:25Z",
        "authors": [
            "Marius Hofert",
            "Gan Yao"
        ],
        "link": "http://arxiv.org/abs/2508.21531v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21524v1",
        "title": "Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory\n  CNN Accelerators",
        "abstract": "Compute-in-memory (CIM) accelerators have emerged as a promising way for\nenhancing the energy efficiency of convolutional neural networks (CNNs).\nDeploying CNNs on CIM platforms generally requires quantization of network\nweights and activations to meet hardware constraints. However, existing\napproaches either prioritize hardware efficiency with binary weight and\nactivation quantization at the cost of accuracy, or utilize multi-bit weights\nand activations for greater accuracy but limited efficiency. In this paper, we\nintroduce a novel binary weight multi-bit activation (BWMA) method for CNNs on\nCIM-based accelerators. Our contributions include: deriving closed-form\nsolutions for weight quantization in each layer, significantly improving the\nrepresentational capabilities of binarized weights; and developing a\ndifferentiable function for activation quantization, approximating the ideal\nmulti-bit function while bypassing the extensive search for optimal settings.\nThrough comprehensive experiments on CIFAR-10 and ImageNet datasets, we show\nthat BWMA achieves notable accuracy improvements over existing methods,\nregistering gains of 1.44\\%-5.46\\% and 0.35\\%-5.37\\% on respective datasets.\nMoreover, hardware simulation results indicate that 4-bit activation\nquantization strikes the optimal balance between hardware cost and model\nperformance.",
        "published": "2025-08-29T11:24:24Z",
        "updated": "2025-08-29T11:24:24Z",
        "authors": [
            "Wenyong Zhou",
            "Zhengwu Liu",
            "Yuan Ren",
            "Ngai Wong"
        ],
        "link": "http://arxiv.org/abs/2508.21524v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21513v1",
        "title": "On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph\n  Ricci Curvature",
        "abstract": "Graph Neural Networks (GNNs) have recently shown promise as solvers for\nBoolean Satisfiability Problems (SATs) by operating on graph representations of\nlogical formulas. However, their performance degrades sharply on harder\ninstances, raising the question of whether this reflects fundamental\narchitectural limitations. In this work, we provide a geometric explanation\nthrough the lens of graph Ricci Curvature (RC), which quantifies local\nconnectivity bottlenecks. We prove that bipartite graphs derived from random\nk-SAT formulas are inherently negatively curved, and that this curvature\ndecreases with instance difficulty. Building on this, we show that GNN-based\nSAT solvers are affected by oversquashing, a phenomenon where long-range\ndependencies become impossible to compress into fixed-length representations.\nWe validate our claims empirically across different SAT benchmarks and confirm\nthat curvature is both a strong indicator of problem complexity and can be used\nto predict performance. Finally, we connect our findings to design principles\nof existing solvers and outline promising directions for future work.",
        "published": "2025-08-29T10:54:19Z",
        "updated": "2025-08-29T10:54:19Z",
        "authors": [
            "Geri Skenderi"
        ],
        "link": "http://arxiv.org/abs/2508.21513v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21512v1",
        "title": "Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval\n  across Table-to-Text Serialization Approaches",
        "abstract": "Large Language Models (LLMs) are increasingly employed in high-stakes\ndecision-making tasks, such as loan approvals. While their applications expand\nacross domains, LLMs struggle to process tabular data, ensuring fairness and\ndelivering reliable predictions. In this work, we assess the performance and\nfairness of LLMs on serialized loan approval datasets from three geographically\ndistinct regions: Ghana, Germany, and the United States. Our evaluation focuses\non the model's zero-shot and in-context learning (ICL) capabilities. Our\nresults reveal that the choice of serialization (Serialization refers to the\nprocess of converting tabular data into text formats suitable for processing by\nLLMs.) format significantly affects both performance and fairness in LLMs, with\ncertain formats such as GReat and LIFT yielding higher F1 scores but\nexacerbating fairness disparities. Notably, while ICL improved model\nperformance by 4.9-59.6% relative to zero-shot baselines, its effect on\nfairness varied considerably across datasets. Our work underscores the\nimportance of effective tabular data representation methods and fairness-aware\nmodels to improve the reliability of LLMs in financial decision-making.",
        "published": "2025-08-29T10:51:41Z",
        "updated": "2025-08-29T10:51:41Z",
        "authors": [
            "Israel Abebe Azime",
            "Deborah D. Kanubala",
            "Tejumade Afonja",
            "Mario Fritz",
            "Isabel Valera",
            "Dietrich Klakow",
            "Philipp Slusallek"
        ],
        "link": "http://arxiv.org/abs/2508.21512v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21505v1",
        "title": "Spiking Decision Transformers: Local Plasticity, Phase-Coding, and\n  Dendritic Routing for Low-Power Sequence Control",
        "abstract": "Reinforcement learning agents based on Transformer architectures have\nachieved impressive performance on sequential decision-making tasks, but their\nreliance on dense matrix operations makes them ill-suited for\nenergy-constrained, edge-oriented platforms. Spiking neural networks promise\nultra-low-power, event-driven inference, yet no prior work has seamlessly\nmerged spiking dynamics with return-conditioned sequence modeling. We present\nthe Spiking Decision Transformer (SNN-DT), which embeds Leaky\nIntegrate-and-Fire neurons into each self-attention block, trains end-to-end\nvia surrogate gradients, and incorporates biologically inspired three-factor\nplasticity, phase-shifted spike-based positional encodings, and a lightweight\ndendritic routing module. Our implementation matches or exceeds standard\nDecision Transformer performance on classic control benchmarks (CartPole-v1,\nMountainCar-v0, Acrobot-v1, Pendulum-v1) while emitting fewer than ten spikes\nper decision, an energy proxy suggesting over four orders-of-magnitude\nreduction in per inference energy. By marrying sequence modeling with\nneuromorphic efficiency, SNN-DT opens a pathway toward real-time, low-power\ncontrol on embedded and wearable devices.",
        "published": "2025-08-29T10:37:37Z",
        "updated": "2025-08-29T10:37:37Z",
        "authors": [
            "Vishal Pandey",
            "Debasmita Biswas"
        ],
        "link": "http://arxiv.org/abs/2508.21505v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21495v1",
        "title": "Failure Prediction Is a Better Performance Proxy for Early-Exit Networks\n  Than Calibration",
        "abstract": "Early-exit models speed up inference by attaching internal classifiers to\nintermediate layers of the model and allowing computation to stop once a\nprediction satisfies an exit criterion. Most early-exit methods rely on\nconfidence-based exit strategies, which motivated some works to calibrate\nintermediate classifiers to improve the performance of the entire model. In\nthis paper, we show that calibration measures can be misleading indicators of\nthe performance of multi-exit models: a well-calibrated classifier may still\nwaste computation, and common calibration methods do not preserve the sample\nranking within a classifier. We demonstrate empirical cases where miscalibrated\nnetworks outperform calibrated ones. As an alternative, we propose to use\nfailure prediction as a more useful proxy for early-exit model performance.\nUnlike calibration, failure prediction accounts for changes in the ranking of\nsamples and shows a strong correlation with efficiency improvements, making it\na more dependable basis for designing and evaluating early-exit models.",
        "published": "2025-08-29T10:21:46Z",
        "updated": "2025-08-29T10:21:46Z",
        "authors": [
            "Piotr Kubaty",
            "Filip Szatkowski",
            "Metod Jazbec",
            "Bartosz Wójcik"
        ],
        "link": "http://arxiv.org/abs/2508.21495v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21488v1",
        "title": "Priors Matter: Addressing Misspecification in Bayesian Deep Q-Learning",
        "abstract": "Uncertainty quantification in reinforcement learning can greatly improve\nexploration and robustness. Approximate Bayesian approaches have recently been\npopularized to quantify uncertainty in model-free algorithms. However, so far\nthe focus has been on improving the accuracy of the posterior approximation,\ninstead of studying the accuracy of the prior and likelihood assumptions\nunderlying the posterior. In this work, we demonstrate that there is a cold\nposterior effect in Bayesian deep Q-learning, where contrary to theory,\nperformance increases when reducing the temperature of the posterior. To\nidentify and overcome likely causes, we challenge common assumptions made on\nthe likelihood and priors in Bayesian model-free algorithms. We empirically\nstudy prior distributions and show through statistical tests that the common\nGaussian likelihood assumption is frequently violated. We argue that developing\nmore suitable likelihoods and priors should be a key focus in future Bayesian\nreinforcement learning research and we offer simple, implementable solutions\nfor better priors in deep Q-learning that lead to more performant Bayesian\nalgorithms.",
        "published": "2025-08-29T10:12:42Z",
        "updated": "2025-08-29T10:12:42Z",
        "authors": [
            "Pascal R. van der Vaart",
            "Neil Yorke-Smith",
            "Matthijs T. J. Spaan"
        ],
        "link": "http://arxiv.org/abs/2508.21488v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21484v1",
        "title": "Data-driven Discovery of Digital Twins in Biomedical Research",
        "abstract": "Recent technological advances have expanded the availability of\nhigh-throughput biological datasets, enabling the reliable design of digital\ntwins of biomedical systems or patients. Such computational tools represent key\nreaction networks driving perturbation or drug response and can guide drug\ndiscovery and personalized therapeutics. Yet, their development still relies on\nlaborious data integration by the human modeler, so that automated approaches\nare critically needed. The success of data-driven system discovery in Physics,\nrooted in clean datasets and well-defined governing laws, has fueled interest\nin applying similar techniques in Biology, which presents unique challenges.\nHere, we reviewed methodologies for automatically inferring digital twins from\nbiological time series, which mostly involve symbolic or sparse regression. We\nevaluate algorithms according to eight biological and methodological\nchallenges, associated to noisy/incomplete data, multiple conditions, prior\nknowledge integration, latent variables, high dimensionality, unobserved\nvariable derivatives, candidate library design, and uncertainty quantification.\nUpon these criteria, sparse regression generally outperformed symbolic\nregression, particularly when using Bayesian frameworks. We further highlight\nthe emerging role of deep learning and large language models, which enable\ninnovative prior knowledge integration, though the reliability and consistency\nof such approaches must be improved. While no single method addresses all\nchallenges, we argue that progress in learning digital twins will come from\nhybrid and modular frameworks combining chemical reaction network-based\nmechanistic grounding, Bayesian uncertainty quantification, and the generative\nand knowledge integration capacities of deep learning. To support their\ndevelopment, we further propose a benchmarking framework to evaluate methods\nacross all challenges.",
        "published": "2025-08-29T10:10:02Z",
        "updated": "2025-08-29T10:10:02Z",
        "authors": [
            "Clémence Métayer",
            "Annabelle Ballesta",
            "Julien Martinelli"
        ],
        "link": "http://arxiv.org/abs/2508.21484v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21482v1",
        "title": "HSFN: Hierarchical Selection for Fake News Detection building\n  Heterogeneous Ensemble",
        "abstract": "Psychological biases, such as confirmation bias, make individuals\nparticularly vulnerable to believing and spreading fake news on social media,\nleading to significant consequences in domains such as public health and\npolitics. Machine learning-based fact-checking systems have been widely studied\nto mitigate this problem. Among them, ensemble methods are particularly\neffective in combining multiple classifiers to improve robustness. However,\ntheir performance heavily depends on the diversity of the constituent\nclassifiers-selecting genuinely diverse models remains a key challenge,\nespecially when models tend to learn redundant patterns. In this work, we\npropose a novel automatic classifier selection approach that prioritizes\ndiversity, also extended by performance. The method first computes pairwise\ndiversity between classifiers and applies hierarchical clustering to organize\nthem into groups at different levels of granularity. A HierarchySelect then\nexplores these hierarchical levels to select one pool of classifiers per level,\neach representing a distinct intra-pool diversity. The most diverse pool is\nidentified and selected for ensemble construction from these. The selection\nprocess incorporates an evaluation metric reflecting each classifiers's\nperformance to ensure the ensemble also generalises well. We conduct\nexperiments with 40 heterogeneous classifiers across six datasets from\ndifferent application domains and with varying numbers of classes. Our method\nis compared against the Elbow heuristic and state-of-the-art baselines. Results\nshow that our approach achieves the highest accuracy on two of six datasets.\nThe implementation details are available on the project's repository:\nhttps://github.com/SaraBCoutinho/HSFN .",
        "published": "2025-08-29T10:09:20Z",
        "updated": "2025-08-29T10:09:20Z",
        "authors": [
            "Sara B. Coutinho",
            "Rafael M. O. Cruz",
            "Francimaria R. S. Nascimento",
            "George D. C. Cavalcanti"
        ],
        "link": "http://arxiv.org/abs/2508.21482v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21571v1",
        "title": "Convergence of Stochastic Gradient Methods for Wide Two-Layer\n  Physics-Informed Neural Networks",
        "abstract": "Physics informed neural networks (PINNs) represent a very popular class of\nneural solvers for partial differential equations. In practice, one often\nemploys stochastic gradient descent type algorithms to train the neural\nnetwork. Therefore, the convergence guarantee of stochastic gradient descent is\nof fundamental importance. In this work, we establish the linear convergence of\nstochastic gradient descent / flow in training over-parameterized two layer\nPINNs for a general class of activation functions in the sense of high\nprobability. These results extend the existing result [18] in which gradient\ndescent was analyzed. The challenge of the analysis lies in handling the\ndynamic randomness introduced by stochastic optimization methods. The key of\nthe analysis lies in ensuring the positive definiteness of suitable Gram\nmatrices during the training. The analysis sheds insight into the dynamics of\nthe optimization process, and provides guarantees on the neural networks\ntrained by stochastic algorithms.",
        "published": "2025-08-29T12:25:51Z",
        "updated": "2025-08-29T12:25:51Z",
        "authors": [
            "Bangti Jin",
            "Longjun Wu"
        ],
        "link": "http://arxiv.org/abs/2508.21571v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21531v1",
        "title": "Adaptive generative moment matching networks for improved learning of\n  dependence structures",
        "abstract": "An adaptive bandwidth selection procedure for the mixture kernel in the\nmaximum mean discrepancy (MMD) for fitting generative moment matching networks\n(GMMNs) is introduced, and its ability to improve the learning of copula random\nnumber generators is demonstrated. Based on the relative error of the training\nloss, the number of kernels is increased during training; additionally, the\nrelative error of the validation loss is used as an early stopping criterion.\nWhile training time of such adaptively trained GMMNs (AGMMNs) is similar to\nthat of GMMNs, training performance is increased significantly in comparison to\nGMMNs, which is assessed and shown based on validation MMD trajectories,\nsamples and validation MMD values. Superiority of AGMMNs over GMMNs, as well as\ntypical parametric copula models, is demonstrated in terms of three\napplications. First, convergence rates of quasi-random versus pseudo-random\nsamples from high-dimensional copulas are investigated for three functionals of\ninterest and in dimensions as large as 100 for the first time. Second,\nreplicated validation MMDs, as well as Monte Carlo and quasi-Monte Carlo\napplications based on the expected payoff of a basked call option and the risk\nmeasure expected shortfall as functionals are used to demonstrate the improved\ntraining of AGMMNs over GMMNs for a copula model fitted to the standardized\nresiduals of the 50 constituents of the S&P 500 index after deGARCHing. Last,\nboth the latter dataset and 50 constituents of the FTSE~100 are used to\ndemonstrate that the improved training of AGMMNs over GMMNs and in comparison\nto the fitting of classical parametric copula models indeed also translates to\nan improved model prediction.",
        "published": "2025-08-29T11:38:25Z",
        "updated": "2025-08-29T11:38:25Z",
        "authors": [
            "Marius Hofert",
            "Gan Yao"
        ],
        "link": "http://arxiv.org/abs/2508.21531v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21484v1",
        "title": "Data-driven Discovery of Digital Twins in Biomedical Research",
        "abstract": "Recent technological advances have expanded the availability of\nhigh-throughput biological datasets, enabling the reliable design of digital\ntwins of biomedical systems or patients. Such computational tools represent key\nreaction networks driving perturbation or drug response and can guide drug\ndiscovery and personalized therapeutics. Yet, their development still relies on\nlaborious data integration by the human modeler, so that automated approaches\nare critically needed. The success of data-driven system discovery in Physics,\nrooted in clean datasets and well-defined governing laws, has fueled interest\nin applying similar techniques in Biology, which presents unique challenges.\nHere, we reviewed methodologies for automatically inferring digital twins from\nbiological time series, which mostly involve symbolic or sparse regression. We\nevaluate algorithms according to eight biological and methodological\nchallenges, associated to noisy/incomplete data, multiple conditions, prior\nknowledge integration, latent variables, high dimensionality, unobserved\nvariable derivatives, candidate library design, and uncertainty quantification.\nUpon these criteria, sparse regression generally outperformed symbolic\nregression, particularly when using Bayesian frameworks. We further highlight\nthe emerging role of deep learning and large language models, which enable\ninnovative prior knowledge integration, though the reliability and consistency\nof such approaches must be improved. While no single method addresses all\nchallenges, we argue that progress in learning digital twins will come from\nhybrid and modular frameworks combining chemical reaction network-based\nmechanistic grounding, Bayesian uncertainty quantification, and the generative\nand knowledge integration capacities of deep learning. To support their\ndevelopment, we further propose a benchmarking framework to evaluate methods\nacross all challenges.",
        "published": "2025-08-29T10:10:02Z",
        "updated": "2025-08-29T10:10:02Z",
        "authors": [
            "Clémence Métayer",
            "Annabelle Ballesta",
            "Julien Martinelli"
        ],
        "link": "http://arxiv.org/abs/2508.21484v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21255v1",
        "title": "Weighted Support Points from Random Measures: An Interpretable\n  Alternative for Generative Modeling",
        "abstract": "Support points summarize a large dataset through a smaller set of\nrepresentative points that can be used for data operations, such as Monte Carlo\nintegration, without requiring access to the full dataset. In this sense,\nsupport points offer a compact yet informative representation of the original\ndata. We build on this idea to introduce a generative modeling framework based\non random weighted support points, where the randomness arises from a weighting\nscheme inspired by the Dirichlet process and the Bayesian bootstrap. The\nproposed method generates diverse and interpretable sample sets from a fixed\ndataset, without relying on probabilistic modeling assumptions or neural\nnetwork architectures. We present the theoretical formulation of the method and\ndevelop an efficient optimization algorithm based on the Convex--Concave\nProcedure (CCP). Empirical results on the MNIST and CelebA-HQ datasets show\nthat our approach produces high-quality and diverse outputs at a fraction of\nthe computational cost of black-box alternatives such as Generative Adversarial\nNetworks (GANs) or Denoising Diffusion Probabilistic Models (DDPMs). These\nresults suggest that random weighted support points offer a principled,\nscalable, and interpretable alternative for generative modeling. A key feature\nis their ability to produce genuinely interpolative samples that preserve\nunderlying data structure.",
        "published": "2025-08-28T22:58:39Z",
        "updated": "2025-08-28T22:58:39Z",
        "authors": [
            "Peiqi Zhao",
            "Carlos E. Rodríguez",
            "Ramsés H. Mena",
            "Stephen G. Walker"
        ],
        "link": "http://arxiv.org/abs/2508.21255v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21184v1",
        "title": "BED-LLM: Intelligent Information Gathering with LLMs and Bayesian\n  Experimental Design",
        "abstract": "We propose a general-purpose approach for improving the ability of Large\nLanguage Models (LLMs) to intelligently and adaptively gather information from\na user or other external source using the framework of sequential Bayesian\nexperimental design (BED). This enables LLMs to act as effective multi-turn\nconversational agents and interactively interface with external environments.\nOur approach, which we call BED-LLM (Bayesian Experimental Design with Large\nLanguage Models), is based on iteratively choosing questions or queries that\nmaximize the expected information gain (EIG) about the task of interest given\nthe responses gathered previously. We show how this EIG can be formulated in a\nprincipled way using a probabilistic model derived from the LLM's belief\ndistribution and provide detailed insights into key decisions in its\nconstruction. Further key to the success of BED-LLM are a number of specific\ninnovations, such as a carefully designed estimator for the EIG, not solely\nrelying on in-context updates for conditioning on previous responses, and a\ntargeted strategy for proposing candidate queries. We find that BED-LLM\nachieves substantial gains in performance across a wide range of tests based on\nthe 20-questions game and using the LLM to actively infer user preferences,\ncompared to direct prompting of the LLM and other adaptive design strategies.",
        "published": "2025-08-28T19:51:43Z",
        "updated": "2025-08-28T19:51:43Z",
        "authors": [
            "Deepro Choudhury",
            "Sinead Williamson",
            "Adam Goliński",
            "Ning Miao",
            "Freddie Bickford Smith",
            "Michael Kirchhof",
            "Yizhe Zhang",
            "Tom Rainforth"
        ],
        "link": "http://arxiv.org/abs/2508.21184v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21146v1",
        "title": "Privacy Auditing Synthetic Data Release through Local Likelihood Attacks",
        "abstract": "Auditing the privacy leakage of synthetic data is an important but unresolved\nproblem. Most existing privacy auditing frameworks for synthetic data rely on\nheuristics and unreasonable assumptions to attack the failure modes of\ngenerative models, exhibiting limited capability to describe and detect the\nprivacy exposure of training data through synthetic data release. In this\npaper, we study designing Membership Inference Attacks (MIAs) that specifically\nexploit the observation that tabular generative models tend to significantly\noverfit to certain regions of the training distribution. Here, we propose\nGenerative Likelihood Ratio Attack (Gen-LRA), a novel, computationally\nefficient No-Box MIA that, with no assumption of model knowledge or access,\nformulates its attack by evaluating the influence a test observation has in a\nsurrogate model's estimation of a local likelihood ratio over the synthetic\ndata. Assessed over a comprehensive benchmark spanning diverse datasets, model\narchitectures, and attack parameters, we find that Gen-LRA consistently\ndominates other MIAs for generative models across multiple performance metrics.\nThese results underscore Gen-LRA's effectiveness as a privacy auditing tool for\nthe release of synthetic data, highlighting the significant privacy risks posed\nby generative model overfitting in real-world applications.",
        "published": "2025-08-28T18:27:40Z",
        "updated": "2025-08-28T18:27:40Z",
        "authors": [
            "Joshua Ward",
            "Chi-Hua Wang",
            "Guang Cheng"
        ],
        "link": "http://arxiv.org/abs/2508.21146v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21022v1",
        "title": "Fast Convergence Rates for Subsampled Natural Gradient Algorithms on\n  Quadratic Model Problems",
        "abstract": "Subsampled natural gradient descent (SNGD) has shown impressive results for\nparametric optimization tasks in scientific machine learning, such as neural\nnetwork wavefunctions and physics-informed neural networks, but it has lacked a\ntheoretical explanation. We address this gap by analyzing the convergence of\nSNGD and its accelerated variant, SPRING, for idealized parametric optimization\nproblems where the model is linear and the loss function is strongly convex and\nquadratic. In the special case of a least-squares loss, namely the standard\nlinear least-squares problem, we prove that SNGD is equivalent to a regularized\nKaczmarz method while SPRING is equivalent to an accelerated regularized\nKaczmarz method. As a result, by leveraging existing analyses we obtain under\nmild conditions (i) the first fast convergence rate for SNGD, (ii) the first\nconvergence guarantee for SPRING in any setting, and (iii) the first proof that\nSPRING can accelerate SNGD. In the case of a general strongly convex quadratic\nloss, we extend the analysis of the regularized Kaczmarz method to obtain a\nfast convergence rate for SNGD under stronger conditions, providing the first\nexplanation for the effectiveness of SNGD outside of the least-squares setting.\nOverall, our results illustrate how tools from randomized linear algebra can\nshed new light on the interplay between subsampling and curvature-aware\noptimization strategies.",
        "published": "2025-08-28T17:24:59Z",
        "updated": "2025-08-28T17:24:59Z",
        "authors": [
            "Gil Goldshlager",
            "Jiang Hu",
            "Lin Lin"
        ],
        "link": "http://arxiv.org/abs/2508.21022v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.20942v1",
        "title": "Transfer Learning for Classification under Decision Rule Drift with\n  Application to Optimal Individualized Treatment Rule Estimation",
        "abstract": "In this paper, we extend the transfer learning classification framework from\nregression function-based methods to decision rules. We propose a novel\nmethodology for modeling posterior drift through Bayes decision rules. By\nexploiting the geometric transformation of the Bayes decision boundary, our\nmethod reformulates the problem as a low-dimensional empirical risk\nminimization problem. Under mild regularity conditions, we establish the\nconsistency of our estimators and derive the risk bounds. Moreover, we\nillustrate the broad applicability of our method by adapting it to the\nestimation of optimal individualized treatment rules. Extensive simulation\nstudies and analyses of real-world data further demonstrate both superior\nperformance and robustness of our approach.",
        "published": "2025-08-28T16:03:06Z",
        "updated": "2025-08-28T16:03:06Z",
        "authors": [
            "Xiaohan Wang",
            "Yang Ning"
        ],
        "link": "http://arxiv.org/abs/2508.20942v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.20886v1",
        "title": "Polynomial Chaos Expansion for Operator Learning",
        "abstract": "Operator learning (OL) has emerged as a powerful tool in scientific machine\nlearning (SciML) for approximating mappings between infinite-dimensional\nfunctional spaces. One of its main applications is learning the solution\noperator of partial differential equations (PDEs). While much of the progress\nin this area has been driven by deep neural network-based approaches such as\nDeep Operator Networks (DeepONet) and Fourier Neural Operator (FNO), recent\nwork has begun to explore traditional machine learning methods for OL. In this\nwork, we introduce polynomial chaos expansion (PCE) as an OL method. PCE has\nbeen widely used for uncertainty quantification (UQ) and has recently gained\nattention in the context of SciML. For OL, we establish a mathematical\nframework that enables PCE to approximate operators in both purely data-driven\nand physics-informed settings. The proposed framework reduces the task of\nlearning the operator to solving a system of equations for the PCE\ncoefficients. Moreover, the framework provides UQ by simply post-processing the\nPCE coefficients, without any additional computational cost. We apply the\nproposed method to a diverse set of PDE problems to demonstrate its\ncapabilities. Numerical results demonstrate the strong performance of the\nproposed method in both OL and UQ tasks, achieving excellent numerical accuracy\nand computational efficiency.",
        "published": "2025-08-28T15:14:44Z",
        "updated": "2025-08-28T15:14:44Z",
        "authors": [
            "Himanshu Sharma",
            "Lukáš Novák",
            "Michael D. Shields"
        ],
        "link": "http://arxiv.org/abs/2508.20886v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.20755v1",
        "title": "Provable Benefits of In-Tool Learning for Large Language Models",
        "abstract": "Tool-augmented language models, equipped with retrieval, memory, or external\nAPIs, are reshaping AI, yet their theoretical advantages remain underexplored.\nIn this paper, we address this question by demonstrating the benefits of\nin-tool learning (external retrieval) over in-weight learning (memorization)\nfor factual recall. We show that the number of facts a model can memorize\nsolely in its weights is fundamentally limited by its parameter count. In\ncontrast, we prove that tool-use enables unbounded factual recall via a simple\nand efficient circuit construction. These results are validated in controlled\nexperiments, where tool-using models consistently outperform memorizing ones.\nWe further show that for pretrained large language models, teaching tool-use\nand general rules is more effective than finetuning facts into memory. Our work\nprovides both a theoretical and empirical foundation, establishing why\ntool-augmented workflows are not just practical, but provably more scalable.",
        "published": "2025-08-28T13:12:19Z",
        "updated": "2025-08-28T13:12:19Z",
        "authors": [
            "Sam Houliston",
            "Ambroise Odonnat",
            "Charles Arnal",
            "Vivien Cabannes"
        ],
        "link": "http://arxiv.org/abs/2508.20755v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.20618v1",
        "title": "Supervised Stochastic Gradient Algorithms for Multi-Trial Source\n  Separation",
        "abstract": "We develop a stochastic algorithm for independent component analysis that\nincorporates multi-trial supervision, which is available in many scientific\ncontexts. The method blends a proximal gradient-type algorithm in the space of\ninvertible matrices with joint learning of a prediction model through\nbackpropagation. We illustrate the proposed algorithm on synthetic and real\ndata experiments. In particular, owing to the additional supervision, we\nobserve an increased success rate of the non-convex optimization and the\nimproved interpretability of the independent components.",
        "published": "2025-08-28T10:06:44Z",
        "updated": "2025-08-28T10:06:44Z",
        "authors": [
            "Ronak Mehta",
            "Mateus Piovezan Otto",
            "Noah Stanis",
            "Azadeh Yazdan-Shahmorad",
            "Zaid Harchaoui"
        ],
        "link": "http://arxiv.org/abs/2508.20618v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.20616v1",
        "title": "Dimension Agnostic Testing of Survey Data Credibility through the Lens\n  of Regression",
        "abstract": "Assessing whether a sample survey credibly represents the population is a\ncritical question for ensuring the validity of downstream research. Generally,\nthis problem reduces to estimating the distance between two high-dimensional\ndistributions, which typically requires a number of samples that grows\nexponentially with the dimension. However, depending on the model used for data\nanalysis, the conclusions drawn from the data may remain consistent across\ndifferent underlying distributions. In this context, we propose a task-based\napproach to assess the credibility of sampled surveys. Specifically, we\nintroduce a model-specific distance metric to quantify this notion of\ncredibility. We also design an algorithm to verify the credibility of survey\ndata in the context of regression models. Notably, the sample complexity of our\nalgorithm is independent of the data dimension. This efficiency stems from the\nfact that the algorithm focuses on verifying the credibility of the survey data\nrather than reconstructing the underlying regression model. Furthermore, we\nshow that if one attempts to verify credibility by reconstructing the\nregression model, the sample complexity scales linearly with the dimensionality\nof the data. We prove the theoretical correctness of our algorithm and\nnumerically demonstrate our algorithm's performance.",
        "published": "2025-08-28T10:02:08Z",
        "updated": "2025-08-28T10:02:08Z",
        "authors": [
            "Debabrota Basu",
            "Sourav Chakraborty",
            "Debarshi Chanda",
            "Buddha Dev Das",
            "Arijit Ghosh",
            "Arnab Ray"
        ],
        "link": "http://arxiv.org/abs/2508.20616v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.20614v1",
        "title": "Towards Trustworthy Amortized Bayesian Model Comparison",
        "abstract": "Amortized Bayesian model comparison (BMC) enables fast probabilistic ranking\nof models via simulation-based training of neural surrogates. However, the\nreliability of neural surrogates deteriorates when simulation models are\nmisspecified - the very case where model comparison is most needed. Thus, we\nsupplement simulation-based training with a self-consistency (SC) loss on\nunlabeled real data to improve BMC estimates under empirical distribution\nshifts. Using a numerical experiment and two case studies with real data, we\ncompare amortized evidence estimates with and without SC against analytic or\nbridge sampling benchmarks. SC improves calibration under model\nmisspecification when having access to analytic likelihoods. However, it offers\nlimited gains with neural surrogate likelihoods, making it most practical for\ntrustworthy BMC when likelihoods are exact.",
        "published": "2025-08-28T10:01:01Z",
        "updated": "2025-08-28T10:01:01Z",
        "authors": [
            "Šimon Kucharský",
            "Aayush Mishra",
            "Daniel Habermann",
            "Stefan T. Radev",
            "Paul-Christian Bürkner"
        ],
        "link": "http://arxiv.org/abs/2508.20614v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.20588v1",
        "title": "Unbiased Stochastic Optimization for Gaussian Processes on Finite\n  Dimensional RKHS",
        "abstract": "Current methods for stochastic hyperparameter learning in Gaussian Processes\n(GPs) rely on approximations, such as computing biased stochastic gradients or\nusing inducing points in stochastic variational inference. However, when using\nsuch methods we are not guaranteed to converge to a stationary point of the\ntrue marginal likelihood. In this work, we propose algorithms for exact\nstochastic inference of GPs with kernels that induce a Reproducing Kernel\nHilbert Space (RKHS) of moderate finite dimension. Our approach can also be\nextended to infinite dimensional RKHSs at the cost of forgoing exactness. Both\nfor finite and infinite dimensional RKHSs, our method achieves better\nexperimental results than existing methods when memory resources limit the\nfeasible batch size and the possible number of inducing points.",
        "published": "2025-08-28T09:27:20Z",
        "updated": "2025-08-28T09:27:20Z",
        "authors": [
            "Neta Shoham",
            "Haim Avron"
        ],
        "link": "http://arxiv.org/abs/2508.20588v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.20327v1",
        "title": "Latent Factor Point Processes for Patient Representation in Electronic\n  Health Records",
        "abstract": "Electronic health records (EHR) contain valuable longitudinal patient-level\ninformation, yet most statistical methods reduce the irregular timing of EHR\ncodes into simple counts, thereby discarding rich temporal structure. Existing\ntemporal models often impose restrictive parametric assumptions or are tailored\nto code level rather than patient-level tasks. We propose the latent factor\npoint process model, which represents code occurrences as a high-dimensional\npoint process whose conditional intensity is driven by a low dimensional latent\nPoisson process. This low-rank structure reflects the clinical reality that\nthousands of codes are governed by a small number of underlying disease\nprocesses, while enabling statistically efficient estimation in high\ndimensions. Building on this model, we introduce the Fourier-Eigen embedding, a\npatient representation constructed from the spectral density matrix of the\nobserved process. We establish theoretical guarantees showing that these\nembeddings efficiently capture subgroup-specific temporal patterns for\ndownstream classification and clustering. Simulations and an application to an\nAlzheimer's disease EHR cohort demonstrate the practical advantages of our\napproach in uncovering clinically meaningful heterogeneity.",
        "published": "2025-08-28T00:08:55Z",
        "updated": "2025-08-28T00:08:55Z",
        "authors": [
            "Parker Knight",
            "Doudou Zhou",
            "Zongqi Xia",
            "Tianxi Cai",
            "Junwei Lu"
        ],
        "link": "http://arxiv.org/abs/2508.20327v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.20326v1",
        "title": "Stochastic Gradients under Nuisances",
        "abstract": "Stochastic gradient optimization is the dominant learning paradigm for a\nvariety of scenarios, from classical supervised learning to modern\nself-supervised learning. We consider stochastic gradient algorithms for\nlearning problems whose objectives rely on unknown nuisance parameters, and\nestablish non-asymptotic convergence guarantees. Our results show that, while\nthe presence of a nuisance can alter the optimum and upset the optimization\ntrajectory, the classical stochastic gradient algorithm may still converge\nunder appropriate conditions, such as Neyman orthogonality. Moreover, even when\nNeyman orthogonality is not satisfied, we show that an algorithm variant with\napproximately orthogonalized updates (with an approximately orthogonalized\ngradient oracle) may achieve similar convergence rates. Examples from\northogonal statistical learning/double machine learning and causal inference\nare discussed.",
        "published": "2025-08-28T00:07:40Z",
        "updated": "2025-08-28T00:07:40Z",
        "authors": [
            "Facheng Yu",
            "Ronak Mehta",
            "Alex Luedtke",
            "Zaid Harchaoui"
        ],
        "link": "http://arxiv.org/abs/2508.20326v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.20257v1",
        "title": "Discovering equations from data: symbolic regression in dynamical\n  systems",
        "abstract": "The process of discovering equations from data lies at the heart of physics\nand in many other areas of research, including mathematical ecology and\nepidemiology. Recently, machine learning methods known as symbolic regression\nhave automated this process. As several methods are available in the\nliterature, it is important to compare them, particularly for dynamic systems\nthat describe complex phenomena. In this paper, five symbolic regression\nmethods were used for recovering equations from nine dynamical processes,\nincluding chaotic dynamics and epidemic models, with the PySR method proving to\nbe the most suitable for inferring equations. Benchmark results demonstrate its\nhigh predictive power and accuracy, with some estimates being indistinguishable\nfrom the original analytical forms. These results highlight the potential of\nsymbolic regression as a robust tool for inferring and modelling real-world\nphenomena.",
        "published": "2025-08-27T20:30:09Z",
        "updated": "2025-08-27T20:30:09Z",
        "authors": [
            "Beatriz R. Brum",
            "Luiza Lober",
            "Isolde Previdelli",
            "Francisco A. Rodrigues"
        ],
        "link": "http://arxiv.org/abs/2508.20257v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.20067v1",
        "title": "Neural Conditional Simulation for Complex Spatial Processes",
        "abstract": "A key objective in spatial statistics is to simulate from the distribution of\na spatial process at a selection of unobserved locations conditional on\nobservations (i.e., a predictive distribution) to enable spatial prediction and\nuncertainty quantification. However, exact conditional simulation from this\npredictive distribution is intractable or inefficient for many spatial process\nmodels. In this paper, we propose neural conditional simulation (NCS), a\ngeneral method for spatial conditional simulation that is based on neural\ndiffusion models. Specifically, using spatial masks, we implement a conditional\nscore-based diffusion model that evolves Gaussian noise into samples from a\npredictive distribution when given a partially observed spatial field and\nspatial process parameters as inputs. The diffusion model relies on a neural\nnetwork that only requires unconditional samples from the spatial process for\ntraining. Once trained, the diffusion model is amortized with respect to the\nobservations in the partially observed field, the number and locations of those\nobservations, and the spatial process parameters, and can therefore be used to\nconditionally simulate from a broad class of predictive distributions without\nretraining the neural network. We assess the NCS-generated simulations against\nsimulations from the true conditional distribution of a Gaussian process model,\nand against Markov chain Monte Carlo (MCMC) simulations from a Brown--Resnick\nprocess model for spatial extremes. In the latter case, we show that it is more\nefficient and accurate to conditionally simulate using NCS than classical MCMC\ntechniques implemented in standard software. We conclude that NCS enables\nefficient and accurate conditional simulation from spatial predictive\ndistributions that are challenging to sample from using traditional methods.",
        "published": "2025-08-27T17:21:32Z",
        "updated": "2025-08-27T17:21:32Z",
        "authors": [
            "Julia Walchessen",
            "Andrew Zammit-Mangion",
            "Raphaël Huser",
            "Mikael Kuusela"
        ],
        "link": "http://arxiv.org/abs/2508.20067v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.20036v1",
        "title": "Eigenvalue distribution of the Neural Tangent Kernel in the quadratic\n  scaling",
        "abstract": "We compute the asymptotic eigenvalue distribution of the neural tangent\nkernel of a two-layer neural network under a specific scaling of dimension.\nNamely, if $X\\in\\mathbb{R}^{n\\times d}$ is an i.i.d random matrix,\n$W\\in\\mathbb{R}^{d\\times p}$ is an i.i.d $\\mathcal{N}(0,1)$ matrix and\n$D\\in\\mathbb{R}^{p\\times p}$ is a diagonal matrix with i.i.d bounded entries,\nwe consider the matrix\n  \\[\n  \\mathrm{NTK}\n  =\n  \\frac{1}{d}XX^\\top\n  \\odot\n  \\frac{1}{p}\n  \\sigma'\\left(\n  \\frac{1}{\\sqrt{d}}XW\n  \\right)D^2\n  \\sigma'\\left(\n  \\frac{1}{\\sqrt{d}}XW\n  \\right)^\\top\n  \\]\n  where $\\sigma'$ is a pseudo-Lipschitz function applied entrywise and under\nthe scaling $\\frac{n}{dp}\\to \\gamma_1$ and $\\frac{p}{d}\\to \\gamma_2$. We\ndescribe the asymptotic distribution as the free multiplicative convolution of\nthe Marchenko--Pastur distribution with a deterministic distribution depending\non $\\sigma$ and $D$.",
        "published": "2025-08-27T16:41:01Z",
        "updated": "2025-08-27T16:41:01Z",
        "authors": [
            "Lucas Benigni",
            "Elliot Paquette"
        ],
        "link": "http://arxiv.org/abs/2508.20036v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19914v1",
        "title": "The Next Layer: Augmenting Foundation Models with Structure-Preserving\n  and Attention-Guided Learning for Local Patches to Global Context Awareness\n  in Computational Pathology",
        "abstract": "Foundation models have recently emerged as powerful feature extractors in\ncomputational pathology, yet they typically omit mechanisms for leveraging the\nglobal spatial structure of tissues and the local contextual relationships\namong diagnostically relevant regions - key elements for understanding the\ntumor microenvironment. Multiple instance learning (MIL) remains an essential\nnext step following foundation model, designing a framework to aggregate\npatch-level features into slide-level predictions. We present EAGLE-Net, a\nstructure-preserving, attention-guided MIL architecture designed to augment\nprediction and interpretability. EAGLE-Net integrates multi-scale absolute\nspatial encoding to capture global tissue architecture, a top-K\nneighborhood-aware loss to focus attention on local microenvironments, and\nbackground suppression loss to minimize false positives. We benchmarked\nEAGLE-Net on large pan-cancer datasets, including three cancer types for\nclassification (10,260 slides) and seven cancer types for survival prediction\n(4,172 slides), using three distinct histology foundation backbones (REMEDIES,\nUni-V1, Uni2-h). Across tasks, EAGLE-Net achieved up to 3% higher\nclassification accuracy and the top concordance indices in 6 of 7 cancer types,\nproducing smooth, biologically coherent attention maps that aligned with expert\nannotations and highlighted invasive fronts, necrosis, and immune infiltration.\nThese results position EAGLE-Net as a generalizable, interpretable framework\nthat complements foundation models, enabling improved biomarker discovery,\nprognostic modeling, and clinical decision support",
        "published": "2025-08-27T14:19:38Z",
        "updated": "2025-08-27T14:19:38Z",
        "authors": [
            "Muhammad Waqas",
            "Rukhmini Bandyopadhyay",
            "Eman Showkatian",
            "Amgad Muneer",
            "Anas Zafar",
            "Frank Rojas Alvarez",
            "Maricel Corredor Marin",
            "Wentao Li",
            "David Jaffray",
            "Cara Haymaker",
            "John Heymach",
            "Natalie I Vokes",
            "Luisa Maren Solis Soto",
            "Jianjun Zhang",
            "Jia Wu"
        ],
        "link": "http://arxiv.org/abs/2508.19914v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19897v1",
        "title": "The Information Dynamics of Generative Diffusion",
        "abstract": "Generative diffusion models have emerged as a powerful class of models in\nmachine learning, yet a unified theoretical understanding of their operation is\nstill developing. This perspective paper provides an integrated perspective on\ngenerative diffusion by connecting their dynamic, information-theoretic, and\nthermodynamic properties under a unified mathematical framework. We demonstrate\nthat the rate of conditional entropy production during generation (i.e. the\ngenerative bandwidth) is directly governed by the expected divergence of the\nscore function's vector field. This divergence, in turn, is linked to the\nbranching of trajectories and generative bifurcations, which we characterize as\nsymmetry-breaking phase transitions in the energy landscape. This synthesis\noffers a powerful insight: the process of generation is fundamentally driven by\nthe controlled, noise-induced breaking of (approximate) symmetries, where peaks\nin information transfer correspond to critical transitions between possible\noutcomes. The score function acts as a dynamic non-linear filter that regulates\nthe bandwidth of the noise by suppressing fluctuations that are incompatible\nwith the data.",
        "published": "2025-08-27T13:53:56Z",
        "updated": "2025-08-27T13:53:56Z",
        "authors": [
            "Luca Ambrogioni"
        ],
        "link": "http://arxiv.org/abs/2508.19897v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19841v1",
        "title": "Conditional Normalizing Flow Surrogate for Monte Carlo Prediction of\n  Radiative Properties in Nanoparticle-Embedded Layers",
        "abstract": "We present a probabilistic, data-driven surrogate model for predicting the\nradiative properties of nanoparticle embedded scattering media. The model uses\nconditional normalizing flows, which learn the conditional distribution of\noptical outputs, including reflectance, absorbance, and transmittance, given\ninput parameters such as the absorption coefficient, scattering coefficient,\nanisotropy factor, and particle size distribution. We generate training data\nusing Monte Carlo radiative transfer simulations, with optical properties\nderived from Mie theory. Unlike conventional neural networks, the conditional\nnormalizing flow model yields full posterior predictive distributions, enabling\nboth accurate forecasts and principled uncertainty quantification. Our results\ndemonstrate that this model achieves high predictive accuracy and reliable\nuncertainty estimates, establishing it as a powerful and efficient surrogate\nfor radiative transfer simulations.",
        "published": "2025-08-27T12:54:06Z",
        "updated": "2025-08-27T12:54:06Z",
        "authors": [
            "Fahime Seyedheydari",
            "Kevin Conley",
            "Simo Särkkä"
        ],
        "link": "http://arxiv.org/abs/2508.19841v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19780v1",
        "title": "Interestingness First Classifiers",
        "abstract": "Most machine learning models are designed to maximize predictive accuracy. In\nthis work, we explore a different goal: building classifiers that are\ninteresting. An ``interesting classifier'' is one that uses unusual or\nunexpected features, even if its accuracy is lower than the best possible\nmodel. For example, predicting room congestion from CO2 levels achieves\nnear-perfect accuracy but is unsurprising. In contrast, predicting room\ncongestion from humidity is less accurate yet more nuanced and intriguing. We\nintroduce EUREKA, a simple framework that selects features according to their\nperceived interestingness. Our method leverages large language models to rank\nfeatures by their interestingness and then builds interpretable classifiers\nusing only the selected interesting features. Across several benchmark\ndatasets, EUREKA consistently identifies features that are non-obvious yet\nstill predictive. For example, in the Occupancy Detection dataset, our method\nfavors humidity over CO2 levels and light intensity, producing classifiers that\nachieve meaningful accuracy while offering insights. In the Twin Papers\ndataset, our method discovers the rule that papers with a colon in the title\nare more likely to be cited in the future. We argue that such models can\nsupport new ways of knowledge discovery and communication, especially in\nsettings where moderate accuracy is sufficient but novelty and interpretability\nare valued.",
        "published": "2025-08-27T11:04:05Z",
        "updated": "2025-08-27T11:04:05Z",
        "authors": [
            "Ryoma Sato"
        ],
        "link": "http://arxiv.org/abs/2508.19780v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19750v1",
        "title": "Fractal Flow: Hierarchical and Interpretable Normalizing Flow via Topic\n  Modeling and Recursive Strategy",
        "abstract": "Normalizing Flows provide a principled framework for high-dimensional density\nestimation and generative modeling by constructing invertible transformations\nwith tractable Jacobian determinants. We propose Fractal Flow, a novel\nnormalizing flow architecture that enhances both expressiveness and\ninterpretability through two key innovations. First, we integrate\nKolmogorov-Arnold Networks and incorporate Latent Dirichlet Allocation into\nnormalizing flows to construct a structured, interpretable latent space and\nmodel hierarchical semantic clusters. Second, inspired by Fractal Generative\nModels, we introduce a recursive modular design into normalizing flows to\nimprove transformation interpretability and estimation accuracy. Experiments on\nMNIST, FashionMNIST, CIFAR-10, and geophysical data demonstrate that the\nFractal Flow achieves latent clustering, controllable generation, and superior\nestimation accuracy.",
        "published": "2025-08-27T10:25:15Z",
        "updated": "2025-08-27T10:25:15Z",
        "authors": [
            "Binhui Zhang",
            "Jianwei Ma"
        ],
        "link": "http://arxiv.org/abs/2508.19750v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19563v2",
        "title": "Robustness is Important: Limitations of LLMs for Data Fitting",
        "abstract": "Large Language Models (LLMs) are being applied in a wide array of settings,\nwell beyond the typical language-oriented use cases. In particular, LLMs are\nincreasingly used as a plug-and-play method for fitting data and generating\npredictions. Prior work has shown that LLMs, via in-context learning or\nsupervised fine-tuning, can perform competitively with many tabular supervised\nlearning techniques in terms of predictive performance. However, we identify a\ncritical vulnerability of using LLMs for data fitting -- making changes to data\nrepresentation that are completely irrelevant to the underlying learning task\ncan drastically alter LLMs' predictions on the same data. For example, simply\nchanging variable names can sway the size of prediction error by as much as 82%\nin certain settings. Such prediction sensitivity with respect to\ntask-irrelevant variations manifests under both in-context learning and\nsupervised fine-tuning, for both close-weight and open-weight general-purpose\nLLMs. Moreover, by examining the attention scores of an open-weight LLM, we\ndiscover a non-uniform attention pattern: training examples and variable\nnames/values which happen to occupy certain positions in the prompt receive\nmore attention when output tokens are generated, even though different\npositions are expected to receive roughly the same attention. This partially\nexplains the sensitivity in the presence of task-irrelevant variations. We also\nconsider a state-of-the-art tabular foundation model (TabPFN) trained\nspecifically for data fitting. Despite being explicitly designed to achieve\nprediction robustness, TabPFN is still not immune to task-irrelevant\nvariations. Overall, despite LLMs' impressive predictive capabilities,\ncurrently they lack even the basic level of robustness to be used as a\nprincipled data-fitting tool.",
        "published": "2025-08-27T04:46:05Z",
        "updated": "2025-08-29T13:46:29Z",
        "authors": [
            "Hejia Liu",
            "Mochen Yang",
            "Gediminas Adomavicius"
        ],
        "link": "http://arxiv.org/abs/2508.19563v2",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19468v1",
        "title": "Weighted Levenberg-Marquardt methods for fitting multichannel nuclear\n  cross section data",
        "abstract": "We present an extension of the Levenberg-Marquardt algorithm for fitting\nmultichannel nuclear cross section data. Our approach offers a practical and\nrobust alternative to conventional trust-region methods for analyzing\nexperimental data. The CoH$_3$ code, based on the Hauser-Feshbach statistical\nmodel, involves a large number of interdependent parameters, making\noptimization challenging due to the presence of \"sloppy\" directions in\nparameter space. To address the uneven distribution of experimental data across\nreaction channels, we construct a weighted Fisher Information Metric by\nintegrating prior distributions over dataset weights. This framework enables a\nmore balanced treatment of heterogeneous data, improving both parameter\nestimation and convergence robustness. We show that the resulting weighted\nLevenberg-Marquardt method yields more physically consistent fits for both raw\nand smoothed datasets, using experimental data for ${}^{148}$Sm as a\nrepresentative example. Additionally, we introduce a geometric scaling strategy\nto accelerate convergence -- a method based on the local geometry of the\nmanifold.",
        "published": "2025-08-26T23:11:54Z",
        "updated": "2025-08-26T23:11:54Z",
        "authors": [
            "M. Imbrišak",
            "A. E. Lovell",
            "M. R. Mumpower"
        ],
        "link": "http://arxiv.org/abs/2508.19468v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.21086v1",
        "title": "Quantum-inspired probability metrics define a complete, universal space\n  for statistical learning",
        "abstract": "Comparing probability distributions is a core challenge across the natural,\nsocial, and computational sciences. Existing methods, such as Maximum Mean\nDiscrepancy (MMD), struggle in high-dimensional and non-compact domains. Here\nwe introduce quantum probability metrics (QPMs), derived by embedding\nprobability measures in the space of quantum states: positive, unit-trace\noperators on a Hilbert space. This construction extends kernel-based methods\nand overcomes the incompleteness of MMD on non-compact spaces. Viewed as an\nintegral probability metric (IPM), QPMs have dual functions that uniformly\napproximate all bounded, uniformly continuous functions on $\\mathbb{R}^n$,\noffering enhanced sensitivity to subtle distributional differences in high\ndimensions. For empirical distributions, QPMs are readily calculated using\neigenvalue methods, with analytic gradients suited for learning and\noptimization. Although computationally more intensive for large sample sizes\n($O(n^3)$ vs. $O(n^2)$), QPMs can significantly improve performance as a\ndrop-in replacement for MMD, as demonstrated in a classic generative modeling\ntask. By combining the rich mathematical framework of quantum mechanics with\nclassical probability theory, this approach lays the foundation for powerful\ntools to analyze and manipulate probability measures.",
        "published": "2025-08-26T22:41:48Z",
        "updated": "2025-08-26T22:41:48Z",
        "authors": [
            "Logan S. McCarty"
        ],
        "link": "http://arxiv.org/abs/2508.21086v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19458v1",
        "title": "The Sample Complexity of Membership Inference and Privacy Auditing",
        "abstract": "A membership-inference attack gets the output of a learning algorithm, and a\ntarget individual, and tries to determine whether this individual is a member\nof the training data or an independent sample from the same distribution. A\nsuccessful membership-inference attack typically requires the attacker to have\nsome knowledge about the distribution that the training data was sampled from,\nand this knowledge is often captured through a set of independent reference\nsamples from that distribution. In this work we study how much information the\nattacker needs for membership inference by investigating the sample\ncomplexity-the minimum number of reference samples required-for a successful\nattack. We study this question in the fundamental setting of Gaussian mean\nestimation where the learning algorithm is given $n$ samples from a Gaussian\ndistribution $\\mathcal{N}(\\mu,\\Sigma)$ in $d$ dimensions, and tries to estimate\n$\\hat\\mu$ up to some error $\\mathbb{E}[\\|\\hat \\mu - \\mu\\|^2_{\\Sigma}]\\leq\n\\rho^2 d$. Our result shows that for membership inference in this setting,\n$\\Omega(n + n^2 \\rho^2)$ samples can be necessary to carry out any attack that\ncompetes with a fully informed attacker. Our result is the first to show that\nthe attacker sometimes needs many more samples than the training algorithm uses\nto train the model. This result has significant implications for practice, as\nall attacks used in practice have a restricted form that uses $O(n)$ samples\nand cannot benefit from $\\omega(n)$ samples. Thus, these attacks may be\nunderestimating the possibility of membership inference, and better attacks may\nbe possible when information about the distribution is easy to obtain.",
        "published": "2025-08-26T22:19:28Z",
        "updated": "2025-08-26T22:19:28Z",
        "authors": [
            "Mahdi Haghifam",
            "Adam Smith",
            "Jonathan Ullman"
        ],
        "link": "http://arxiv.org/abs/2508.19458v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19448v1",
        "title": "Reduced-Order Modeling of Cyclo-Stationary Time Series Using Score-Based\n  Generative Methods",
        "abstract": "Many natural systems exhibit cyclo-stationary behavior characterized by\nperiodic forcing such as annual and diurnal cycles. We present a data-driven\nmethod leveraging recent advances in score-based generative modeling to\nconstruct reduced-order models for such cyclo-stationary time series. Our\napproach accurately reproduces the statistical properties and temporal\ncorrelations of the original data, enabling efficient generation of synthetic\ntrajectories. We demonstrate the performance of the method through application\nto the Planet Simulator (PlaSim) climate model, constructing a reduced-order\nmodel for the 20 leading principal components of surface temperature driven by\nthe annual cycle. The resulting surrogate model accurately reproduces the\nmarginal and joint probability distributions, autocorrelation functions, and\nspatial coherence of the original climate system across multiple validation\nmetrics. The approach offers substantial computational advantages, enabling\ngeneration of centuries of synthetic climate data in minutes compared to weeks\nrequired for equivalent full model simulations. This work opens new\npossibilities for efficient modeling of periodically forced systems across\ndiverse scientific domains, providing a principled framework for balancing\ncomputational efficiency with physical fidelity in reduced-order modeling\napplications.",
        "published": "2025-08-26T21:49:48Z",
        "updated": "2025-08-26T21:49:48Z",
        "authors": [
            "Ludovico Theo Giorgini",
            "Tobias Bischoff",
            "Andre Noguiera Souza"
        ],
        "link": "http://arxiv.org/abs/2508.19448v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19445v1",
        "title": "On Surjectivity of Neural Networks: Can you elicit any behavior from\n  your model?",
        "abstract": "Given a trained neural network, can any specified output be generated by some\ninput? Equivalently, does the network correspond to a function that is\nsurjective? In generative models, surjectivity implies that any output,\nincluding harmful or undesirable content, can in principle be generated by the\nnetworks, raising concerns about model safety and jailbreak vulnerabilities. In\nthis paper, we prove that many fundamental building blocks of modern neural\narchitectures, such as networks with pre-layer normalization and\nlinear-attention modules, are almost always surjective. As corollaries, widely\nused generative frameworks, including GPT-style transformers and diffusion\nmodels with deterministic ODE solvers, admit inverse mappings for arbitrary\noutputs. By studying surjectivity of these modern and commonly used neural\narchitectures, we contribute a formalism that sheds light on their unavoidable\nvulnerability to a broad class of adversarial attacks.",
        "published": "2025-08-26T21:36:45Z",
        "updated": "2025-08-26T21:36:45Z",
        "authors": [
            "Haozhe Jiang",
            "Nika Haghtalab"
        ],
        "link": "http://arxiv.org/abs/2508.19445v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19441v1",
        "title": "Data-Augmented Few-Shot Neural Stencil Emulation for System\n  Identification of Computer Models",
        "abstract": "Partial differential equations (PDEs) underpin the modeling of many natural\nand engineered systems. It can be convenient to express such models as neural\nPDEs rather than using traditional numerical PDE solvers by replacing part or\nall of the PDE's governing equations with a neural network representation.\nNeural PDEs are often easier to differentiate, linearize, reduce, or use for\nuncertainty quantification than the original numerical solver. They are usually\ntrained on solution trajectories obtained by long time integration of the PDE\nsolver. Here we propose a more sample-efficient data-augmentation strategy for\ngenerating neural PDE training data from a computer model by space-filling\nsampling of local \"stencil\" states. This approach removes a large degree of\nspatiotemporal redundancy present in trajectory data and oversamples states\nthat may be rarely visited but help the neural PDE generalize across the state\nspace. We demonstrate that accurate neural PDE stencil operators can be learned\nfrom synthetic training data generated by the computational equivalent of 10\ntimesteps' worth of numerical simulation. Accuracy is further improved if we\nassume access to a single full-trajectory simulation from the computer model,\nwhich is typically available in practice. Across several PDE systems, we show\nthat our data-augmented synthetic stencil data yield better trained neural\nstencil operators, with clear performance gains compared with naively sampled\nstencil data from simulation trajectories.",
        "published": "2025-08-26T21:22:11Z",
        "updated": "2025-08-26T21:22:11Z",
        "authors": [
            "Sanket Jantre",
            "Deepak Akhare",
            "Xiaoning Qian",
            "Nathan M. Urban"
        ],
        "link": "http://arxiv.org/abs/2508.19441v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19201v1",
        "title": "Understanding Tool-Integrated Reasoning",
        "abstract": "We study why Tool-Integrated Reasoning (TIR) makes Large Language Models\n(LLMs) more capable. While LLMs integrated with tools like Python code\ninterpreters show great promise, a principled theory explaining why this\nparadigm is effective has been missing. This work provides the first formal\nproof that TIR fundamentally expands an LLM's capabilities. We demonstrate that\ntools enable a strict expansion of the model's empirical and feasible support,\nbreaking the capability ceiling of pure-text models by unlocking\nproblem-solving strategies that are otherwise impossible or intractably\nverbose. To guide model behavior without compromising training stability and\nperformance, we also introduce Advantage Shaping Policy Optimization (ASPO), a\nnovel algorithm that directly modifies the advantage function to guide the\npolicy behavior. We conduct comprehensive experiments on challenging\nmathematical benchmarks, leveraging a Python interpreter as the external tool.\nOur results show that the TIR model decisively outperforms its pure-text\ncounterpart on the pass@k metric. Crucially, this advantage is not confined to\ncomputationally-intensive problems but extends to those requiring significant\nabstract insight. We further identify the emergent cognitive patterns that\nillustrate how models learn to think with tools. Finally, we report improved\ntool usage behavior with early code invocation and much more interactive turns\nwith ASPO. Overall, our work provides the first principled explanation for\nTIR's success, shifting the focus from the mere fact that tools work to why and\nhow they enable more powerful reasoning.",
        "published": "2025-08-26T17:03:46Z",
        "updated": "2025-08-26T17:03:46Z",
        "authors": [
            "Heng Lin",
            "Zhongwen Xu"
        ],
        "link": "http://arxiv.org/abs/2508.19201v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19145v1",
        "title": "Echoes of the past: A unified perspective on fading memory and echo\n  states",
        "abstract": "Recurrent neural networks (RNNs) have become increasingly popular in\ninformation processing tasks involving time series and temporal data. A\nfundamental property of RNNs is their ability to create reliable input/output\nresponses, often linked to how the network handles its memory of the\ninformation it processed. Various notions have been proposed to conceptualize\nthe behavior of memory in RNNs, including steady states, echo states, state\nforgetting, input forgetting, and fading memory. Although these notions are\noften used interchangeably, their precise relationships remain unclear. This\nwork aims to unify these notions in a common language, derive new implications\nand equivalences between them, and provide alternative proofs to some existing\nresults. By clarifying the relationships between these concepts, this research\ncontributes to a deeper understanding of RNNs and their temporal information\nprocessing capabilities.",
        "published": "2025-08-26T15:55:14Z",
        "updated": "2025-08-26T15:55:14Z",
        "authors": [
            "Juan-Pablo Ortega",
            "Florian Rossmannek"
        ],
        "link": "http://arxiv.org/abs/2508.19145v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.19104v1",
        "title": "Composition and Alignment of Diffusion Models using Constrained Learning",
        "abstract": "Diffusion models have become prevalent in generative modeling due to their\nability to sample from complex distributions. To improve the quality of\ngenerated samples and their compliance with user requirements, two commonly\nused methods are: (i) Alignment, which involves fine-tuning a diffusion model\nto align it with a reward; and (ii) Composition, which combines several\npre-trained diffusion models, each emphasizing a desirable attribute in the\ngenerated outputs. However, trade-offs often arise when optimizing for multiple\nrewards or combining multiple models, as they can often represent competing\nproperties. Existing methods cannot guarantee that the resulting model\nfaithfully generates samples with all the desired properties. To address this\ngap, we propose a constrained optimization framework that unifies alignment and\ncomposition of diffusion models by enforcing that the aligned model satisfies\nreward constraints and/or remains close to (potentially multiple) pre-trained\nmodels. We provide a theoretical characterization of the solutions to the\nconstrained alignment and composition problems and develop a Lagrangian-based\nprimal-dual training algorithm to approximate these solutions. Empirically, we\ndemonstrate the effectiveness and merits of our proposed approach in image\ngeneration, applying it to alignment and composition, and show that our aligned\nor composed model satisfies constraints effectively, and improves on the\nequally-weighted approach. Our implementation can be found at\nhttps://github.com/shervinkhalafi/constrained_comp_align.",
        "published": "2025-08-26T15:06:30Z",
        "updated": "2025-08-26T15:06:30Z",
        "authors": [
            "Shervin Khalafi",
            "Ignacio Hounie",
            "Dongsheng Ding",
            "Alejandro Ribeiro"
        ],
        "link": "http://arxiv.org/abs/2508.19104v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.18948v1",
        "title": "The GINN framework: a stochastic QED correspondence for stability and\n  chaos in deep neural networks",
        "abstract": "The development of a Euclidean stochastic field-theoretic approach that maps\ndeep neural networks (DNNs) to quantum electrodynamics (QED) with local U(1)\nsymmetry is presented. Neural activations and weights are represented by\nfermionic matter and gauge fields, with a fictitious Langevin time enabling\ncovariant gauge fixing. This mapping identifies the gauge parameter with kernel\ndesign choices in wide DNNs, relating stability thresholds to gauge-dependent\namplification factors. Finite-width fluctuations correspond to loop corrections\nin QED. As a proof of concept, we validate the theoretical predictions through\nnumerical simulations of standard multilayer perceptrons and, in parallel,\npropose a gauge-invariant neural network (GINN) implementation using\nmagnitude--phase parameterization of weights. Finally, a double-copy replica\napproach is shown to unify the computation of the largest Lyapunov exponent in\nstochastic QED and wide DNNs.",
        "published": "2025-08-26T11:41:11Z",
        "updated": "2025-08-26T11:41:11Z",
        "authors": [
            "Rodrigo Carmo Terin"
        ],
        "link": "http://arxiv.org/abs/2508.18948v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.18901v1",
        "title": "Sparse minimum Redundancy Maximum Relevance for feature selection",
        "abstract": "We propose a feature screening method that integrates both feature-feature\nand feature-target relationships. Inactive features are identified via a\npenalized minimum Redundancy Maximum Relevance (mRMR) procedure, which is the\ncontinuous version of the classic mRMR penalized by a non-convex regularizer,\nand where the parameters estimated as zero coefficients represent the set of\ninactive features. We establish the conditions under which zero coefficients\nare correctly identified to guarantee accurate recovery of inactive features.\nWe introduce a multi-stage procedure based on the knockoff filter enabling the\npenalized mRMR to discard inactive features while controlling the false\ndiscovery rate (FDR). Our method performs comparably to HSIC-LASSO but is more\nconservative in the number of selected features. It only requires setting an\nFDR threshold, rather than specifying the number of features to retain. The\neffectiveness of the method is illustrated through simulations and real-world\ndatasets. The code to reproduce this work is available on the following GitHub:\nhttps://github.com/PeterJackNaylor/SmRMR.",
        "published": "2025-08-26T10:18:23Z",
        "updated": "2025-08-26T10:18:23Z",
        "authors": [
            "Peter Naylor",
            "Benjamin Poignard",
            "Héctor Climente-González",
            "Makoto Yamada"
        ],
        "link": "http://arxiv.org/abs/2508.18901v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.18774v1",
        "title": "Federated Learning with Heterogeneous and Private Label Sets",
        "abstract": "Although common in real-world applications, heterogeneous client label sets\nare rarely investigated in federated learning (FL). Furthermore, in the cases\nthey are, clients are assumed to be willing to share their entire label sets\nwith other clients. Federated learning with private label sets, shared only\nwith the central server, adds further constraints on learning algorithms and\nis, in general, a more difficult problem to solve. In this work, we study the\neffects of label set heterogeneity on model performance, comparing the public\nand private label settings -- when the union of label sets in the federation is\nknown to clients and when it is not. We apply classical methods for the\nclassifier combination problem to FL using centralized tuning, adapt common FL\nmethods to the private label set setting, and discuss the justification of both\napproaches under practical assumptions. Our experiments show that reducing the\nnumber of labels available to each client harms the performance of all methods\nsubstantially. Centralized tuning of client models for representational\nalignment can help remedy this, but often at the cost of higher variance.\nThroughout, our proposed adaptations of standard FL methods perform well,\nshowing similar performance in the private label setting as the standard\nmethods achieve in the public setting. This shows that clients can enjoy\nincreased privacy at little cost to model accuracy.",
        "published": "2025-08-26T07:57:36Z",
        "updated": "2025-08-26T07:57:36Z",
        "authors": [
            "Adam Breitholtz",
            "Edvin Listo Zec",
            "Fredrik D. Johansson"
        ],
        "link": "http://arxiv.org/abs/2508.18774v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.18768v1",
        "title": "Efficient Best-of-Both-Worlds Algorithms for Contextual Combinatorial\n  Semi-Bandits",
        "abstract": "We introduce the first best-of-both-worlds algorithm for contextual\ncombinatorial semi-bandits that simultaneously guarantees\n$\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret in the adversarial regime and\n$\\widetilde{\\mathcal{O}}(\\ln T)$ regret in the corrupted stochastic regime. Our\napproach builds on the Follow-the-Regularized-Leader (FTRL) framework equipped\nwith a Shannon entropy regularizer, yielding a flexible method that admits\nefficient implementations. Beyond regret bounds, we tackle the practical\nbottleneck in FTRL (or, equivalently, Online Stochastic Mirror Descent) arising\nfrom the high-dimensional projection step encountered in each round of\ninteraction. By leveraging the Karush-Kuhn-Tucker conditions, we transform the\n$K$-dimensional convex projection problem into a single-variable root-finding\nproblem, dramatically accelerating each round. Empirical evaluations\ndemonstrate that this combined strategy not only attains the attractive regret\nbounds of best-of-both-worlds algorithms but also delivers substantial\nper-round speed-ups, making it well-suited for large-scale, real-time\napplications.",
        "published": "2025-08-26T07:51:22Z",
        "updated": "2025-08-26T07:51:22Z",
        "authors": [
            "Mengmeng Li",
            "Philipp Schneider",
            "Jelisaveta Aleksić",
            "Daniel Kuhn"
        ],
        "link": "http://arxiv.org/abs/2508.18768v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.18698v1",
        "title": "Lightweight posterior construction for gravitational-wave catalogs with\n  the Kolmogorov-Arnold network",
        "abstract": "Neural density estimation has seen widespread applications in the\ngravitational-wave (GW) data analysis, which enables real-time parameter\nestimation for compact binary coalescences and enhances rapid inference for\nsubsequent analysis such as population inference. In this work, we explore the\napplication of using the Kolmogorov-Arnold network (KAN) to construct efficient\nand interpretable neural density estimators for lightweight posterior\nconstruction of GW catalogs. By replacing conventional activation functions\nwith learnable splines, KAN achieves superior interpretability, higher\naccuracy, and greater parameter efficiency on related scientific tasks.\nLeveraging this feature, we propose a KAN-based neural density estimator, which\ningests megabyte-scale GW posterior samples and compresses them into model\nweights of tens of kilobytes. Subsequently, analytic expressions requiring only\nseveral kilobytes can be further distilled from these neural network weights\nwith minimal accuracy trade-off. In practice, GW posterior samples with\nfidelity can be regenerated rapidly using the model weights or analytic\nexpressions for subsequent analysis. Our lightweight posterior construction\nstrategy is expected to facilitate user-level data storage and transmission,\npaving a path for efficient analysis of numerous GW events in the\nnext-generation GW detectors.",
        "published": "2025-08-26T06:00:27Z",
        "updated": "2025-08-26T06:00:27Z",
        "authors": [
            "Wenshuai Liu",
            "Yiming Dong",
            "Ziming Wang",
            "Lijing Shao"
        ],
        "link": "http://arxiv.org/abs/2508.18698v1",
        "source_category": "arxiv"
    },
    {
        "id": "http://arxiv.org/abs/2508.18604v1",
        "title": "Revisiting Follow-the-Perturbed-Leader with Unbounded Perturbations in\n  Bandit Problems",
        "abstract": "Follow-the-Regularized-Leader (FTRL) policies have achieved\nBest-of-Both-Worlds (BOBW) results in various settings through hybrid\nregularizers, whereas analogous results for Follow-the-Perturbed-Leader (FTPL)\nremain limited due to inherent analytical challenges. To advance the analytical\nfoundations of FTPL, we revisit classical FTRL-FTPL duality for unbounded\nperturbations and establish BOBW results for FTPL under a broad family of\nasymmetric unbounded Fr\\'echet-type perturbations, including hybrid\nperturbations combining Gumbel-type and Fr\\'echet-type tails. These results not\nonly extend the BOBW results of FTPL but also offer new insights into designing\nalternative FTPL policies competitive with hybrid regularization approaches.\nMotivated by earlier observations in two-armed bandits, we further investigate\nthe connection between the $1/2$-Tsallis entropy and a Fr\\'echet-type\nperturbation. Our numerical observations suggest that it corresponds to a\nsymmetric Fr\\'echet-type perturbation, and based on this, we establish the\nfirst BOBW guarantee for symmetric unbounded perturbations in the two-armed\nsetting. In contrast, in general multi-armed bandits, we find an instance in\nwhich symmetric Fr\\'echet-type perturbations violate the key condition for\nstandard BOBW analysis, which is a problem not observed with asymmetric or\nnonnegative Fr\\'echet-type perturbations. Although this example does not rule\nout alternative analyses achieving BOBW results, it suggests the limitations of\ndirectly applying the relationship observed in two-armed cases to the general\ncase and thus emphasizes the need for further investigation to fully understand\nthe behavior of FTPL in broader settings.",
        "published": "2025-08-26T02:12:18Z",
        "updated": "2025-08-26T02:12:18Z",
        "authors": [
            "Jongyeong Lee",
            "Junya Honda",
            "Shinji Ito",
            "Min-hwan Oh"
        ],
        "link": "http://arxiv.org/abs/2508.18604v1",
        "source_category": "arxiv"
    },
    {
        "id": "uxlfoundation/oneDAL",
        "title": "oneDAL",
        "abstract": "oneAPI Data Analytics Library (oneDAL)",
        "published": "2016-03-28T22:39:32Z",
        "updated": "2025-09-02T07:18:13Z",
        "authors": [
            "uxlfoundation"
        ],
        "link": "https://github.com/uxlfoundation/oneDAL",
        "source_category": "github"
    },
    {
        "id": "Qyxxlor/StockSharp-Multi-Asset-Trading",
        "title": "StockSharp-Multi-Asset-Trading",
        "abstract": "A simple app for tracking monthly spending and working towards financial goals.",
        "published": "2025-07-24T20:52:01Z",
        "updated": "2025-09-02T07:18:11Z",
        "authors": [
            "Qyxxlor"
        ],
        "link": "https://github.com/Qyxxlor/StockSharp-Multi-Asset-Trading",
        "source_category": "github"
    },
    {
        "id": "nietiene/Machine-Learning-with-numPy",
        "title": "Machine-Learning-with-numPy",
        "abstract": "this repo contain codes about numPy with is mostly used in ML",
        "published": "2025-08-26T06:11:52Z",
        "updated": "2025-09-02T07:17:50Z",
        "authors": [
            "nietiene"
        ],
        "link": "https://github.com/nietiene/Machine-Learning-with-numPy",
        "source_category": "github"
    },
    {
        "id": "rickiepark/python-machine-learning-book-3rd-edition",
        "title": "python-machine-learning-book-3rd-edition",
        "abstract": "<머신 러닝 교과서 3판>의 코드 저장소",
        "published": "2020-09-08T00:50:23Z",
        "updated": "2025-09-02T07:17:06Z",
        "authors": [
            "rickiepark"
        ],
        "link": "https://github.com/rickiepark/python-machine-learning-book-3rd-edition",
        "source_category": "github"
    },
    {
        "id": "NatLibFi/Annif",
        "title": "Annif",
        "abstract": "Annif is a multi-algorithm automated subject indexing tool for libraries, archives and museums.",
        "published": "2017-08-21T09:36:52Z",
        "updated": "2025-09-02T07:16:44Z",
        "authors": [
            "NatLibFi"
        ],
        "link": "https://github.com/NatLibFi/Annif",
        "source_category": "github"
    },
    {
        "id": "Vulthen/myhhub-stock-analysis-suite-toolkit",
        "title": "myhhub-stock-analysis-suite-toolkit",
        "abstract": "Dot Ledger is a Free and Open Source personal finance management tool.",
        "published": "2025-06-17T21:09:44Z",
        "updated": "2025-09-02T07:16:08Z",
        "authors": [
            "Vulthen"
        ],
        "link": "https://github.com/Vulthen/myhhub-stock-analysis-suite-toolkit",
        "source_category": "github"
    },
    {
        "id": "pytorch/pytorch",
        "title": "pytorch",
        "abstract": "Tensors and Dynamic neural networks in Python with strong GPU acceleration",
        "published": "2016-08-13T05:26:41Z",
        "updated": "2025-09-02T07:16:08Z",
        "authors": [
            "pytorch"
        ],
        "link": "https://github.com/pytorch/pytorch",
        "source_category": "github"
    },
    {
        "id": "Vixoq/vnpy-Machine-Learning",
        "title": "vnpy-Machine-Learning",
        "abstract": "A simple and secure money manager that keeps you financially vigilant.",
        "published": "2025-06-17T20:42:01Z",
        "updated": "2025-09-02T07:15:26Z",
        "authors": [
            "Vixoq"
        ],
        "link": "https://github.com/Vixoq/vnpy-Machine-Learning",
        "source_category": "github"
    },
    {
        "id": "basf/mlipx",
        "title": "mlipx",
        "abstract": "Machine-Learned Interatomic Potential eXploration (mlipx) is designed at BASF for evaluating machine-learned interatomic potentials (MLIPs). It offers a growing set of evaluation methods alongside powerful visualization and comparison tools.",
        "published": "2024-10-18T08:48:26Z",
        "updated": "2025-09-02T07:12:10Z",
        "authors": [
            "basf"
        ],
        "link": "https://github.com/basf/mlipx",
        "source_category": "github"
    },
    {
        "id": "xieguigang/sciBASIC",
        "title": "sciBASIC",
        "abstract": "sciBASIC# is a kind of dialect language which is derive from the native VB.NET language, and written for the data scientist.",
        "published": "2016-01-05T15:51:04Z",
        "updated": "2025-09-02T07:12:12Z",
        "authors": [
            "xieguigang"
        ],
        "link": "https://github.com/xieguigang/sciBASIC",
        "source_category": "github"
    },
    {
        "id": "Phylmox/OpenBB-finance",
        "title": "OpenBB-finance",
        "abstract": "Investment Research for Everyone, Everywhere.",
        "published": "2025-06-17T21:28:14Z",
        "updated": "2025-09-02T07:12:03Z",
        "authors": [
            "Phylmox"
        ],
        "link": "https://github.com/Phylmox/OpenBB-finance",
        "source_category": "github"
    },
    {
        "id": "kherrick/news-summary",
        "title": "news-summary",
        "abstract": "A variety of tech related news summarized regularly.",
        "published": "2025-01-25T01:42:02Z",
        "updated": "2025-09-02T07:11:46Z",
        "authors": [
            "kherrick"
        ],
        "link": "https://github.com/kherrick/news-summary",
        "source_category": "github"
    },
    {
        "id": "Juan-Nathan/wvs-confidence-analysis",
        "title": "wvs-confidence-analysis",
        "abstract": "Analysis of World Values Survey (WVS) data in R to model predictors of confidence in social organizations using linear regression and clustering, with a focus on Romania.",
        "published": "2025-08-02T07:06:35Z",
        "updated": "2025-09-02T07:11:43Z",
        "authors": [
            "Juan-Nathan"
        ],
        "link": "https://github.com/Juan-Nathan/wvs-confidence-analysis",
        "source_category": "github"
    },
    {
        "id": "root-project/root",
        "title": "root",
        "abstract": "The official repository for ROOT: analyzing, storing and visualizing big data, scientifically",
        "published": "2013-06-27T10:46:59Z",
        "updated": "2025-09-02T07:11:37Z",
        "authors": [
            "root-project"
        ],
        "link": "https://github.com/root-project/root",
        "source_category": "github"
    },
    {
        "id": "jeffthedeveloper/jeffthedeveloper",
        "title": "jeffthedeveloper",
        "abstract": "Transforming complex data into production-grade AI solutions. Expert in Python, MLOps pipelines, and full-stack data application development.",
        "published": "2025-03-02T18:54:47Z",
        "updated": "2025-09-02T07:10:30Z",
        "authors": [
            "jeffthedeveloper"
        ],
        "link": "https://github.com/jeffthedeveloper/jeffthedeveloper",
        "source_category": "github"
    },
    {
        "id": "huggingface/huggingface.js",
        "title": "huggingface.js",
        "abstract": "Use Hugging Face with JavaScript",
        "published": "2023-02-06T18:37:09Z",
        "updated": "2025-09-02T07:10:24Z",
        "authors": [
            "huggingface"
        ],
        "link": "https://github.com/huggingface/huggingface.js",
        "source_category": "github"
    },
    {
        "id": "Hexroth/juspay-hyperswitch-realtime-dashboard",
        "title": "juspay-hyperswitch-realtime-dashboard",
        "abstract": "Rescript powered React library for seamless payment integration and customization.",
        "published": "2025-06-17T20:44:26Z",
        "updated": "2025-09-02T07:09:55Z",
        "authors": [
            "Hexroth"
        ],
        "link": "https://github.com/Hexroth/juspay-hyperswitch-realtime-dashboard",
        "source_category": "github"
    },
    {
        "id": "SMTorg/smt",
        "title": "smt",
        "abstract": "Surrogate Modeling Toolbox",
        "published": "2016-11-08T23:00:31Z",
        "updated": "2025-09-02T07:09:36Z",
        "authors": [
            "SMTorg"
        ],
        "link": "https://github.com/SMTorg/smt",
        "source_category": "github"
    },
    {
        "id": "cs-internship/cs-internship-spec",
        "title": "cs-internship-spec",
        "abstract": "Specifications for the CS Internship program.",
        "published": "2019-06-17T23:40:33Z",
        "updated": "2025-09-02T07:09:32Z",
        "authors": [
            "cs-internship"
        ],
        "link": "https://github.com/cs-internship/cs-internship-spec",
        "source_category": "github"
    },
    {
        "id": "pytorch/executorch",
        "title": "executorch",
        "abstract": "On-device AI across mobile, embedded and edge for PyTorch",
        "published": "2022-02-25T17:58:31Z",
        "updated": "2025-09-02T07:07:43Z",
        "authors": [
            "pytorch"
        ],
        "link": "https://github.com/pytorch/executorch",
        "source_category": "github"
    },
    {
        "id": "pytorch/executorch",
        "title": "executorch",
        "abstract": "On-device AI across mobile, embedded and edge for PyTorch",
        "published": "2022-02-25T17:58:31Z",
        "updated": "2025-09-02T07:07:43Z",
        "authors": [
            "pytorch"
        ],
        "link": "https://github.com/pytorch/executorch",
        "source_category": "github"
    },
    {
        "id": "CliMA/Oceananigans.jl",
        "title": "Oceananigans.jl",
        "abstract": "🌊  Julia software for fast, friendly, flexible, ocean-flavored fluid dynamics on CPUs and GPUs",
        "published": "2018-10-13T14:15:44Z",
        "updated": "2025-09-02T07:07:07Z",
        "authors": [
            "CliMA"
        ],
        "link": "https://github.com/CliMA/Oceananigans.jl",
        "source_category": "github"
    },
    {
        "id": "anhnt02hp/CS229-Fall2018-FullCourse",
        "title": "CS229-Fall2018-FullCourse",
        "abstract": "Complete solutions to all CS229 problem sets. Written from Python with detailed explanations.",
        "published": "2025-03-16T15:36:43Z",
        "updated": "2025-09-02T07:05:43Z",
        "authors": [
            "anhnt02hp"
        ],
        "link": "https://github.com/anhnt02hp/CS229-Fall2018-FullCourse",
        "source_category": "github"
    },
    {
        "id": "PaddlePaddle/Paddle",
        "title": "Paddle",
        "abstract": "PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice （『飞桨』核心框架，深度学习&机器学习高性能单机、分布式训练和跨平台部署）",
        "published": "2016-08-15T06:59:08Z",
        "updated": "2025-09-02T07:12:01Z",
        "authors": [
            "PaddlePaddle"
        ],
        "link": "https://github.com/PaddlePaddle/Paddle",
        "source_category": "github"
    },
    {
        "id": "Calaestivox/Ai-Crypto-Trading-Bot-Automated-Bitcoin-Cryptocurrency",
        "title": "Ai-Crypto-Trading-Bot-Automated-Bitcoin-Cryptocurrency",
        "abstract": "This repository offers an AI-powered crypto trading bot designed for automated trading of Bitcoin and other cryptocurrencies. It provides tools and algorithms for efficient and intelligent trading, maximizing profit and minimizing risk.",
        "published": "2025-07-24T21:03:03Z",
        "updated": "2025-09-02T06:58:59Z",
        "authors": [
            "Calaestivox"
        ],
        "link": "https://github.com/Calaestivox/Ai-Crypto-Trading-Bot-Automated-Bitcoin-Cryptocurrency",
        "source_category": "github"
    },
    {
        "id": "Krexind/quant-trading-toolkit",
        "title": "quant-trading-toolkit",
        "abstract": "Open-source personal finance tracking web application powered by ChatGPT.",
        "published": "2025-06-17T20:51:58Z",
        "updated": "2025-09-02T06:58:47Z",
        "authors": [
            "Krexind"
        ],
        "link": "https://github.com/Krexind/quant-trading-toolkit",
        "source_category": "github"
    },
    {
        "id": "vorxiiDav/Krux-Binance-Trading-Bot-Futures-Crypto-Coins-Volatility",
        "title": "Krux-Binance-Trading-Bot-Futures-Crypto-Coins-Volatility",
        "abstract": "This repository provides Krux, a trading bot for Binance focused on futures trading of cryptocurrencies. It is designed to handle market volatility effectively, leveraging advanced algorithms to optimize trading strategies and maximize profits.",
        "published": "2025-07-25T18:01:47Z",
        "updated": "2025-09-02T06:58:45Z",
        "authors": [
            "vorxiiDav"
        ],
        "link": "https://github.com/vorxiiDav/Krux-Binance-Trading-Bot-Futures-Crypto-Coins-Volatility",
        "source_category": "github"
    },
    {
        "id": "Grulmex/UFund-Me-Qbot-smart-routing",
        "title": "UFund-Me-Qbot-smart-routing",
        "abstract": "Extremely simple, self-hosted expense tracker with a beautiful UI.",
        "published": "2025-06-17T21:09:37Z",
        "updated": "2025-09-02T06:58:04Z",
        "authors": [
            "Grulmex"
        ],
        "link": "https://github.com/Grulmex/UFund-Me-Qbot-smart-routing",
        "source_category": "github"
    },
    {
        "id": "YaronKoresh/definers",
        "title": "definers",
        "abstract": "A comprehensive Python toolkit for AI, data processing, media manipulation, and system utilities.",
        "published": "2025-08-31T17:02:17Z",
        "updated": "2025-09-02T06:58:00Z",
        "authors": [
            "YaronKoresh"
        ],
        "link": "https://github.com/YaronKoresh/definers",
        "source_category": "github"
    },
    {
        "id": "Calaestivox/Juno-Binance-Trade-Bot-Automated-Cryptocurrency-Margin-Algorithmic",
        "title": "Juno-Binance-Trade-Bot-Automated-Cryptocurrency-Margin-Algorithmic",
        "abstract": "This repository features Juno, an automated trade bot for Binance, designed for margin trading of cryptocurrencies. It utilizes advanced algorithmic strategies to optimize trading decisions and enhance profitability.",
        "published": "2025-07-24T21:03:02Z",
        "updated": "2025-09-02T06:57:59Z",
        "authors": [
            "Calaestivox"
        ],
        "link": "https://github.com/Calaestivox/Juno-Binance-Trade-Bot-Automated-Cryptocurrency-Margin-Algorithmic",
        "source_category": "github"
    },
    {
        "id": "Driztek/Maybe-Finance-Dashboard",
        "title": "Maybe-Finance-Dashboard",
        "abstract": "Money Manager Ex is an easy to use, money management application built",
        "published": "2025-06-17T21:09:38Z",
        "updated": "2025-09-02T06:57:54Z",
        "authors": [
            "Driztek"
        ],
        "link": "https://github.com/Driztek/Maybe-Finance-Dashboard",
        "source_category": "github"
    },
    {
        "id": "Thyznol/firefly-iii-Pico-Data-Importer",
        "title": "firefly-iii-Pico-Data-Importer",
        "abstract": "The Firefly III Data Importer can import data into Firefly, Automatically categorize your expenses using OpenAI",
        "published": "2025-06-17T21:28:13Z",
        "updated": "2025-09-02T06:57:39Z",
        "authors": [
            "Thyznol"
        ],
        "link": "https://github.com/Thyznol/firefly-iii-Pico-Data-Importer",
        "source_category": "github"
    },
    {
        "id": "brain4j-org/brain4j",
        "title": "brain4j",
        "abstract": "Open-source machine learning framework for Java. Designed with speed and lightweight in mind.",
        "published": "2024-07-28T11:54:08Z",
        "updated": "2025-09-02T06:57:02Z",
        "authors": [
            "brain4j-org"
        ],
        "link": "https://github.com/brain4j-org/brain4j",
        "source_category": "github"
    },
    {
        "id": "1minds3t/omnipkg",
        "title": "omnipkg",
        "abstract": "One environment. Infinite packages. Zero conflicts.",
        "published": "2025-08-04T18:38:19Z",
        "updated": "2025-09-02T06:54:37Z",
        "authors": [
            "1minds3t"
        ],
        "link": "https://github.com/1minds3t/omnipkg",
        "source_category": "github"
    },
    {
        "id": "Waelkens/Binance-Crypto-Payment-Ecommerce-Gateway-Transaction-Tracker",
        "title": "Binance-Crypto-Payment-Ecommerce-Gateway-Transaction-Tracker",
        "abstract": "A trading bot uses APIs to automate crypto trading on exchanges like Binance. It tracks bitcoin, ethereum, and litecoin using blockchain tech. Features include automated strategies, margin trading, and machine learning for efficient tracking.",
        "published": "2025-08-30T20:12:55Z",
        "updated": "2025-09-02T06:54:27Z",
        "authors": [
            "Waelkens"
        ],
        "link": "https://github.com/Waelkens/Binance-Crypto-Payment-Ecommerce-Gateway-Transaction-Tracker",
        "source_category": "github"
    },
    {
        "id": "DONGHO5270/enterprise-mcp-infrastructure",
        "title": "enterprise-mcp-infrastructure",
        "abstract": " 286 verified AI tools in production-ready MCP infrastructure. Enterprise-grade reliability with dual licensing (MIT/Commercial). ✅ 100% verified ✅ 1-minute setup ✅ Fortune 500 ready",
        "published": "2025-08-24T05:22:52Z",
        "updated": "2025-09-02T06:51:55Z",
        "authors": [
            "DONGHO5270"
        ],
        "link": "https://github.com/DONGHO5270/enterprise-mcp-infrastructure",
        "source_category": "github"
    },
    {
        "id": "aws-samples/generative-ai-ml-latam-samples",
        "title": "generative-ai-ml-latam-samples",
        "abstract": "This repo provides Generative AI and AI/ML code samples, blueprints (end-to-end solutions) and proof of concepts oriented to the LATAM market",
        "published": "2024-11-08T06:39:08Z",
        "updated": "2025-09-02T06:46:35Z",
        "authors": [
            "aws-samples"
        ],
        "link": "https://github.com/aws-samples/generative-ai-ml-latam-samples",
        "source_category": "github"
    },
    {
        "id": "robertknight/rten",
        "title": "rten",
        "abstract": "ONNX neural network inference engine",
        "published": "2022-08-31T20:43:45Z",
        "updated": "2025-09-02T06:44:54Z",
        "authors": [
            "robertknight"
        ],
        "link": "https://github.com/robertknight/rten",
        "source_category": "github"
    },
    {
        "id": "inception-project/inception",
        "title": "inception",
        "abstract": "INCEpTION provides a semantic annotation platform offering intelligent annotation assistance and knowledge management.",
        "published": "2018-03-27T15:04:00Z",
        "updated": "2025-09-02T06:42:35Z",
        "authors": [
            "inception-project"
        ],
        "link": "https://github.com/inception-project/inception",
        "source_category": "github"
    },
    {
        "id": "streamlit/streamlit",
        "title": "streamlit",
        "abstract": "Streamlit — A faster way to build and share data apps.",
        "published": "2019-08-24T00:14:52Z",
        "updated": "2025-09-02T06:50:28Z",
        "authors": [
            "streamlit"
        ],
        "link": "https://github.com/streamlit/streamlit",
        "source_category": "github"
    },
    {
        "id": "marcelwa/aigverse",
        "title": "aigverse",
        "abstract": "A Python library for working with logic networks, synthesis, and optimization.",
        "published": "2024-09-04T18:05:23Z",
        "updated": "2025-09-02T06:41:43Z",
        "authors": [
            "marcelwa"
        ],
        "link": "https://github.com/marcelwa/aigverse",
        "source_category": "github"
    },
    {
        "id": "Comfy-Org/ComfyUI_frontend",
        "title": "ComfyUI_frontend",
        "abstract": "Official front-end implementation of ComfyUI",
        "published": "2024-06-13T00:22:24Z",
        "updated": "2025-09-02T06:41:11Z",
        "authors": [
            "Comfy-Org"
        ],
        "link": "https://github.com/Comfy-Org/ComfyUI_frontend",
        "source_category": "github"
    },
    {
        "id": "huggingface/transformers",
        "title": "transformers",
        "abstract": "🤗 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
        "published": "2018-10-29T13:56:00Z",
        "updated": "2025-09-02T07:13:24Z",
        "authors": [
            "huggingface"
        ],
        "link": "https://github.com/huggingface/transformers",
        "source_category": "github"
    },
    {
        "id": "hyunsooseol/snowCluster",
        "title": "snowCluster",
        "abstract": "This module allows users to analyze k-means & hierarchical clustering, and visualize results of Principal Component, Correspondence Analysis, Discriminant analysis, Decision tree, Multidimensional scaling, Multiple Factor Analysis, Machine learning, and Prophet analysis.",
        "published": "2020-10-18T07:23:38Z",
        "updated": "2025-09-02T06:39:20Z",
        "authors": [
            "hyunsooseol"
        ],
        "link": "https://github.com/hyunsooseol/snowCluster",
        "source_category": "github"
    },
    {
        "id": "nuclia/nucliadb",
        "title": "nucliadb",
        "abstract": "NucliaDB, The AI Search database for RAG",
        "published": "2022-04-05T20:21:54Z",
        "updated": "2025-09-02T06:37:17Z",
        "authors": [
            "nuclia"
        ],
        "link": "https://github.com/nuclia/nucliadb",
        "source_category": "github"
    },
    {
        "id": "hashim21223445/Andoka-",
        "title": "Andoka-",
        "abstract": "",
        "published": "2025-03-08T23:12:21Z",
        "updated": "2025-09-02T06:31:35Z",
        "authors": [
            "hashim21223445"
        ],
        "link": "https://github.com/hashim21223445/Andoka-",
        "source_category": "github"
    },
    {
        "id": "denev6/deep-learning-codes",
        "title": "deep-learning-codes",
        "abstract": "🗂️ Deep-Learning Projects",
        "published": "2024-05-09T02:10:49Z",
        "updated": "2025-09-02T06:31:03Z",
        "authors": [
            "denev6"
        ],
        "link": "https://github.com/denev6/deep-learning-codes",
        "source_category": "github"
    },
    {
        "id": "iterative/dvc",
        "title": "dvc",
        "abstract": "🦉 Data Versioning and ML Experiments",
        "published": "2017-03-04T08:16:33Z",
        "updated": "2025-09-02T06:30:25Z",
        "authors": [
            "iterative"
        ],
        "link": "https://github.com/iterative/dvc",
        "source_category": "github"
    },
    {
        "id": "rockerritesh/maithili-news",
        "title": "maithili-news",
        "abstract": "Maithili-news-gnews is a project aimed at automating the fetching of news from the GNews API, translating it into the Maithili language, and then classifying it using a machine learning model. The classified news articles are then displayed on a website using Streamlit.",
        "published": "2024-01-24T02:07:25Z",
        "updated": "2025-09-02T06:30:17Z",
        "authors": [
            "rockerritesh"
        ],
        "link": "https://github.com/rockerritesh/maithili-news",
        "source_category": "github"
    },
    {
        "id": "jvalido/bitcoin-price-prediction-ML-DL",
        "title": "bitcoin-price-prediction-ML-DL",
        "abstract": "📈 Predict Bitcoin prices using various machine learning and deep learning models with historical data to explore trends and make informed decisions.",
        "published": "2025-08-26T18:30:46Z",
        "updated": "2025-09-02T06:25:08Z",
        "authors": [
            "jvalido"
        ],
        "link": "https://github.com/jvalido/bitcoin-price-prediction-ML-DL",
        "source_category": "github"
    },
    {
        "id": "jmevatt/vex",
        "title": "vex",
        "abstract": "🔥 VEX - Your local AI coding assistant with attitude and deep expertise",
        "published": "2025-09-02T06:13:34Z",
        "updated": "2025-09-02T07:18:20Z",
        "authors": [
            "jmevatt"
        ],
        "link": "https://github.com/jmevatt/vex",
        "source_category": "github"
    },
    {
        "id": "wellround-me/website",
        "title": "website",
        "abstract": "Official launch website of Wellround",
        "published": "2025-08-16T08:49:31Z",
        "updated": "2025-09-02T07:18:19Z",
        "authors": [
            "wellround-me"
        ],
        "link": "https://github.com/wellround-me/website",
        "source_category": "github"
    },
    {
        "id": "3bsalam-1/MountainCar",
        "title": "MountainCar",
        "abstract": "⛰ Reinforcement learning model trying to make car reach to top of mountain ",
        "published": "2024-03-12T23:26:43Z",
        "updated": "2025-09-02T07:18:16Z",
        "authors": [
            "3bsalam-1"
        ],
        "link": "https://github.com/3bsalam-1/MountainCar",
        "source_category": "github"
    },
    {
        "id": "Qyxxlor/StockSharp-Multi-Asset-Trading",
        "title": "StockSharp-Multi-Asset-Trading",
        "abstract": "A simple app for tracking monthly spending and working towards financial goals.",
        "published": "2025-07-24T20:52:01Z",
        "updated": "2025-09-02T07:18:11Z",
        "authors": [
            "Qyxxlor"
        ],
        "link": "https://github.com/Qyxxlor/StockSharp-Multi-Asset-Trading",
        "source_category": "github"
    },
    {
        "id": "Valtherismyzv7P/SmartAI-Trading-Bot",
        "title": "SmartAI-Trading-Bot",
        "abstract": "Trading Smart AI Bot for Ethereum blockchain",
        "published": "2025-08-27T12:23:48Z",
        "updated": "2025-09-02T07:18:04Z",
        "authors": [
            "Valtherismyzv7P"
        ],
        "link": "https://github.com/Valtherismyzv7P/SmartAI-Trading-Bot",
        "source_category": "github"
    },
    {
        "id": "BBismaarshad/GIAIC-Generative-AI",
        "title": "GIAIC-Generative-AI",
        "abstract": "This repository contains all the classwork, assignments, and project code for the Generative AI course (Quarter 4) offered by the Governor Initiative for AI and Computing (GIAIC).Ideal for learners and developers interested in mastering Generative AI with real-world applications.",
        "published": "2025-05-07T09:28:08Z",
        "updated": "2025-09-02T07:17:14Z",
        "authors": [
            "BBismaarshad"
        ],
        "link": "https://github.com/BBismaarshad/GIAIC-Generative-AI",
        "source_category": "github"
    },
    {
        "id": "embabel/embabel-agent",
        "title": "embabel-agent",
        "abstract": "Agent framework for the JVM. Pronounced Em-BAY-bel /ɛmˈbeɪbəl/",
        "published": "2025-04-10T03:06:07Z",
        "updated": "2025-09-02T07:17:12Z",
        "authors": [
            "embabel"
        ],
        "link": "https://github.com/embabel/embabel-agent",
        "source_category": "github"
    },
    {
        "id": "loryanstrant/simple-wik",
        "title": "simple-wik",
        "abstract": "A simple, sweet wiki solution that keeps your knowledge organized just like Simple Rick's life - uncomplicated and homey. Self-hosted with Markdown storage and RAG-ready for AI integration.",
        "published": "2025-09-01T23:28:05Z",
        "updated": "2025-09-02T07:17:06Z",
        "authors": [
            "loryanstrant"
        ],
        "link": "https://github.com/loryanstrant/simple-wik",
        "source_category": "github"
    },
    {
        "id": "apache/doris",
        "title": "doris",
        "abstract": "Apache Doris is an easy-to-use, high performance and unified analytics database.",
        "published": "2017-08-10T12:13:30Z",
        "updated": "2025-09-02T07:16:35Z",
        "authors": [
            "apache"
        ],
        "link": "https://github.com/apache/doris",
        "source_category": "github"
    },
    {
        "id": "Vulthen/myhhub-stock-analysis-suite-toolkit",
        "title": "myhhub-stock-analysis-suite-toolkit",
        "abstract": "Dot Ledger is a Free and Open Source personal finance management tool.",
        "published": "2025-06-17T21:09:44Z",
        "updated": "2025-09-02T07:16:08Z",
        "authors": [
            "Vulthen"
        ],
        "link": "https://github.com/Vulthen/myhhub-stock-analysis-suite-toolkit",
        "source_category": "github"
    },
    {
        "id": "Bytedesk/bytedesk",
        "title": "bytedesk",
        "abstract": "Enterprise IM Solution with AI powered live chat, email support, omni-channel customer service & team im，alternative to slack + zendesk/intercom",
        "published": "2023-10-12T00:31:36Z",
        "updated": "2025-09-02T07:16:04Z",
        "authors": [
            "Bytedesk"
        ],
        "link": "https://github.com/Bytedesk/bytedesk",
        "source_category": "github"
    },
    {
        "id": "bespokedesignservices/cortexcart-insight-dashboard",
        "title": "cortexcart-insight-dashboard",
        "abstract": "An e-commerce analytics platform with an AI-powered recommendation engine, built with Next.js, NextAuth, and MySQL.",
        "published": "2025-06-14T19:53:08Z",
        "updated": "2025-09-02T07:16:00Z",
        "authors": [
            "bespokedesignservices"
        ],
        "link": "https://github.com/bespokedesignservices/cortexcart-insight-dashboard",
        "source_category": "github"
    },
    {
        "id": "m5stack/Xiaozhi-Card",
        "title": "Xiaozhi-Card",
        "abstract": "Xiaozhi-Card source code, fork from 78/xiaozhi-esp32",
        "published": "2025-07-25T02:02:55Z",
        "updated": "2025-09-02T07:15:34Z",
        "authors": [
            "m5stack"
        ],
        "link": "https://github.com/m5stack/Xiaozhi-Card",
        "source_category": "github"
    },
    {
        "id": "Vixoq/vnpy-Machine-Learning",
        "title": "vnpy-Machine-Learning",
        "abstract": "A simple and secure money manager that keeps you financially vigilant.",
        "published": "2025-06-17T20:42:01Z",
        "updated": "2025-09-02T07:15:26Z",
        "authors": [
            "Vixoq"
        ],
        "link": "https://github.com/Vixoq/vnpy-Machine-Learning",
        "source_category": "github"
    },
    {
        "id": "SoraEase/sora-prompt",
        "title": "sora-prompt",
        "abstract": "Sora Prompt Collection, a repository dedicated to inspiring AI-driven video creation with Sora. ",
        "published": "2024-02-24T06:35:08Z",
        "updated": "2025-09-02T07:15:24Z",
        "authors": [
            "SoraEase"
        ],
        "link": "https://github.com/SoraEase/sora-prompt",
        "source_category": "github"
    },
    {
        "id": "mckinsey/agents-at-scale-ark",
        "title": "agents-at-scale-ark",
        "abstract": "Provider-agnostic operations for agentic resources. ARK codifies patterns and practices developed across dozens of agentic application projects.",
        "published": "2025-08-28T15:48:47Z",
        "updated": "2025-09-02T07:15:18Z",
        "authors": [
            "mckinsey"
        ],
        "link": "https://github.com/mckinsey/agents-at-scale-ark",
        "source_category": "github"
    },
    {
        "id": "DaoCloud/DaoCloud-docs",
        "title": "DaoCloud-docs",
        "abstract": "DaoCloud Enterprise 5.0 Documentation",
        "published": "2015-11-07T08:00:00Z",
        "updated": "2025-09-02T07:14:35Z",
        "authors": [
            "DaoCloud"
        ],
        "link": "https://github.com/DaoCloud/DaoCloud-docs",
        "source_category": "github"
    },
    {
        "id": "Qyxxlor/Visualig-Ai-Make-Your-Ai-Api-Chat-Photo-Coding-Video",
        "title": "Visualig-Ai-Make-Your-Ai-Api-Chat-Photo-Coding-Video",
        "abstract": "versatile AI platform that lets users create custom AI models via API, enabling chatbots, image generation, coding assistance, and video creation all in one integrated interface.",
        "published": "2025-07-24T20:45:26Z",
        "updated": "2025-09-02T07:13:42Z",
        "authors": [
            "Qyxxlor"
        ],
        "link": "https://github.com/Qyxxlor/Visualig-Ai-Make-Your-Ai-Api-Chat-Photo-Coding-Video",
        "source_category": "github"
    },
    {
        "id": "Bobbu/dcc",
        "title": "dcc",
        "abstract": "DCC - Demo Claude Code. A simple Quote app that gets created and improved upon through collaboration with Claude Code.",
        "published": "2025-08-12T19:16:48Z",
        "updated": "2025-09-02T07:12:37Z",
        "authors": [
            "Bobbu"
        ],
        "link": "https://github.com/Bobbu/dcc",
        "source_category": "github"
    },
    {
        "id": "awesome-sora/awesome-sora",
        "title": "awesome-sora",
        "abstract": "😎 Awesome list of interesting topics on Sora",
        "published": "2024-02-24T07:50:19Z",
        "updated": "2025-09-02T07:12:30Z",
        "authors": [
            "awesome-sora"
        ],
        "link": "https://github.com/awesome-sora/awesome-sora",
        "source_category": "github"
    },
    {
        "id": "starwit/waste-detection",
        "title": "waste-detection",
        "abstract": "Contains training data for waste detection models",
        "published": "2025-07-30T08:02:32Z",
        "updated": "2025-09-02T07:12:31Z",
        "authors": [
            "starwit"
        ],
        "link": "https://github.com/starwit/waste-detection",
        "source_category": "github"
    },
    {
        "id": "mudler/LocalAI",
        "title": "LocalAI",
        "abstract": ":robot: The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI,  running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more models architectures. Features: Generate Text, Audio, Video, Images, Voice Cloning, Distributed, P2P inference",
        "published": "2023-03-18T22:58:02Z",
        "updated": "2025-09-02T07:18:23Z",
        "authors": [
            "mudler"
        ],
        "link": "https://github.com/mudler/LocalAI",
        "source_category": "github"
    },
    {
        "id": "Phylmox/OpenBB-finance",
        "title": "OpenBB-finance",
        "abstract": "Investment Research for Everyone, Everywhere.",
        "published": "2025-06-17T21:28:14Z",
        "updated": "2025-09-02T07:12:03Z",
        "authors": [
            "Phylmox"
        ],
        "link": "https://github.com/Phylmox/OpenBB-finance",
        "source_category": "github"
    },
    {
        "id": "stacklok/toolhive",
        "title": "toolhive",
        "abstract": "ToolHive makes deploying MCP servers easy, secure and fun",
        "published": "2025-03-12T14:49:15Z",
        "updated": "2025-09-02T07:11:48Z",
        "authors": [
            "stacklok"
        ],
        "link": "https://github.com/stacklok/toolhive",
        "source_category": "github"
    },
    {
        "id": "langgenius/dify",
        "title": "dify",
        "abstract": "Production-ready platform for agentic workflow development.",
        "published": "2023-04-12T07:40:24Z",
        "updated": "2025-09-02T07:17:52Z",
        "authors": [
            "langgenius"
        ],
        "link": "https://github.com/langgenius/dify",
        "source_category": "github"
    },
    {
        "id": "Its-me-nishmal/fun",
        "title": "fun",
        "abstract": "",
        "published": "2025-07-27T16:13:49Z",
        "updated": "2025-09-02T07:10:34Z",
        "authors": [
            "Its-me-nishmal"
        ],
        "link": "https://github.com/Its-me-nishmal/fun",
        "source_category": "github"
    },
    {
        "id": "jeffthedeveloper/jeffthedeveloper",
        "title": "jeffthedeveloper",
        "abstract": "Transforming complex data into production-grade AI solutions. Expert in Python, MLOps pipelines, and full-stack data application development.",
        "published": "2025-03-02T18:54:47Z",
        "updated": "2025-09-02T07:10:30Z",
        "authors": [
            "jeffthedeveloper"
        ],
        "link": "https://github.com/jeffthedeveloper/jeffthedeveloper",
        "source_category": "github"
    },
    {
        "id": "targed/Awesome-Plugins",
        "title": "Awesome-Plugins",
        "abstract": "Awesome-Plugins is a GitHub repository that serves as a comprehensive list of plugins, add-ons, and extensions for ChatGPT, as well as other language models that are compatible with the GPT architecture.",
        "published": "2023-05-08T05:17:39Z",
        "updated": "2025-09-02T07:10:17Z",
        "authors": [
            "targed"
        ],
        "link": "https://github.com/targed/Awesome-Plugins",
        "source_category": "github"
    },
    {
        "id": "coze-dev/coze-loop",
        "title": "coze-loop",
        "abstract": "Next-generation AI Agent Optimization Platform: Cozeloop addresses challenges in AI agent development by providing full-lifecycle management capabilities from development, debugging, and evaluation to monitoring.  ",
        "published": "2025-06-24T00:26:28Z",
        "updated": "2025-09-02T07:14:57Z",
        "authors": [
            "coze-dev"
        ],
        "link": "https://github.com/coze-dev/coze-loop",
        "source_category": "github"
    },
    {
        "id": "Hexroth/juspay-hyperswitch-realtime-dashboard",
        "title": "juspay-hyperswitch-realtime-dashboard",
        "abstract": "Rescript powered React library for seamless payment integration and customization.",
        "published": "2025-06-17T20:44:26Z",
        "updated": "2025-09-02T07:09:55Z",
        "authors": [
            "Hexroth"
        ],
        "link": "https://github.com/Hexroth/juspay-hyperswitch-realtime-dashboard",
        "source_category": "github"
    },
    {
        "id": "refly-ai/refly",
        "title": "refly",
        "abstract": "The Open-Source Agentic Workspace for Human-AI Collaboration.",
        "published": "2024-02-19T11:04:30Z",
        "updated": "2025-09-02T07:09:19Z",
        "authors": [
            "refly-ai"
        ],
        "link": "https://github.com/refly-ai/refly",
        "source_category": "github"
    },
    {
        "id": "containers/podman-desktop-extension-ai-lab",
        "title": "podman-desktop-extension-ai-lab",
        "abstract": "Work with LLMs on a local environment using containers",
        "published": "2023-12-19T20:59:01Z",
        "updated": "2025-09-02T07:08:31Z",
        "authors": [
            "containers"
        ],
        "link": "https://github.com/containers/podman-desktop-extension-ai-lab",
        "source_category": "github"
    },
    {
        "id": "thirtycalendar/thirty",
        "title": "thirty",
        "abstract": "The AI Calendar. Discord: https://discord.gg/Y8XQ4dV4UC",
        "published": "2025-05-18T15:01:40Z",
        "updated": "2025-09-02T07:08:26Z",
        "authors": [
            "thirtycalendar"
        ],
        "link": "https://github.com/thirtycalendar/thirty",
        "source_category": "github"
    },
    {
        "id": "ILDaviz/aicontextator",
        "title": "aicontextator",
        "abstract": "Aicontextator is a simple, practical CLI that bundles a project's files into a single LLM-ready context string.",
        "published": "2025-08-31T12:09:56Z",
        "updated": "2025-09-02T07:07:52Z",
        "authors": [
            "ILDaviz"
        ],
        "link": "https://github.com/ILDaviz/aicontextator",
        "source_category": "github"
    },
    {
        "id": "sporaeaware/sporae",
        "title": "sporae",
        "abstract": "Cognition diffused through spore-born networks. Powered by Hugging Face.",
        "published": "2025-05-17T23:29:04Z",
        "updated": "2025-09-02T07:07:35Z",
        "authors": [
            "sporaeaware"
        ],
        "link": "https://github.com/sporaeaware/sporae",
        "source_category": "github"
    },
    {
        "id": "lokeswaran-aj/open-fiesta",
        "title": "open-fiesta",
        "abstract": "Open Fiesta lets you chat with 100+ AI models like OpenAI, Gemini, Claude, Perplexity, Deepseek, and Grok in one place. Compare model responses side-by-side in real-time and choose the best AI for every task",
        "published": "2025-08-22T10:42:56Z",
        "updated": "2025-09-02T07:07:11Z",
        "authors": [
            "lokeswaran-aj"
        ],
        "link": "https://github.com/lokeswaran-aj/open-fiesta",
        "source_category": "github"
    },
    {
        "id": "vikiboss/viki-home",
        "title": "viki-home",
        "abstract": "Sharable personal homepage, Solid.js + UnoCSS (SPA), 99% coded by Trae AI Editor.",
        "published": "2025-01-22T08:39:01Z",
        "updated": "2025-09-02T07:06:46Z",
        "authors": [
            "vikiboss"
        ],
        "link": "https://github.com/vikiboss/viki-home",
        "source_category": "github"
    },
    {
        "id": "anhnt02hp/CS229-Fall2018-FullCourse",
        "title": "CS229-Fall2018-FullCourse",
        "abstract": "Complete solutions to all CS229 problem sets. Written from Python with detailed explanations.",
        "published": "2025-03-16T15:36:43Z",
        "updated": "2025-09-02T07:05:43Z",
        "authors": [
            "anhnt02hp"
        ],
        "link": "https://github.com/anhnt02hp/CS229-Fall2018-FullCourse",
        "source_category": "github"
    },
    {
        "id": "n8n-io/n8n",
        "title": "n8n",
        "abstract": "Fair-code workflow automation platform with native AI capabilities. Combine visual building with custom code, self-host or cloud, 400+ integrations.",
        "published": "2019-06-22T09:24:21Z",
        "updated": "2025-09-02T07:10:30Z",
        "authors": [
            "n8n-io"
        ],
        "link": "https://github.com/n8n-io/n8n",
        "source_category": "github"
    },
    {
        "id": "tryAGI/CSharpToJsonSchema",
        "title": "CSharpToJsonSchema",
        "abstract": "Helpers and Source generator to define OpenAI/Ollama/Anthropic/Gemini/LangChain tools natively through C# interfaces and without Reflection",
        "published": "2024-10-16T20:12:47Z",
        "updated": "2025-09-02T07:04:30Z",
        "authors": [
            "tryAGI"
        ],
        "link": "https://github.com/tryAGI/CSharpToJsonSchema",
        "source_category": "github"
    },
    {
        "id": "antgroup/hugescm",
        "title": "hugescm",
        "abstract": "HugeSCM - A next generation cloud-based version control system",
        "published": "2024-11-20T01:42:41Z",
        "updated": "2025-09-02T07:04:26Z",
        "authors": [
            "antgroup"
        ],
        "link": "https://github.com/antgroup/hugescm",
        "source_category": "github"
    },
    {
        "id": "https://medium.com/p/3a76724e1adc",
        "title": "6 Python AI Side Hustles That Quietly Pay Entrepreneurs Every Month",
        "abstract": "Code once. Deploy. Get paid.Continue reading on Venture »",
        "published": "2025-09-02T07:10:46Z",
        "updated": "2025-09-02T07:10:46Z",
        "authors": [
            "Arfa"
        ],
        "link": "https://blog.venturemagazine.net/6-python-ai-side-hustles-that-quietly-pay-entrepreneurs-every-month-3a76724e1adc?source=rss------artificial_intelligence-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/c402efc9cca5",
        "title": "Unlock the Future with AIFEXA",
        "abstract": "Artificial Intelligence is no longer just a buzzword&#x200a;&#x2014;&#x200a;it&#x2019;s becoming the backbone of how businesses innovate, scale, and compete&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T07:10:27Z",
        "updated": "2025-09-02T07:10:27Z",
        "authors": [
            "AI Fexa"
        ],
        "link": "https://medium.com/@aifexaa/unlock-the-future-with-aifexa-c402efc9cca5?source=rss------artificial_intelligence-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/de5a973e5fb3",
        "title": "09010712940شماره خاله #شماره خاله# تهران #شماره خاله# اصفهان\nشماره خاله #شماره خاله# تهران #شماره…",
        "abstract": "&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;f\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647;&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T07:08:24Z",
        "updated": "2025-09-02T07:08:24Z",
        "authors": [
            "شماره خاله تهران شماره خاله شهریار\nشماره خاله بابل"
        ],
        "link": "https://medium.com/@dhnhaznhhkhabbhhlkhldd/09010712940%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%AA%D9%87%D8%B1%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%AA%D9%87%D8%B1%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-de5a973e5fb3?source=rss------artificial_intelligence-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/3b24007e8859",
        "title": "If AI Can Code, Why Can’t It Pick Up My Socks?",
        "abstract": "This real life problem needs an urgent solution!Continue reading on Medium »",
        "published": "2025-09-02T07:08:21Z",
        "updated": "2025-09-02T07:08:21Z",
        "authors": [
            "Gupta Apoorva"
        ],
        "link": "https://medium.com/@gupta.apoorva/if-ai-can-code-why-cant-it-pick-up-my-socks-3b24007e8859?source=rss------artificial_intelligence-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/b057e74a1abf",
        "title": "The Future of Scalping: Using Smart Money Concepts + AI Tools",
        "abstract": "Stop staring at charts for hours. The new edge in trading is a partnership between human strategy and machine intelligence.Continue reading on Coinmonks »",
        "published": "2025-09-02T07:07:11Z",
        "updated": "2025-09-02T07:07:11Z",
        "authors": [
            "Fahad's Foresight"
        ],
        "link": "https://medium.com/coinmonks/the-future-of-scalping-using-smart-money-concepts-ai-tools-b057e74a1abf?source=rss------artificial_intelligence-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/23e4d7dccac2",
        "title": "Robotophilia vs. Robotophobia: two mirrors, one anxiety",
        "abstract": "Me:Continue reading on Medium »",
        "published": "2025-09-02T07:07:08Z",
        "updated": "2025-09-02T07:07:08Z",
        "authors": [
            "Konstantin Rovinskiy"
        ],
        "link": "https://cryptosamadhi.medium.com/robotophilia-vs-robotophobia-two-mirrors-one-anxiety-23e4d7dccac2?source=rss------artificial_intelligence-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/e80123d2aea1",
        "title": "09010712940شماره خاله #شماره خاله# تهران #شماره خاله# اصفهان\nشماره خاله #شماره خاله# تهران #شماره…",
        "abstract": "&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;f\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647;&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T07:07:03Z",
        "updated": "2025-09-02T07:07:03Z",
        "authors": [
            "شماره خاله تهران شماره خاله شهریار\nشماره خاله بابل"
        ],
        "link": "https://medium.com/@dhnhaznhhkhabbhhlkhldd/09010712940%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%AA%D9%87%D8%B1%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%AA%D9%87%D8%B1%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-e80123d2aea1?source=rss------artificial_intelligence-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/ee59bd96fbc4",
        "title": "New to Monday.com? Here’s the 10-Minute Setup for Your First Project Board",
        "abstract": "Struggling to organize projects? Learn how to set up your first Monday.com project board in just 10 minutes and boost your productivity&#x2026;Continue reading on Tech Orbit »",
        "published": "2025-09-02T07:06:44Z",
        "updated": "2025-09-02T07:06:44Z",
        "authors": [
            "TechHarry"
        ],
        "link": "https://medium.com/tech-orbit/new-to-monday-com-heres-the-10-minute-setup-for-your-first-project-board-ee59bd96fbc4?source=rss------artificial_intelligence-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/9e379b715dd7",
        "title": "09010712940شماره خاله #شماره خاله# تهران #شماره خاله# اصفهان\nشماره خاله #شماره خاله# تهران #شماره…",
        "abstract": "&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;f\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647;&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T07:06:12Z",
        "updated": "2025-09-02T07:06:12Z",
        "authors": [
            "شماره خاله تهران شماره خاله شهریار\nشماره خاله بابل"
        ],
        "link": "https://medium.com/@dhnhaznhhkhabbhhlkhldd/09010712940%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%AA%D9%87%D8%B1%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%AA%D9%87%D8%B1%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-9e379b715dd7?source=rss------artificial_intelligence-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/440ee271ac4f",
        "title": "I Think I Caught the AI’s Writing Style — After 20 Million Words of Conversation",
        "abstract": "Author&#x2019;s Note:\nThis article is based entirely on my personal observations and hypotheses as a general user. It does not reflect any&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T07:05:50Z",
        "updated": "2025-09-02T07:05:50Z",
        "authors": [
            "Izumain"
        ],
        "link": "https://medium.com/@izumain/i-think-i-caught-the-ais-writing-style-after-20-million-words-of-conversation-440ee271ac4f?source=rss------artificial_intelligence-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/440ee271ac4f",
        "title": "I Think I Caught the AI’s Writing Style — After 20 Million Words of Conversation",
        "abstract": "Author&#x2019;s Note:\nThis article is based entirely on my personal observations and hypotheses as a general user. It does not reflect any&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T07:05:50Z",
        "updated": "2025-09-02T07:05:50Z",
        "authors": [
            "Izumain"
        ],
        "link": "https://medium.com/@izumain/i-think-i-caught-the-ais-writing-style-after-20-million-words-of-conversation-440ee271ac4f?source=rss------machine_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/e2dc3f447814",
        "title": "K-Nearest Neighbor Algorithm",
        "abstract": "Part 3: Learning Machine Learning/Deep LearningContinue reading on Medium »",
        "published": "2025-09-02T07:04:12Z",
        "updated": "2025-09-02T07:04:12Z",
        "authors": [
            "Hemasri"
        ],
        "link": "https://medium.com/@lellas.odyssey/k-nearest-neighbor-algorithm-e2dc3f447814?source=rss------machine_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/0a775a825011",
        "title": "Hyperdimensional OCR with Tropical Decoding: Building a Practical HDC + Markov + Beam Search System",
        "abstract": "IntroductionContinue reading on Medium »",
        "published": "2025-09-02T06:51:30Z",
        "updated": "2025-09-02T06:51:30Z",
        "authors": [
            "Robert McMenemy"
        ],
        "link": "https://rabmcmenemy.medium.com/hyperdimensional-ocr-with-tropical-decoding-building-a-practical-hdc-markov-beam-search-system-0a775a825011?source=rss------machine_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/fb2e1e4c5359",
        "title": "cnsjcbsiشماره 09374389086خاله #شماره خاله# تهران #شماره خاله# اصفهان #شماره خاله #شیراز #شماره…",
        "abstract": "&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6cc;&#x632;&#x62f; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6af;&#x631;&#x6af;&#x627;&#x646; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x633;&#x646;&#x646;&#x62f;&#x62c; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6a9;&#x631;&#x645;&#x627;&#x646; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x642;&#x632;&#x648;&#x6cc;&#x646; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x627;&#x631;&#x648;&#x645;&#x6cc;&#x647; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6a9;&#x6cc;&#x634; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x642;&#x634;&#x645; &#x634;&#x645;&#x627;&#x631;&#x647;&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:49:10Z",
        "updated": "2025-09-02T06:49:10Z",
        "authors": [
            "شماره خاله #شماره خاله#تهران #شماره خاله#اصفهان\nشم"
        ],
        "link": "https://medium.com/@cnsidjcjxjsjdb/cnsjcbsi%D8%B4%D9%85%D8%A7%D8%B1%D9%87-09374389086%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%AA%D9%87%D8%B1%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%DB%8C%D8%B1%D8%A7%D8%B2-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-fb2e1e4c5359?source=rss------machine_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/a2d3a689e0dd",
        "title": "nxwobcsjشماره 09374389086خاله #شماره خاله# تهران #شماره خاله# اصفهان #شماره خاله #شیراز #شماره…",
        "abstract": "&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;f\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647;&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:48:31Z",
        "updated": "2025-09-02T06:48:31Z",
        "authors": [
            "شماره خاله #شماره خاله#تهران #شماره خاله#اصفهان\nشم"
        ],
        "link": "https://medium.com/@cnsidjcjxjsjdb/nxwobcsj%D8%B4%D9%85%D8%A7%D8%B1%D9%87-09374389086%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%AA%D9%87%D8%B1%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%DB%8C%D8%B1%D8%A7%D8%B2-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-a2d3a689e0dd?source=rss------machine_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/88c97a1f4530",
        "title": "09374389086",
        "abstract": "&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6cc;&#x632;&#x62f; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6af;&#x631;&#x6af;&#x627;&#x646; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x633;&#x646;&#x646;&#x62f;&#x62c; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6a9;&#x631;&#x645;&#x627;&#x646; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x642;&#x632;&#x648;&#x6cc;&#x646; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x627;&#x631;&#x648;&#x645;&#x6cc;&#x647; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6a9;&#x6cc;&#x634; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x642;&#x634;&#x645; &#x634;&#x645;&#x627;&#x631;&#x647;&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:47:51Z",
        "updated": "2025-09-02T06:47:51Z",
        "authors": [
            "شماره خاله #شماره خاله#تهران #شماره خاله#اصفهان\nشم"
        ],
        "link": "https://medium.com/@cnsidjcjxjsjdb/09374389086-88c97a1f4530?source=rss------machine_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/2cdde6dc4042",
        "title": "Automate Your Feature Engineering: Cut Model Development Time by 70%",
        "abstract": "Explore the tools and techniques that automate the most time-consuming step in machine learning, from data prep to feature selection.Continue reading on NexTechie: The Infotech Magazine »",
        "published": "2025-09-02T06:46:32Z",
        "updated": "2025-09-02T06:46:32Z",
        "authors": [
            "NexTechie"
        ],
        "link": "https://medium.com/nextechie/automate-your-feature-engineering-cut-model-development-time-by-70-2cdde6dc4042?source=rss------machine_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/e4314c900010",
        "title": "Build AI Agents with LangGraph and Ollama",
        "abstract": "In the previous blog, we built our first chat agent from scratch. While it worked for basic demonstrations, it had several production&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:45:19Z",
        "updated": "2025-09-02T06:45:19Z",
        "authors": [
            "anirudh Bhatnagar"
        ],
        "link": "https://medium.com/@anirudh.bh/build-ai-agents-with-langgraph-and-ollama-e4314c900010?source=rss------machine_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/613a967f4e21",
        "title": "GGNES: How I Built a Deterministic Brain Evolution System That Discovers Its Own Building Blocks",
        "abstract": "The story of creating a neural architecture search system that evolves like DNA, thinks in graph grammars, and guarantees perfect&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:38:21Z",
        "updated": "2025-09-02T06:38:21Z",
        "authors": [
            "Amin Sedaghat"
        ],
        "link": "https://medium.com/@amin32846/ggnes-how-i-built-a-deterministic-brain-evolution-system-that-discovers-its-own-building-blocks-613a967f4e21?source=rss------machine_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/64c02bcbf7ed",
        "title": "The Ultimate Guide to Text Annotation Services for AI",
        "abstract": "In the rapidly evolving field of artificial intelligence, high-quality training data is the cornerstone of successful machine learning&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:26:53Z",
        "updated": "2025-09-02T06:26:53Z",
        "authors": [
            "Sohan Lal"
        ],
        "link": "https://medium.com/@sohan.lal_54278/the-ultimate-guide-to-text-annotation-services-for-ai-64c02bcbf7ed?source=rss------machine_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/5c5bb3dd189e",
        "title": "The Ultimate Test Of Character",
        "abstract": "Yousuf Series&#x200a;&#x2014;&#x200a;Article -04Continue reading on Being »",
        "published": "2025-09-02T07:00:53Z",
        "updated": "2025-09-02T07:00:53Z",
        "authors": [
            "Faith and Feelings"
        ],
        "link": "https://medium.com/being/the-ultimate-test-of-character-5c5bb3dd189e?source=rss------deep_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/9aa1d908599c",
        "title": "Why OpenAI’s o3-mini Revolutionizes Space Mission Planning by 2050",
        "abstract": "Continue reading on Cubed »",
        "published": "2025-09-02T06:54:23Z",
        "updated": "2025-09-02T06:54:23Z",
        "authors": [
            "Shailendra Kumar"
        ],
        "link": "https://blog.cubed.run/why-openais-o3-mini-revolutionizes-space-mission-planning-by-2050-9aa1d908599c?source=rss------deep_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/7ca8df881b82",
        "title": "You Deserve to Be Loved and Chosen — Not Almost",
        "abstract": "you are not hard to love &#x2014; they were just not ready to love you right.Continue reading on Medium »",
        "published": "2025-09-02T06:54:10Z",
        "updated": "2025-09-02T06:54:10Z",
        "authors": [
            "con ᯓᡣ"
        ],
        "link": "https://medium.com/@mariconmartinnn/you-deserve-to-be-loved-and-chosen-not-almost-7ca8df881b82?source=rss------deep_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/6c2041a32d8b",
        "title": "Enhance your understanding of any topic more quickly with these 8 ChatGPT prompts.",
        "abstract": "If you&#x2019;ve ever struggled to learn faster and retain more, you&#x2019;re not alone.Continue reading on Medium »",
        "published": "2025-09-02T06:51:54Z",
        "updated": "2025-09-02T06:51:54Z",
        "authors": [
            "Talal Hussain"
        ],
        "link": "https://medium.com/@talbalushi/enhance-your-understanding-of-any-topic-more-quickly-with-these-8-chatgpt-prompts-6c2041a32d8b?source=rss------deep_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/dcad256c4209",
        "title": "The AI Education Void: Why we need a high-trust, continuous learning revolution now",
        "abstract": "The artificial intelligence (AI) revolution is no longer a distant promise&#x200a;&#x2014;&#x200a;it&#x2019;s here, reshaping industries, redefining workflows, and&#x2026;Continue reading on Startup Stash »",
        "published": "2025-09-02T06:30:04Z",
        "updated": "2025-09-02T06:30:04Z",
        "authors": [
            "Abhinav Jain"
        ],
        "link": "https://blog.startupstash.com/the-ai-education-void-why-we-need-a-high-trust-continuous-learning-revolution-now-dcad256c4209?source=rss------deep_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/c1f05e49519c",
        "title": "Evolution of Decoder-only models",
        "abstract": "How encoder-only became champion modern day LLMsContinue reading on Medium »",
        "published": "2025-09-02T06:24:13Z",
        "updated": "2025-09-02T06:24:13Z",
        "authors": [
            "Mayukh Sarkar"
        ],
        "link": "https://thedatageek.medium.com/evolution-of-decoder-only-models-c1f05e49519c?source=rss------deep_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/12aaa0f65467",
        "title": "Advanced TypeScript: Mastering Utility Types for Real-World Development",
        "abstract": "TypeScript has quickly become the go-to language for modern developers &#x1f468;&#x200d;&#x1f4bb;. Beyond just type-checking, it brings powerful features that&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:11:34Z",
        "updated": "2025-09-02T06:11:34Z",
        "authors": [
            "Miss Avantika ✨"
        ],
        "link": "https://medium.com/@missAvantika/advanced-typescript-mastering-utility-types-for-real-world-development-12aaa0f65467?source=rss------deep_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/ce78db232f7c",
        "title": "“I Am Not My Mind, I Am Not My Body — Then Who Am I? The Truth Everyone Forgets”",
        "abstract": "We are all lost in our own identity.Continue reading on Medium »",
        "published": "2025-09-02T06:02:44Z",
        "updated": "2025-09-02T06:02:44Z",
        "authors": [
            "Sahil maurya"
        ],
        "link": "https://medium.com/@sahilmaurya1995/i-am-not-my-mind-i-am-not-my-body-then-who-am-i-the-truth-everyone-forgets-ce78db232f7c?source=rss------deep_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/6e74460c6cde",
        "title": "Flutter Isolates & Concurrency — A Deep Advanced Dive",
        "abstract": "Flutter is powerful when it comes to building smooth, responsive apps. But here&#x2019;s the catch: Flutter, like Dart itself, is single-threaded&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T05:47:34Z",
        "updated": "2025-09-02T05:47:34Z",
        "authors": [
            "The Hack Habitual"
        ],
        "link": "https://medium.com/@theHackHabitual/flutter-isolates-concurrency-a-deep-advanced-dive-6e74460c6cde?source=rss------deep_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/64c02bcbf7ed",
        "title": "The Ultimate Guide to Text Annotation Services for AI",
        "abstract": "In the rapidly evolving field of artificial intelligence, high-quality training data is the cornerstone of successful machine learning&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:26:53Z",
        "updated": "2025-09-02T06:26:53Z",
        "authors": [
            "Sohan Lal"
        ],
        "link": "https://medium.com/@sohan.lal_54278/the-ultimate-guide-to-text-annotation-services-for-ai-64c02bcbf7ed?source=rss------nlp-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/d0384cf030c5",
        "title": "Papers Explained 444: POLARIS",
        "abstract": "POLARIS is a post-training recipe focused on calibrated data difficulty, enhanced data diversity, inference-time length scaling, and&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T03:29:42Z",
        "updated": "2025-09-02T03:29:42Z",
        "authors": [
            "Ritvik Rastogi"
        ],
        "link": "https://ritvik19.medium.com/papers-explained-444-polaris-d0384cf030c5?source=rss------nlp-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/259b0d603fc1",
        "title": "“Gen AI Essentials: Transformers, Tokens, Embeddings, and Prompting Explained”",
        "abstract": "IntroductionContinue reading on Medium »",
        "published": "2025-09-02T02:18:05Z",
        "updated": "2025-09-02T02:18:05Z",
        "authors": [
            "Likitha  Chendrimada Suguna"
        ],
        "link": "https://medium.com/@likithasuguna2006/gen-ai-essentials-transformers-tokens-embeddings-and-prompting-explained-259b0d603fc1?source=rss------nlp-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/4573315f3c1e",
        "title": "AI in School vs. Industry — A Deep Dive",
        "abstract": "Picture this: you join a company where AI tools are part of the daily workflow&#x200a;&#x2014;&#x200a;drafting emails, summarizing meetings, coding, even&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T01:12:11Z",
        "updated": "2025-09-02T01:12:11Z",
        "authors": [
            "Tiancheng Ethan He"
        ],
        "link": "https://medium.com/@tiancheng.ethan.he/ai-in-school-vs-industry-a-deep-dive-4573315f3c1e?source=rss------nlp-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/6c486fd5a764",
        "title": "09039689679",
        "abstract": "Continue reading on Medium »",
        "published": "2025-09-01T23:29:45Z",
        "updated": "2025-09-01T23:29:45Z",
        "authors": [
            "شماره خاله #شماره خاله# تهران #شماره خاله# اصفهان"
        ],
        "link": "https://medium.com/@knvhhhh4/09039689679-6c486fd5a764?source=rss------nlp-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/547d379e29d9",
        "title": "A New Journey Begin’s: A Coder’s Log",
        "abstract": "Today is September 1st, 2025. I am beginning the first day of my journey. A builder of tomorrow, a strategist of the unseen. This space&#x2026;Continue reading on Medium »",
        "published": "2025-09-01T21:04:38Z",
        "updated": "2025-09-01T21:04:38Z",
        "authors": [
            "Alezandór Coup de Grâce"
        ],
        "link": "https://medium.com/@alezandor/a-new-journey-begins-a-coder-s-log-547d379e29d9?source=rss------nlp-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/b5ba6de30d24",
        "title": "The Skip-Gram Model",
        "abstract": "Learning word embeddings by predicting the context of a wordContinue reading on Medium »",
        "published": "2025-09-01T20:47:25Z",
        "updated": "2025-09-01T20:47:25Z",
        "authors": [
            "Tilo Flasche"
        ],
        "link": "https://medium.com/@tnodecode/the-skip-gram-model-b5ba6de30d24?source=rss------nlp-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/b11c5dde8195",
        "title": "\"This work presents a novel abstract methodology that enables both Arabs and non-Arabic speakers to…",
        "abstract": "Continue reading on Medium »",
        "published": "2025-09-01T18:00:41Z",
        "updated": "2025-09-01T18:00:41Z",
        "authors": [
            "Mae H"
        ],
        "link": "https://medium.com/@mae.h.arabian/this-work-presents-a-novel-abstract-methodology-that-enables-both-arabs-and-non-arabic-speakers-to-b11c5dde8195?source=rss------nlp-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/21bff1bf87fe",
        "title": "Naive Bayes Algorithm in Machine Learning",
        "abstract": "The Naive Bayes algorithm is a simple yet powerful classification technique based on Bayes&#x2019; Theorem with a strong assumption of feature&#x2026;Continue reading on Medium »",
        "published": "2025-09-01T18:00:38Z",
        "updated": "2025-09-01T18:00:38Z",
        "authors": [
            "Vanita AI"
        ],
        "link": "https://medium.com/@vanitaaiofficial/naive-bayes-algorithm-in-machine-learning-21bff1bf87fe?source=rss------nlp-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/b6fb21f6b0ee",
        "title": "What is Ridge Regression?",
        "abstract": "If you&#x2019;re just starting out with machine learning or statistics, you might already know about linear regression, a method used to predict&#x2026;Continue reading on Medium »",
        "published": "2025-09-01T17:59:45Z",
        "updated": "2025-09-01T17:59:45Z",
        "authors": [
            "Vanita AI"
        ],
        "link": "https://medium.com/@vanitaaiofficial/what-is-ridge-regression-b6fb21f6b0ee?source=rss------nlp-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/4ee2db55cbc4",
        "title": "Reimagining Workflows with PixLab Vision Workspace",
        "abstract": "PixLab Vision Workspace is an AI-powered suite designed to transform the way you work with documents. PixLab vision workspace offers a&#x2026;Continue reading on Stackademic »",
        "published": "2025-09-02T06:13:01Z",
        "updated": "2025-09-02T06:13:01Z",
        "authors": [
            "Ekemini Samuel"
        ],
        "link": "https://blog.stackademic.com/reimagining-workflows-with-pixlab-vision-workspace-4ee2db55cbc4?source=rss------computer_vision-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/4f5df4462c3a",
        "title": "Biometric Recognition: From Traditional to Deep Learning",
        "abstract": "Biometric recognition&#x200a;&#x2014;&#x200a;whether it involves face, iris, or fingerprint recognition&#x200a;&#x2014;&#x200a;has long relied on alignment as a critical step&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T04:27:33Z",
        "updated": "2025-09-02T04:27:33Z",
        "authors": [
            "Thuan Bui Huy"
        ],
        "link": "https://medium.com/@testth02/biometric-recognition-from-traditional-to-deep-learning-4f5df4462c3a?source=rss------computer_vision-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/77f8b8b52492",
        "title": "MiniCPM-V 4.5:",
        "abstract": "Run multimodal AI locally&#x200a;&#x2014;&#x200a;no cloud needed.Continue reading on Medium »",
        "published": "2025-09-02T02:15:09Z",
        "updated": "2025-09-02T02:15:09Z",
        "authors": [
            "AdaGao"
        ],
        "link": "https://medium.com/@AdaGaoYY/minicpm-v-4-5-77f8b8b52492?source=rss------computer_vision-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/15a3dcabb3df",
        "title": "Understanding the architecture of vision transformer and its variants: A review",
        "abstract": "The impressive performance of Transformer models in natural language tasks has sparked considerable interest within the computer vision&#x2026;Continue reading on Medium »",
        "published": "2025-09-01T21:46:42Z",
        "updated": "2025-09-01T21:46:42Z",
        "authors": [
            "Neha"
        ],
        "link": "https://medium.com/@ek_min/understanding-the-architecture-of-vision-transformer-and-its-variants-a-review-15a3dcabb3df?source=rss------computer_vision-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/94f7c78ef037",
        "title": "Exploring the MemryX M.2 AI Accelerator: First Impressions & Benchmarks",
        "abstract": "MemryX MX3 M.2 AI accelerator experience. Where this board is excellent, what are the limitsContinue reading on Medium »",
        "published": "2025-09-01T20:57:57Z",
        "updated": "2025-09-01T20:57:57Z",
        "authors": [
            "Anton Maltsev"
        ],
        "link": "https://medium.com/@zlodeibaal/exploring-the-memryx-m-2-ai-accelerator-first-impressions-benchmarks-94f7c78ef037?source=rss------computer_vision-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/107166d3e2b5",
        "title": "YOLOv11 vs YOLOv12 — What’s different (and which should you use)?",
        "abstract": "which should you use&#xa0;??Continue reading on OSINT Team »",
        "published": "2025-09-01T20:34:41Z",
        "updated": "2025-09-01T20:34:41Z",
        "authors": [
            "Mustapha Aitigunaoun"
        ],
        "link": "https://osintteam.blog/yolov11-vs-yolov12-whats-different-and-which-should-you-use-107166d3e2b5?source=rss------computer_vision-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/32f9487eef29",
        "title": "Unitlab AI: A Smarter Way to Handle the Hard Work of Data Labeling",
        "abstract": "It was 2 AM and I was still staring at a dataset of cars. The task sounded simple enough: draw boxes around every vehicle, label them&#x2026;Continue reading on Medium »",
        "published": "2025-09-01T16:36:15Z",
        "updated": "2025-09-01T16:36:15Z",
        "authors": [
            "JOEL BHASKAR NADAR"
        ],
        "link": "https://joelnadarai.medium.com/unitlab-ai-a-smarter-way-to-handle-the-hard-work-of-data-labeling-32f9487eef29?source=rss------computer_vision-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/0bc38e716274",
        "title": "Feature Extraction and Matching: The Foundation of Modern Computer Vision",
        "abstract": "Computer VisionContinue reading on Artificial Intelligence in Plain English »",
        "published": "2025-09-01T15:53:52Z",
        "updated": "2025-09-01T15:53:52Z",
        "authors": [
            "JIN"
        ],
        "link": "https://ai.plainenglish.io/feature-extraction-and-matching-the-foundation-of-modern-computer-vision-0bc38e716274?source=rss------computer_vision-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/768048565efa",
        "title": "Trendjacking Visuals: Detect Brand Cues in Fast-Moving Moments",
        "abstract": "Trendjacking is now visual&#x200a;&#x2014;&#x200a; discover how real-time brand detection helps executives capture cultural moments and protect reputation.Continue reading on Medium »",
        "published": "2025-09-01T15:07:37Z",
        "updated": "2025-09-01T15:07:37Z",
        "authors": [
            "API4AI"
        ],
        "link": "https://medium.com/@API4AI/trendjacking-visuals-detect-brand-cues-in-fast-moving-moments-768048565efa?source=rss------computer_vision-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/411ca1790bf3",
        "title": "Computer Vision Wars: YOLO vs R-CNN — Who Wins in 2025?",
        "abstract": "Continue reading on Medium »",
        "published": "2025-09-01T14:01:42Z",
        "updated": "2025-09-01T14:01:42Z",
        "authors": [
            "The Bot Group"
        ],
        "link": "https://medium.com/@theBotGroup/computer-vision-wars-yolo-vs-r-cnn-who-wins-in-2025-411ca1790bf3?source=rss------computer_vision-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/5914e884de7e",
        "title": "Gemini Report: Kombinasi Pembelajaran Penguatan dan Memori Jangka Panjang",
        "abstract": "1. Pendahuluan: Sebuah Jembatan antara Memori dan KeputusanContinue reading on Medium »",
        "published": "2025-09-02T04:26:40Z",
        "updated": "2025-09-02T04:26:40Z",
        "authors": [
            "Ilham Masykuri Hadi"
        ],
        "link": "https://medium.com/@ilhammasykurihadi/gemini-report-kombinasi-pembelajaran-penguatan-dan-memori-jangka-panjang-5914e884de7e?source=rss------reinforcement_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/a32908a4802e",
        "title": "[Paper Review] Actor-Critic Algorithm",
        "abstract": "Actor-Critic AlgorithmsContinue reading on Medium »",
        "published": "2025-09-02T01:56:43Z",
        "updated": "2025-09-02T01:56:43Z",
        "authors": [
            "Voyager466920"
        ],
        "link": "https://voyager466920.medium.com/paper-review-actor-critic-algorithm-a32908a4802e?source=rss------reinforcement_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/362483a8e2b8",
        "title": "When an LLM Meets SymPy: Training a Baby AI Gauss",
        "abstract": "It&#x2019;s 2030. Imagine the headlines: &#x201c;AI Wins Every Nobel Prize&#x201d;&#x200a;&#x2014;&#x200a;in Physics, Chemistry, Literature, Physiology and Economics, while also&#x2026;Continue reading on Medium »",
        "published": "2025-09-01T15:00:43Z",
        "updated": "2025-09-01T15:00:43Z",
        "authors": [
            "Sean Moran"
        ],
        "link": "https://medium.com/@sean.j.moran/when-an-llm-meets-sympy-training-a-baby-ai-gauss-362483a8e2b8?source=rss------reinforcement_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/10fcaa5ce39a",
        "title": "Unlocking RL: A Friendly Introduction to Reinforcement Learning",
        "abstract": "Computers have achieved superhuman performance in complex games such as Chess and Go by leveraging a branch of artificial intelligence&#x2026;Continue reading on Medium »",
        "published": "2025-09-01T12:12:32Z",
        "updated": "2025-09-01T12:12:32Z",
        "authors": [
            "Om Shah"
        ],
        "link": "https://omshah74.medium.com/unlocking-rl-a-friendly-introduction-to-reinforcement-learning-10fcaa5ce39a?source=rss------reinforcement_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/b1207d3dc836",
        "title": "Geotextile Asphalt Reinforcement Market: Analyzing Key Trends, Industry Drivers, and Growth…",
        "abstract": "In today&#x2019;s world of rapidly expanding infrastructure, durability and sustainability have become top priorities.Continue reading on Medium »",
        "published": "2025-09-01T08:26:33Z",
        "updated": "2025-09-01T08:26:33Z",
        "authors": [
            "Kaverigiri"
        ],
        "link": "https://medium.com/@kaverigiri2002/geotextile-asphalt-reinforcement-market-analyzing-key-trends-industry-drivers-and-growth-b1207d3dc836?source=rss------reinforcement_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/f8a6945018ea",
        "title": "Paper Review: Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image…",
        "abstract": "Pairwise Preference Reward-based GRPO instead of Pointwise, helping stable TtI Reinforcement LearningContinue reading on Medium »",
        "published": "2025-09-01T08:23:44Z",
        "updated": "2025-09-01T08:23:44Z",
        "authors": [
            "Andrew Lukyanenko"
        ],
        "link": "https://artgor.medium.com/paper-review-pref-grpo-pairwise-preference-reward-based-grpo-for-stable-text-to-image-f8a6945018ea?source=rss------reinforcement_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/1a5ba33baf0e",
        "title": "Model-free vs. Model-based Reinforcement Learning",
        "abstract": "Optimal Control vs. PPO on the Inverted Pendulum&#x200a;&#x2014;&#x200a;with Code You Can RunContinue reading on Toward Humanoids »",
        "published": "2025-09-01T04:55:57Z",
        "updated": "2025-09-01T04:55:57Z",
        "authors": [
            "Nikolaus Correll"
        ],
        "link": "https://medium.com/correll-lab/model-free-vs-model-based-reinforcement-learning-1a5ba33baf0e?source=rss------reinforcement_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/d0351b061b9f",
        "title": "Monte Carlo Off-Policy for the Maze Problem",
        "abstract": "Tutorial 8.2: Implementing the Off-Policy MC Method for Our Maze ProblemContinue reading on Towards AI »",
        "published": "2025-09-01T00:02:53Z",
        "updated": "2025-09-01T00:02:53Z",
        "authors": [
            "Rem E"
        ],
        "link": "https://pub.towardsai.net/monte-carlo-off-policy-for-the-maze-problem-d0351b061b9f?source=rss------reinforcement_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/f49786ca083b",
        "title": "Do you know architecture of Recommendation System at Netflix?",
        "abstract": "With the increasing popularity of social media and content that is being uploaded or created has led to the raise in demand for better&#x2026;Continue reading on Medium »",
        "published": "2025-08-31T21:08:57Z",
        "updated": "2025-08-31T21:08:57Z",
        "authors": [
            "Shilpa Thota"
        ],
        "link": "https://shilpathota.medium.com/do-you-know-architecture-of-recommendation-system-at-netflix-f49786ca083b?source=rss------reinforcement_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/c55a6095474b",
        "title": "Self-Searching Minds: How Tsinghua’s Reinforcement Learning Algo Teaches LLMs to Be Their Own…",
        "abstract": "SSRL (Self-Search Reinforcement Learning) shows that large language models can &#x201c;search their own brains&#x201d; to solve information-seeking&#x2026;Continue reading on Data Science in Your Pocket »",
        "published": "2025-08-31T18:14:27Z",
        "updated": "2025-08-31T18:14:27Z",
        "authors": [
            "Sai Dheeraj Gummadi"
        ],
        "link": "https://medium.com/data-science-in-your-pocket/self-searching-minds-how-tsinghuas-ssrl-teaches-llms-to-be-their-own-search-engines-c55a6095474b?source=rss------reinforcement_learning-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/b8f8ecb24558",
        "title": "Stopping Malicious Web Crawlers from Wasting Your Bandwidth with SafeLine WAF",
        "abstract": "1. BackgroundContinue reading on Medium »",
        "published": "2025-09-02T07:19:00Z",
        "updated": "2025-09-02T07:19:00Z",
        "authors": [
            "AerieWhole123"
        ],
        "link": "https://medium.com/@tvvzvpb186/stopping-malicious-web-crawlers-from-wasting-your-bandwidth-with-safeline-waf-b8f8ecb24558?source=rss------cybersecurity-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/3347780c79c5",
        "title": "☁️ Cloud Chaos: How Misconfigured Buckets Spilled Sensitive Data Everywhere",
        "abstract": "Hey there!&#x1f601;Continue reading on Medium »",
        "published": "2025-09-02T07:17:18Z",
        "updated": "2025-09-02T07:17:18Z",
        "authors": [
            "Iski"
        ],
        "link": "https://medium.com/@iski/%EF%B8%8F-cloud-chaos-how-misconfigured-buckets-spilled-sensitive-data-everywhere-3347780c79c5?source=rss------cybersecurity-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/cb9171235b50",
        "title": "Securing AI Agents: Risks, Threat Models, and Defences",
        "abstract": "Artificial intelligence agents are quickly moving from experimental labs into production environments, where they automate tasks&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T07:06:13Z",
        "updated": "2025-09-02T07:06:13Z",
        "authors": [
            "George Zarkadakis"
        ],
        "link": "https://georgezarkadakis.medium.com/securing-ai-agents-risks-threat-models-and-defences-cb9171235b50?source=rss------cybersecurity-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/4cc453849b9c",
        "title": "The CyberDiplomat’s Daily Report",
        "abstract": "2nd September 2025 | Tuesday | Contributor: Dhruv KaushikContinue reading on Medium »",
        "published": "2025-09-02T07:01:10Z",
        "updated": "2025-09-02T07:01:10Z",
        "authors": [
            "The CyberDiplomat LLC"
        ],
        "link": "https://medium.com/@cyberdiplomacy/the-cyberdiplomats-daily-report-4cc453849b9c?source=rss------cybersecurity-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/933f165fd038",
        "title": "Top 10 Cybersecurity Certifications (2025–2026)",
        "abstract": "the fastest-growing and highest-paying career fields. With cyberattacks increasing in frequency and complexity, organizations across the&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:56:50Z",
        "updated": "2025-09-02T06:56:50Z",
        "authors": [
            "Manisha Chaudhary"
        ],
        "link": "https://medium.com/@manishachaudhary_43386/top-10-cybersecurity-certifications-2025-2026-933f165fd038?source=rss------cybersecurity-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/41172adf74ad",
        "title": "Equifax (2017)",
        "abstract": "In September 2017 Equifax&#x200a;&#x2014;&#x200a;one of the three major U.S. credit reporting agencies&#x200a;&#x2014;&#x200a;disclosed a catastrophic data breach that exposed the&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:54:54Z",
        "updated": "2025-09-02T06:54:54Z",
        "authors": [
            "Muhammad Zohaib"
        ],
        "link": "https://medium.com/@muhammad_zohaib/equifax-2017-41172adf74ad?source=rss------cybersecurity-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/0d1a40169721",
        "title": "Day 48- Isolation in Cybersecurity for beginners",
        "abstract": "In today&#x2019;s digital world, protecting data and systems is more important than ever.Continue reading on Medium »",
        "published": "2025-09-02T06:49:51Z",
        "updated": "2025-09-02T06:49:51Z",
        "authors": [
            "unica 02"
        ],
        "link": "https://medium.com/@Sky_higher_freak../day-48-isolation-in-cybersecurity-for-beginners-0d1a40169721?source=rss------cybersecurity-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/cabd6b24bed3",
        "title": "Cyber Threats Are Evolving: Here’s How to Stay Secure in 2025",
        "abstract": "In the past threat are only possible to us thru physical means yet in the rise of technology our views of threats had changed that even&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:35:15Z",
        "updated": "2025-09-02T06:35:15Z",
        "authors": [
            "Sleuth_joe"
        ],
        "link": "https://medium.com/@sleuthjoe7/cyber-threats-are-evolving-heres-how-to-stay-secure-in-2025-cabd6b24bed3?source=rss------cybersecurity-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/9ce0b60a806c",
        "title": "The Nude That Never Existed",
        "abstract": "A close call with sextortion, the long shadow of &#x201c;revenge porn,&#x201d; and how deepfakes turned fear into a business model.Continue reading on Medium »",
        "published": "2025-09-02T06:31:39Z",
        "updated": "2025-09-02T06:31:39Z",
        "authors": [
            "Aditi Jha"
        ],
        "link": "https://medium.com/@aditijhawrites/the-nude-that-never-existed-9ce0b60a806c?source=rss------cybersecurity-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/68d4ea6f0faa",
        "title": "Which is the best IT training and development company in Nagercoil?",
        "abstract": "If you&#x2019;re looking for quality career-focused IT training in Nagercoil, one of the most trusted names is Logos Technologies.Continue reading on Medium »",
        "published": "2025-09-02T06:20:41Z",
        "updated": "2025-09-02T06:20:41Z",
        "authors": [
            "Jewelljerin"
        ],
        "link": "https://medium.com/@jewelljerin/which-is-the-best-it-training-and-development-company-in-nagercoil-68d4ea6f0faa?source=rss------cybersecurity-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/e5b2153d059a",
        "title": "Deploy Hundreds of Projects for Free on Vercel — With Built-In Support for Databases and S3-Style…",
        "abstract": "Your users do not care how clever your stack is. They care that the page loads now.Continue reading on Medium »",
        "published": "2025-09-02T07:13:57Z",
        "updated": "2025-09-02T07:13:57Z",
        "authors": [
            "Mohit"
        ],
        "link": "https://medium.com/@mbodhija80/deploy-hundreds-of-projects-for-free-on-vercel-with-built-in-support-for-databases-and-s3-style-e5b2153d059a?source=rss------software_engineering-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/8cf3e66ebff2",
        "title": "How Deadlocks Occur in Java (And Smart Strategies to Avoid Them)",
        "abstract": "Why every senior developer fears the &#x201c;deadlock monster&#x201d; and how you can tame it.Continue reading on Javarevisited »",
        "published": "2025-09-02T06:58:31Z",
        "updated": "2025-09-02T06:58:31Z",
        "authors": [
            "Pudari Madhavi"
        ],
        "link": "https://medium.com/javarevisited/how-deadlocks-occur-in-java-and-smart-strategies-to-avoid-them-8cf3e66ebff2?source=rss------software_engineering-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/ee1d3f0145dc",
        "title": "Decoding Technical Debt: A Product Manager’s Guide to Speaking Engineer",
        "abstract": "How to understand, prioritize, and manage the invisible force that&#x2019;s slowing down your roadmap.Continue reading on Medium »",
        "published": "2025-09-02T06:53:12Z",
        "updated": "2025-09-02T06:53:12Z",
        "authors": [
            "Khushbu Sharma"
        ],
        "link": "https://medium.com/@khushbusharmabb/decoding-technical-debt-a-product-managers-guide-to-speaking-engineer-ee1d3f0145dc?source=rss------software_engineering-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/fb2e1e4c5359",
        "title": "cnsjcbsiشماره 09374389086خاله #شماره خاله# تهران #شماره خاله# اصفهان #شماره خاله #شیراز #شماره…",
        "abstract": "&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6cc;&#x632;&#x62f; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6af;&#x631;&#x6af;&#x627;&#x646; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x633;&#x646;&#x646;&#x62f;&#x62c; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6a9;&#x631;&#x645;&#x627;&#x646; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x642;&#x632;&#x648;&#x6cc;&#x646; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x627;&#x631;&#x648;&#x645;&#x6cc;&#x647; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6a9;&#x6cc;&#x634; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x642;&#x634;&#x645; &#x634;&#x645;&#x627;&#x631;&#x647;&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:49:10Z",
        "updated": "2025-09-02T06:49:10Z",
        "authors": [
            "شماره خاله #شماره خاله#تهران #شماره خاله#اصفهان\nشم"
        ],
        "link": "https://medium.com/@cnsidjcjxjsjdb/cnsjcbsi%D8%B4%D9%85%D8%A7%D8%B1%D9%87-09374389086%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%AA%D9%87%D8%B1%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%DB%8C%D8%B1%D8%A7%D8%B2-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-fb2e1e4c5359?source=rss------software_engineering-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/a2d3a689e0dd",
        "title": "nxwobcsjشماره 09374389086خاله #شماره خاله# تهران #شماره خاله# اصفهان #شماره خاله #شیراز #شماره…",
        "abstract": "&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;f\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647;&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:48:31Z",
        "updated": "2025-09-02T06:48:31Z",
        "authors": [
            "شماره خاله #شماره خاله#تهران #شماره خاله#اصفهان\nشم"
        ],
        "link": "https://medium.com/@cnsidjcjxjsjdb/nxwobcsj%D8%B4%D9%85%D8%A7%D8%B1%D9%87-09374389086%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%AA%D9%87%D8%B1%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%DB%8C%D8%B1%D8%A7%D8%B2-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-a2d3a689e0dd?source=rss------software_engineering-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/88c97a1f4530",
        "title": "09374389086",
        "abstract": "&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6cc;&#x632;&#x62f; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6af;&#x631;&#x6af;&#x627;&#x646; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x633;&#x646;&#x646;&#x62f;&#x62c; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6a9;&#x631;&#x645;&#x627;&#x646; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x642;&#x632;&#x648;&#x6cc;&#x646; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x627;&#x631;&#x648;&#x645;&#x6cc;&#x647; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x6a9;&#x6cc;&#x634; &#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; &#x642;&#x634;&#x645; &#x634;&#x645;&#x627;&#x631;&#x647;&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:47:51Z",
        "updated": "2025-09-02T06:47:51Z",
        "authors": [
            "شماره خاله #شماره خاله#تهران #شماره خاله#اصفهان\nشم"
        ],
        "link": "https://medium.com/@cnsidjcjxjsjdb/09374389086-88c97a1f4530?source=rss------software_engineering-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/4bac1cc67c4b",
        "title": "7 Things You Must Do Before Writing a Single Line of Code",
        "abstract": "So you&#x2019;re a hotshot Software Engineer in a big corporate, and you&#x2019;ve just been assigned a development task.Continue reading on Medium »",
        "published": "2025-09-02T06:40:54Z",
        "updated": "2025-09-02T06:40:54Z",
        "authors": [
            "Tahreem Iqbal"
        ],
        "link": "https://medium.com/@tahreem_iqbal/7-things-you-must-do-before-writing-a-single-line-of-code-4bac1cc67c4b?source=rss------software_engineering-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/81ec653c15e0",
        "title": "AI Literacy: The New Workplace Currency You Can’t Ignore",
        "abstract": "I was reading an article in The Washington Post that stopped me in my tracks.Continue reading on Medium »",
        "published": "2025-09-02T06:34:09Z",
        "updated": "2025-09-02T06:34:09Z",
        "authors": [
            "Manish Saini"
        ],
        "link": "https://manishsaini74.medium.com/ai-literacy-the-new-workplace-currency-you-cant-ignore-81ec653c15e0?source=rss------software_engineering-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/0216895cc97a",
        "title": "Logs & Troubleshooting — Your Guide to Debugging a Failed Service in Ubuntu",
        "abstract": "Learn how to use journalctl and dmesg to diagnose system issues like a proContinue reading on Medium »",
        "published": "2025-09-02T06:31:36Z",
        "updated": "2025-09-02T06:31:36Z",
        "authors": [
            "Adarsh K"
        ],
        "link": "https://medium.com/@adarsh0011/logs-troubleshooting-your-guide-to-debugging-a-failed-service-in-ubuntu-0216895cc97a?source=rss------software_engineering-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/9e3f725a2706",
        "title": "⚡ Async Programming: Senkron Kodun Ötesinde Güçlü Bir Yaklaşım",
        "abstract": "&#x1f4cc; Giri&#x15f;Continue reading on Medium »",
        "published": "2025-09-02T06:30:29Z",
        "updated": "2025-09-02T06:30:29Z",
        "authors": [
            "Yusuf Emin IRKI"
        ],
        "link": "https://medium.com/@yusufeminirki/async-programming-senkron-kodun-%C3%B6tesinde-g%C3%BC%C3%A7l%C3%BC-bir-yakla%C5%9F%C4%B1m-9e3f725a2706?source=rss------software_engineering-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/2083d2162c8f",
        "title": "DFEmbedder Decoded: Transforming DataFrames into High-Performance Vector Stores",
        "abstract": "Explore how DFEmbedder transforms ordinary DataFrames into powerful vector stores, bridging structured data w modern AI search&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:16:00Z",
        "updated": "2025-09-02T06:16:00Z",
        "authors": [
            "Indrajit"
        ],
        "link": "https://medium.com/@indrajit7448/dfembedder-decoded-transforming-dataframes-into-high-performance-vector-stores-2083d2162c8f?source=rss------data_structures-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/a952c1c5b36e",
        "title": "Intro to Algorithms",
        "abstract": "The Secret Recipe Behind Your Digital World: What is an Algorithm?Continue reading on Medium »",
        "published": "2025-09-02T04:57:00Z",
        "updated": "2025-09-02T04:57:00Z",
        "authors": [
            "Anil Gurindapalli"
        ],
        "link": "https://medium.com/@agurindapalli/intro-to-algorithms-a952c1c5b36e?source=rss------data_structures-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/8ef2c105de97",
        "title": "Understanding Binary Trees in Java",
        "abstract": "Binary trees are one of the most fundamental data structures in computer science and software engineering. They form the basis for&#x2026;Continue reading on softAai Blogs »",
        "published": "2025-09-02T03:02:03Z",
        "updated": "2025-09-02T03:02:03Z",
        "authors": [
            "amol pawar"
        ],
        "link": "https://medium.com/softaai-blogs/understanding-binary-trees-in-java-8ef2c105de97?source=rss------data_structures-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/efc0c0349a14",
        "title": "Mastering Binary Trees in Kotlin: From Basics to Advanced",
        "abstract": "When you start learning data structures in Kotlin, one of the most useful (and surprisingly elegant) structures you&#x2019;ll come across is the&#x2026;Continue reading on softAai Blogs »",
        "published": "2025-09-02T02:49:34Z",
        "updated": "2025-09-02T02:49:34Z",
        "authors": [
            "amol pawar"
        ],
        "link": "https://medium.com/softaai-blogs/mastering-binary-trees-in-kotlin-from-basics-to-advanced-efc0c0349a14?source=rss------data_structures-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/459e1538d582",
        "title": "Insertion Sort in Java Explained: Algorithm, Code & Complexity",
        "abstract": "Sorting is one of the most common operations in computer science, and there are multiple algorithms to get it done.Continue reading on softAai Blogs »",
        "published": "2025-09-02T01:00:29Z",
        "updated": "2025-09-02T01:00:29Z",
        "authors": [
            "amol pawar"
        ],
        "link": "https://medium.com/softaai-blogs/insertion-sort-in-java-explained-algorithm-code-complexity-459e1538d582?source=rss------data_structures-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/d325ddfeaaae",
        "title": "Doubly Linked List in Java Explained: A Beginner’s Guide",
        "abstract": "When working with data structures in Java, choosing the right type of linked list can significantly impact performance and flexibility&#x2026;Continue reading on softAai Blogs »",
        "published": "2025-09-02T00:55:26Z",
        "updated": "2025-09-02T00:55:26Z",
        "authors": [
            "amol pawar"
        ],
        "link": "https://medium.com/softaai-blogs/doubly-linked-list-in-java-explained-a-beginners-guide-d325ddfeaaae?source=rss------data_structures-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/b59c063b93ab",
        "title": "Untangling the Stack: How TypeScript and Dirty Dishes Share the Same Data Structure",
        "abstract": "It&#x2019;s Friday night, and after a hard week, all you want is to play ranked matches in Street Fighter 6 for that dopamine rush from defeating&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T00:39:06Z",
        "updated": "2025-09-02T00:39:06Z",
        "authors": [
            "Ruben O. Alvarado"
        ],
        "link": "https://medium.com/@rubenosmaralvarado/untangling-the-stack-how-typescript-and-dirty-dishes-share-the-same-data-structure-b59c063b93ab?source=rss------data_structures-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/4439236431f4",
        "title": "DSA Lecture 37 : Memory Problem in Queue",
        "abstract": "Let&#x2019;s learn how to make queue efficient, one of the most asked coding interview problemContinue reading on Coding Nexus »",
        "published": "2025-09-01T17:57:24Z",
        "updated": "2025-09-01T17:57:24Z",
        "authors": [
            "Yogi Code"
        ],
        "link": "https://medium.com/coding-nexus/dsa-lecture-37-memory-problem-in-queue-4439236431f4?source=rss------data_structures-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/7a9feed5b363",
        "title": "Articulation Points vs Bridges in Graphs: A Visual & Intuitive Guide",
        "abstract": "Graph theory is full of fascinating little details. Two concepts that often get mixed up are articulation points and bridges. At first&#x2026;Continue reading on Medium »",
        "published": "2025-09-01T17:13:51Z",
        "updated": "2025-09-01T17:13:51Z",
        "authors": [
            "Dipayan Sanyal"
        ],
        "link": "https://medium.com/@dipayansanyal/articulation-points-vs-bridges-in-graphs-a-visual-intuitive-guide-7a9feed5b363?source=rss------data_structures-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/d81529c99884",
        "title": "Why I Stopped Using Lists for Large Data in Python",
        "abstract": "Lists are great&#x200a;&#x2014;&#x200a;until they silently become your biggest bottleneck. Here&#x2019;s what I learned (and what I use instead).Continue reading on Medium »",
        "published": "2025-09-01T15:01:43Z",
        "updated": "2025-09-01T15:01:43Z",
        "authors": [
            "Kaushalsinh"
        ],
        "link": "https://medium.com/@kaushalsinh73/why-i-stopped-using-lists-for-large-data-in-python-d81529c99884?source=rss------data_structures-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/9f719b36668e",
        "title": "HashMap vs HashSet: Concepts, Use Cases, and Code Walkthroughs",
        "abstract": "Learn how to use HashMap and HashSet for solving real-world problems like pair sum, Sudoku validation, and matrix transformations.Continue reading on appkodersolution »",
        "published": "2025-09-02T07:14:09Z",
        "updated": "2025-09-02T07:14:09Z",
        "authors": [
            "Subhobroto Roy"
        ],
        "link": "https://medium.com/appkodersolution/hashmap-vs-hashset-concepts-use-cases-and-code-walkthroughs-9f719b36668e?source=rss------algorithms-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/b70cf4ff8e78",
        "title": "The Algorithm Trap: Why Your Best Content Gets 10 Likes (And Your Worst Goes Viral)",
        "abstract": "We&#x2019;ve All Been HereContinue reading on Medium »",
        "published": "2025-09-02T06:31:36Z",
        "updated": "2025-09-02T06:31:36Z",
        "authors": [
            "Yukti Arora"
        ],
        "link": "https://medium.com/@arorayukti984/the-algorithm-trap-why-your-best-content-gets-10-likes-and-your-worst-goes-viral-b70cf4ff8e78?source=rss------algorithms-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/da11550263da",
        "title": "The Viral Stupidity Paradox: Why Your Brain Loves Bad Ideas!",
        "abstract": "Why That Dance Video Got 2M Views While Your Brilliant Idea Got IgnoredContinue reading on Medium »",
        "published": "2025-09-02T06:05:57Z",
        "updated": "2025-09-02T06:05:57Z",
        "authors": [
            "Simon P"
        ],
        "link": "https://medium.com/@ameritor/the-viral-stupidity-paradox-why-your-brain-loves-bad-ideas-da11550263da?source=rss------algorithms-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/3e6b814d84c9",
        "title": "Automating the Search for Optimal LangChain Agent Architectures with Quality-Diversity Algorithms",
        "abstract": "How I Built Smarter AI Agents That Unlock Trading, E-commerce, and Monetization at ScaleContinue reading on Artificial Intelligence in Plain English »",
        "published": "2025-09-02T06:04:43Z",
        "updated": "2025-09-02T06:04:43Z",
        "authors": [
            "Dustdusky"
        ],
        "link": "https://ai.plainenglish.io/automating-the-search-for-optimal-langchain-agent-architectures-with-quality-diversity-algorithms-3e6b814d84c9?source=rss------algorithms-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/a952c1c5b36e",
        "title": "Intro to Algorithms",
        "abstract": "The Secret Recipe Behind Your Digital World: What is an Algorithm?Continue reading on Medium »",
        "published": "2025-09-02T04:57:00Z",
        "updated": "2025-09-02T04:57:00Z",
        "authors": [
            "Anil Gurindapalli"
        ],
        "link": "https://medium.com/@agurindapalli/intro-to-algorithms-a952c1c5b36e?source=rss------algorithms-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/393a83b17284",
        "title": "The Future of Therapy Includes Our Digital Diet",
        "abstract": "The average therapy session costs between $100-$250, though Medicare and Medicaid can help cover it. Meanwhile, AI chatbots and large&#x2026;Continue reading on Predict »",
        "published": "2025-09-02T03:35:01Z",
        "updated": "2025-09-02T03:35:01Z",
        "authors": [
            "The Parasocial Paradox"
        ],
        "link": "https://medium.com/predict/the-future-of-therapy-includes-our-digital-diet-393a83b17284?source=rss------algorithms-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/8ef2c105de97",
        "title": "Understanding Binary Trees in Java",
        "abstract": "Binary trees are one of the most fundamental data structures in computer science and software engineering. They form the basis for&#x2026;Continue reading on softAai Blogs »",
        "published": "2025-09-02T03:02:03Z",
        "updated": "2025-09-02T03:02:03Z",
        "authors": [
            "amol pawar"
        ],
        "link": "https://medium.com/softaai-blogs/understanding-binary-trees-in-java-8ef2c105de97?source=rss------algorithms-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/5969c02b0625",
        "title": "Solving the Array Product Problem with Prefix/Suffix Accumulation",
        "abstract": "Recently, I was exploring classic algorithm problems and came across an intriguing challenge: given an integer array nums, return an array&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T00:59:49Z",
        "updated": "2025-09-02T00:59:49Z",
        "authors": [
            "Daniel Bacci"
        ],
        "link": "https://medium.com/@danielhdbacci/solving-the-array-product-problem-with-prefix-suffix-accumulation-5969c02b0625?source=rss------algorithms-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/864c648ae40c",
        "title": "Instant Funding Prop Firm and the PMAX Strategy In Forex",
        "abstract": "A trading strategy is a rule-based system for making buy and sell decisions. The PMAX strategy focuses on spotting trends while filtering&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T00:42:49Z",
        "updated": "2025-09-02T00:42:49Z",
        "authors": [
            "Johannbirletrade"
        ],
        "link": "https://medium.com/@johannbirletrade/instant-funding-prop-firm-and-the-pmax-strategy-in-forex-864c648ae40c?source=rss------algorithms-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/b59c063b93ab",
        "title": "Untangling the Stack: How TypeScript and Dirty Dishes Share the Same Data Structure",
        "abstract": "It&#x2019;s Friday night, and after a hard week, all you want is to play ranked matches in Street Fighter 6 for that dopamine rush from defeating&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T00:39:06Z",
        "updated": "2025-09-02T00:39:06Z",
        "authors": [
            "Ruben O. Alvarado"
        ],
        "link": "https://medium.com/@rubenosmaralvarado/untangling-the-stack-how-typescript-and-dirty-dishes-share-the-same-data-structure-b59c063b93ab?source=rss------algorithms-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/8884caccb27f",
        "title": "System Design for Humans: Database Foundations You’ll Actually Get",
        "abstract": "Demystifying databases&#x200a;&#x2014;&#x200a;understand the basics and query languages without the complexity.Continue reading on Medium »",
        "published": "2025-09-02T07:06:01Z",
        "updated": "2025-09-02T07:06:01Z",
        "authors": [
            "Sourabh Varshney"
        ],
        "link": "https://medium.com/@sourabhvarshney111/system-design-for-humans-database-foundations-youll-actually-get-8884caccb27f?source=rss------distributed_systems-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/22d4fadb8d79",
        "title": "How X (Formerly Twitter) Handles Millions of Tweets Every Second",
        "abstract": "X, previously known as Twitter, is one of the world&#x2019;s most influential social platforms, supporting over 611 million monthly users and&#x2026;Continue reading on Stackademic »",
        "published": "2025-09-02T06:20:19Z",
        "updated": "2025-09-02T06:20:19Z",
        "authors": [
            "Satyabrata Mohanty"
        ],
        "link": "https://blog.stackademic.com/how-x-formerly-twitter-handles-millions-of-tweets-every-second-22d4fadb8d79?source=rss------distributed_systems-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/877f4d07c03a",
        "title": "Why Data Platforms Need SOLID Principles",
        "abstract": "The engineering patterns that distinguish scalable data organizations from technical debt graveyardsContinue reading on Medium »",
        "published": "2025-09-02T05:30:45Z",
        "updated": "2025-09-02T05:30:45Z",
        "authors": [
            "KYFEX"
        ],
        "link": "https://medium.com/@kyfex/why-data-platforms-need-solid-principles-877f4d07c03a?source=rss------distributed_systems-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/b2fd0816689a",
        "title": "Static Immutability: How Modern P2P Networks Build Trust Without Servers or Blockchain",
        "abstract": "The False Dichotomy of Digital TrustContinue reading on GenosDB »",
        "published": "2025-09-02T03:32:46Z",
        "updated": "2025-09-02T03:32:46Z",
        "authors": [
            "estebanrfp"
        ],
        "link": "https://genosdb.com/static-immutability-how-modern-p2p-networks-build-trust-without-servers-or-blockchain-b2fd0816689a?source=rss------distributed_systems-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/0f7cd97521f6",
        "title": "MongoDB Architecture: A Complete, Senior Engineer‑Grade Guide",
        "abstract": "A practical deep dive for architects and senior engineers who need to design, scale, and operate MongoDB with confidence.Continue reading on Medium »",
        "published": "2025-09-02T01:57:40Z",
        "updated": "2025-09-02T01:57:40Z",
        "authors": [
            "Anurag Goel"
        ],
        "link": "https://beerus11.medium.com/mongodb-architecture-a-complete-senior-engineer-grade-guide-0f7cd97521f6?source=rss------distributed_systems-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/15022308d3e5",
        "title": "Distributed Systems — Introduction",
        "abstract": "With increasing demand for software products, software application have a huge number of users sometimes in the billions of users. With&#x2026;Continue reading on Medium »",
        "published": "2025-09-01T22:05:06Z",
        "updated": "2025-09-01T22:05:06Z",
        "authors": [
            "Pavan Vadrevu"
        ],
        "link": "https://medium.com/@pavankumarsai18/distributed-systems-introduction-15022308d3e5?source=rss------distributed_systems-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/79d4625d7ba2",
        "title": "Top Amazon SDE-III / Senior SDE Interview Questions",
        "abstract": "This blog curates the most frequently reported and verified questions asked during Amazon SDE-III (Senior SDE) interviews across coding&#x2026;Continue reading on Medium »",
        "published": "2025-09-01T16:34:52Z",
        "updated": "2025-09-01T16:34:52Z",
        "authors": [
            "Anurag Goel"
        ],
        "link": "https://beerus11.medium.com/top-amazon-sde-iii-senior-sde-interview-questions-79d4625d7ba2?source=rss------distributed_systems-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/be72bcaf544c",
        "title": "How to Architect Backend Systems for Edge Computing",
        "abstract": "IntroductionContinue reading on Stackademic »",
        "published": "2025-09-01T15:05:28Z",
        "updated": "2025-09-01T15:05:28Z",
        "authors": [
            "Kisalay"
        ],
        "link": "https://blog.stackademic.com/how-to-architect-backend-systems-for-edge-computing-be72bcaf544c?source=rss------distributed_systems-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/d8ecd2bff825",
        "title": "How to Earn More with Hokkaidu Inu in 2025",
        "abstract": "Optimize your staking strategies for the futureContinue reading on Medium »",
        "published": "2025-09-02T07:17:00Z",
        "updated": "2025-09-02T07:17:00Z",
        "authors": [
            "Hokkaidu Inu"
        ],
        "link": "https://medium.com/@HOKK_STAKING_41965/how-to-earn-more-with-hokkaidu-inu-in-2025-d8ecd2bff825?source=rss------blockchain-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/ac3f9f2da213",
        "title": "How to Earn More with BelaAqua in 2025",
        "abstract": "Optimize your staking strategies for the futureContinue reading on Medium »",
        "published": "2025-09-02T07:15:10Z",
        "updated": "2025-09-02T07:15:10Z",
        "authors": [
            "BelaAqua"
        ],
        "link": "https://medium.com/@AQUA_STAKING_55066/how-to-earn-more-with-belaaqua-in-2025-ac3f9f2da213?source=rss------blockchain-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/ede8d44d5937",
        "title": "Mobile Device Management Using Squid Router",
        "abstract": "## My Squid Proxy and the Great Mobile Device Rebellion (or, How I Learned to Stop Worrying and Love the Packet Sniffer)Continue reading on Medium »",
        "published": "2025-09-02T07:14:39Z",
        "updated": "2025-09-02T07:14:39Z",
        "authors": [
            "Cyra Ven Elen"
        ],
        "link": "https://medium.com/@CyraVenElen/mobile-device-management-using-squid-router-ede8d44d5937?source=rss------blockchain-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/aac5ab74850c",
        "title": "How to Amplify Your Crypto Holdings with Universe Meta",
        "abstract": "Discover innovative ways to increase your UVM rewardsContinue reading on Medium »",
        "published": "2025-09-02T07:12:13Z",
        "updated": "2025-09-02T07:12:13Z",
        "authors": [
            "Leah\n Adams"
        ],
        "link": "https://medium.com/@UVM_STAKING_27118/how-to-amplify-your-crypto-holdings-with-universe-meta-aac5ab74850c?source=rss------blockchain-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/45421c79869c",
        "title": "XDC Network Invests $1 Million in Kasu to Lead Strategic Private Credit Integration",
        "abstract": "September 1, 2025&#x200a;&#x2014;&#x200a;XDC Network, via their VC arm&#x200a;&#x2014;&#x200a;XDC Ventures, has committed $1 million into Kasu Finance as the lead investor in the&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T07:09:02Z",
        "updated": "2025-09-02T07:09:02Z",
        "authors": [
            "Kasu"
        ],
        "link": "https://medium.com/@KasuFinance/xdc-network-invests-1-million-in-kasu-to-lead-strategic-private-credit-integration-45421c79869c?source=rss------blockchain-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/faf91c8943c9",
        "title": "How to Earn More with Thing One in 2025",
        "abstract": "Optimize your staking strategies for the futureContinue reading on Medium »",
        "published": "2025-09-02T07:07:06Z",
        "updated": "2025-09-02T07:07:06Z",
        "authors": [
            "Thing One"
        ],
        "link": "https://medium.com/@THING1_STAKING_73753/how-to-earn-more-with-thing-one-in-2025-faf91c8943c9?source=rss------blockchain-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/3e7e33c3cfae",
        "title": "Building Long-Term Wealth with Georgecoin",
        "abstract": "Secure your future with Georgecoin stakingContinue reading on Medium »",
        "published": "2025-09-02T07:06:58Z",
        "updated": "2025-09-02T07:06:58Z",
        "authors": [
            "Georgecoin"
        ],
        "link": "https://medium.com/@GEORGE_STAKING_44300/building-long-term-wealth-with-georgecoin-3e7e33c3cfae?source=rss------blockchain-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/25c4f5936abe",
        "title": "Building an AI Agent with Rust: From Basic Chat to Blockchain Integration",
        "abstract": "AI agents are moving fast from toy experiments to serious applications. But when I tested different frameworks, both battle-tested and&#x2026;Continue reading on Coinmonks »",
        "published": "2025-09-02T07:06:49Z",
        "updated": "2025-09-02T07:06:49Z",
        "authors": [
            "Yehia Tarek"
        ],
        "link": "https://medium.com/coinmonks/building-an-ai-agent-with-rust-from-basic-chat-to-blockchain-integration-25c4f5936abe?source=rss------blockchain-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/638c6072815e",
        "title": "Squid Router and Open Source Community Contributions",
        "abstract": "## The Unexpected Joy of Squid: How a Reverse Proxy Taught Me About CommunityContinue reading on Medium »",
        "published": "2025-09-02T07:06:42Z",
        "updated": "2025-09-02T07:06:42Z",
        "authors": [
            "Daxon Orien Valen"
        ],
        "link": "https://medium.com/@DaxonOrienValen/squid-router-and-open-source-community-contributions-638c6072815e?source=rss------blockchain-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/059a0e08a331",
        "title": "Revolutionizing Crypto Earnings with StakeWise  GNO",
        "abstract": "Maximize your earnings in the decentralized worldContinue reading on Medium »",
        "published": "2025-09-02T07:06:30Z",
        "updated": "2025-09-02T07:06:30Z",
        "authors": [
            "Tyler\n Flores"
        ],
        "link": "https://medium.com/@SGNO_STAKING_60281/revolutionizing-crypto-earnings-with-stakewise-gno-059a0e08a331?source=rss------blockchain-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/3a76724e1adc",
        "title": "6 Python AI Side Hustles That Quietly Pay Entrepreneurs Every Month",
        "abstract": "Code once. Deploy. Get paid.Continue reading on Venture »",
        "published": "2025-09-02T07:10:46Z",
        "updated": "2025-09-02T07:10:46Z",
        "authors": [
            "Arfa"
        ],
        "link": "https://blog.venturemagazine.net/6-python-ai-side-hustles-that-quietly-pay-entrepreneurs-every-month-3a76724e1adc?source=rss------programming-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/31ac8e8dd2bd",
        "title": "7 Advanced TypeScript Conditional Patterns for Runtime-Safe API Architecture That Eliminate Bugs",
        "abstract": "Continue reading on TechKoala Insights »",
        "published": "2025-09-02T07:07:12Z",
        "updated": "2025-09-02T07:07:12Z",
        "authors": [
            "Aarav Joshi"
        ],
        "link": "https://medium.techkoalainsights.com/7-advanced-typescript-conditional-patterns-for-runtime-safe-api-architecture-that-eliminate-bugs-31ac8e8dd2bd?source=rss------programming-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/5eaf22d01cae",
        "title": "Why Angular 20 Selector-less Components Will Transform Your Development Workflow",
        "abstract": "Discover how Angular 20&#x2019;s selector-less components are revolutionising component architecture and why every developer should master this&#x2026;Continue reading on JavaScript in Plain English »",
        "published": "2025-09-02T07:01:37Z",
        "updated": "2025-09-02T07:01:37Z",
        "authors": [
            "Rajat"
        ],
        "link": "https://javascript.plainenglish.io/why-angular-20-selector-less-components-will-transform-your-development-workflow-5eaf22d01cae?source=rss------programming-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/e4d019031619",
        "title": "The Strange Way Prompts Became My Most Valuable Asset",
        "abstract": "Turns out, my brain-to-text pipeline was a business all along.Continue reading on Stackademic »",
        "published": "2025-09-02T07:00:18Z",
        "updated": "2025-09-02T07:00:18Z",
        "authors": [
            "Arfa"
        ],
        "link": "https://blog.stackademic.com/the-strange-way-prompts-became-my-most-valuable-asset-e4d019031619?source=rss------programming-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/deecea34ccc4",
        "title": "From Compilers to AI: The Great Abstraction Ladder Keeps Climbing",
        "abstract": "We&#x2019;re witnessing the next rung in computing&#x2019;s abstraction ladder, and it&#x2019;s causing the same existential crisis that every previous leap&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:59:03Z",
        "updated": "2025-09-02T06:59:03Z",
        "authors": [
            "Ivo Titscher"
        ],
        "link": "https://medium.com/@it_66472/from-compilers-to-ai-the-great-abstraction-ladder-keeps-climbing-deecea34ccc4?source=rss------programming-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/8cf3e66ebff2",
        "title": "How Deadlocks Occur in Java (And Smart Strategies to Avoid Them)",
        "abstract": "Why every senior developer fears the &#x201c;deadlock monster&#x201d; and how you can tame it.Continue reading on Javarevisited »",
        "published": "2025-09-02T06:58:31Z",
        "updated": "2025-09-02T06:58:31Z",
        "authors": [
            "Pudari Madhavi"
        ],
        "link": "https://medium.com/javarevisited/how-deadlocks-occur-in-java-and-smart-strategies-to-avoid-them-8cf3e66ebff2?source=rss------programming-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/fa1b0b94fa04",
        "title": "Putting Design-To-Code AI Tools to the Test",
        "abstract": "IntroductionContinue reading on Medium »",
        "published": "2025-09-02T06:44:59Z",
        "updated": "2025-09-02T06:44:59Z",
        "authors": [
            "Erez Cohen"
        ],
        "link": "https://medium.com/@erezcohentlv/putting-design-to-code-ai-tools-to-the-test-fa1b0b94fa04?source=rss------programming-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/0b860bdc4a14",
        "title": "⚙️ Mastering TypeScript 5.x: Variadic Tuple Types & Partial Inference ✨",
        "abstract": "TypeScript continues to evolve at a rapid pace, and with every release, developers gain more expressive power and safety guarantees in&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:38:28Z",
        "updated": "2025-09-02T06:38:28Z",
        "authors": [
            "Sofia ✨"
        ],
        "link": "https://medium.com/@Sofia07/%EF%B8%8F-mastering-typescript-5-x-variadic-tuple-types-partial-inference-0b860bdc4a14?source=rss------programming-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/9dff6afd5686",
        "title": "nxwochdbشماره 09374389086خاله #شماره خاله# تهران #شماره خاله# اصفهان #شماره خاله #شیراز #شماره…",
        "abstract": "&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;f\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647;&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:38:26Z",
        "updated": "2025-09-02T06:38:26Z",
        "authors": [
            "#شماره خاله# اهواز #شماره #خاله #تبریز #شماره #خال"
        ],
        "link": "https://medium.com/@mxkscbsjjxbxdh/nxwochdb%D8%B4%D9%85%D8%A7%D8%B1%D9%87-09374389086%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%AA%D9%87%D8%B1%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%DB%8C%D8%B1%D8%A7%D8%B2-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-9dff6afd5686?source=rss------programming-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/eb2730200025",
        "title": "xnwoxhsjشماره 09374389086خاله #شماره خاله# تهران #شماره خاله# اصفهان #شماره خاله #شیراز #شماره…",
        "abstract": "&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x627;&#x635;&#x641;&#x647;&#x627;&#x646;f\n&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647; #&#x634;&#x645;&#x627;&#x631;&#x647; &#x62e;&#x627;&#x644;&#x647;# &#x62a;&#x647;&#x631;&#x627;&#x646; #&#x634;&#x645;&#x627;&#x631;&#x647;&#x2026;Continue reading on Medium »",
        "published": "2025-09-02T06:37:41Z",
        "updated": "2025-09-02T06:37:41Z",
        "authors": [
            "#شماره خاله# اهواز #شماره #خاله #تبریز #شماره #خال"
        ],
        "link": "https://medium.com/@mxkscbsjjxbxdh/xnwoxhsj%D8%B4%D9%85%D8%A7%D8%B1%D9%87-09374389086%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%AA%D9%87%D8%B1%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-%D8%AE%D8%A7%D9%84%D9%87-%D8%B4%DB%8C%D8%B1%D8%A7%D8%B2-%D8%B4%D9%85%D8%A7%D8%B1%D9%87-eb2730200025?source=rss------programming-5",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/2a5047bb66e0",
        "title": "Data Roles, Small Language Models, Knowledge Graphs, and More: Our January Must-Reads",
        "abstract": "The Variable is moving soon—sign up here to ensure you receive all future newsletters.Our prolific authors delivered some excellent work this past month, channeling all the renewed energy and excitement we’ve come to expect from January on TDS. From career advice to core programming and data-processing tasks, our most-read and -shared articles in the past month cover the topics that data professionals care about the most as they plan their next move and aim to expand their skill set.We invite you to explore this month’s must-reads with an open mind: from the ever-shifting terrain of job descriptions to the rise of small language models (alongside large ones), they tackle well-covered areas in data science and machine learning from a fresh, actionable, and pragmatic perspective. Let’s get started.How to Pick Between Data Science, Data Analytics, Data Engineering, ML Engineering, and SW Engineering“When the job titles sound so similar and the roles have a good amount of overlap” it can be difficult to choose the right path for your own interests and priorities as a data practitioner. Marina Wyss - Gratitude Driven’s clear and detailed overview will help you make an informed decision.Your Company Needs Small Language ModelsIs it time to reassess the axiom that in AI, bigger is always better? Sergei Savvov makes a compelling case for the growing footprint of small language models in industry contexts, outlining the ways “they can reduce costs, improve accuracy, and maintain control of your data,” and urges us to stay mindful of these models’ current limitations.The Large Language Model CourseFor anyone whose new year’s resolutions included expanding their knowledge of (and practical experience with) LLMs, Maxime Labonne’s comprehensive course is the one-stop resource you’ll need to get started—it offers a well-structured curriculum that assumes no advanced knowledge, and comes full of recommended articles, tutorials, and tools.Photo by Rima Kruciene on Unsplash5 Simple Projects to Start Today: A Learning Roadmap for Data EngineeringFor all the aspiring data engineers out there, Sarah Lea outlines a realistic, four-month plan that covers all the essentials. Most importantly, it guides you through several project ideas so that you not only grow your theoretical knowledge, but also get to practice the concepts and workflows you’re learning about.How to Build a Knowledge Graph in Minutes (And Make It Enterprise-Ready)Looking to apply a similar hands-on approach to knowledge graphs? After facing major setbacks in the past, Thuwarakesh Murallie shows how you can build one on your own by leveraging the power of LLMs.Top 12 Skills Data Scientists Need to Succeed in 2025“Many things are changing, but other things are not. Understanding which changes require your attention is the key to success.” Benjamin Bodner breaks down the skills that remain essential for data professionals amid the disruptive pressure of AI tools.Deep Dive into Multithreading, Multiprocessing, and AsyncioWhether you’re taking your first steps in Python or are already a seasoned programmer, there’s always more to learn; Clara Chong’s recent post zooms in on concurrency models—approaches to handle multiple tasks simultaneously—and unpacks the stakes of choosing the right one, depending on your project’s needs.How to Run Jupyter Notebooks and Generate HTML Reports with Python ScriptsFor another Python-focused tutorial that foregrounds the ability to streamline tedious workflows with code, don’t miss Amanda Iglesias Moreno’s guide to automating Jupyter Notebook execution and report generation (with a helpful detour through synthetic-data creation).My Experience Switching From Power BI to Looker (as a Senior Data Analyst)It’s often difficult to assess the tradeoffs and gains you’ll be making by adopting a new tool without first trying different options—or hearing firsthand accounts from professionals in a similar situation to yours. Tomas Jancovic (It's AI Thomas) walks us through the journey of moving to Looker for his data analytics workflows, and presents a balanced, concrete account of its pros and cons.Three Important Pandas Functions You Need to KnowNew models and generative-AI apps come and go, but Pandas is still here with us, playing an important role in data scientists’ day-to-day work. Jiayan Yin recently shared a helpful guide for early-stage learners, zooming in on three essential functions that you’ll likely turn to again and again as you process and analyze datasets.Our latest cohort of new authorsEvery month, we’re thrilled to see a fresh group of authors join TDS, each sharing their own unique voice, knowledge, and experience with our community. If you’re looking for new writers to explore and follow, just browse the work of our latest additions from the past couple of months, including Ramsha Ali, Derick Ruiz, Dr. Marcel Müller, Rodrigo M Carrillo Larco, MD, PhD, Ilona Hetsevich, Federico Zabeo, Vladyslav Fliahin, Jérôme DIAZ, Mandeep Kular, Glenn Kong, Vladimir Kukushkin, Viktor Malyi, Ruben Broekx, Iqbal Hamdi, Richa Gadgil, Piotr Gruszecki, Jonathan Fürst, Sirine Bhouri, Kyoosik Kim, Sunghyun Ahn, Afjal Chowdhury, Tim Wibiral, Kunal Santosh Sawant, Aman Agrawal, Abdelkader HASSINE, Florian Trautweiler, Mohammed AbuSadeh, Loic Merckel, Lukasz Gatarek, Zombor Varnagy-Toth, Marc Matterson, Manelle Nouar, Paula LC, Shitanshu Bhushan, Matthew Senick, Lewis James | Data Science, Clara Chong, Bilal Ahmed, Pavel Krautsou, Erol Çıtak, Cristovao Cordeiro, Vladimir Zhyvov, Yuval Gorchover, Zach Flynn, Allon Korem | CEO, Bell Statistics, Tony Albanese, Sandra E.G., Miguel Cardona Polo, James Thorn, Vineet Upadhya, Kaushik Rajan, Mahmoud Abdelaziz, PhD, Benjamin Assel, Shirley Li, Marina Wyss - Gratitude Driven, Michal Davidson, Rémy Garnier, Uladzimir Yancharuk, David Lindelöf, Ricardo Ribas, Hunjae Timothy Lee, Ashley Peacock, Rohit Ramaprasad, Alejandro Alvarez Pérez, David Martin, Ben Tengelsen, César Ortega Quintero, Jaemin Han, Max Surkiz, Massimo Capobianco, Tobias Cabanski, Jimin Kang, Felix Schmidt, Paolo Molignini, PhD, Sayali Kulkarni, Alan Nekhom, and Chris Lettieri, among others.Thank you for supporting the work of our authors! We love publishing articles from new authors, so if you’ve recently written an interesting project walkthrough, tutorial, or theoretical reflection on any of our core topics, don’t hesitate to share it with us.Until the next Variable,TDS TeamData Roles, Small Language Models, Knowledge Graphs, and More: Our January Must-Reads was originally published in TDS Archive on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2025-01-30T14:31:56Z",
        "updated": "2025-01-30T14:31:56Z",
        "authors": [
            "TDS Editors"
        ],
        "link": "https://medium.com/data-science/data-roles-small-language-models-knowledge-graphs-and-more-our-january-must-reads-2a5047bb66e0?source=rss-7e12c71dfa81------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/3e0fa027efe9",
        "title": "Building Successful AI Apps: The Dos and Don’ts",
        "abstract": "As businesses and organizations scramble to find good use cases for AI, several crucial questions consistently emerge: do you even need AI-powered tools? How should you go about building or integrating them into your existing workflows? And how will you know if the effort was worth it?Whether you’re an independent practitioner or part of a larger team trying to make sense of this emerging technology, you’ll find concrete and actionable insights in the lineup of articles we’ve selected this week. They each tackle the nuts and bolts of building AI apps and leveraging their potential for well-defined goals, while avoiding common pain points.While these posts zoom in on specific topics and business problems, they all offer a pragmatic, accessible approach, making them useful for readers across a wide spectrum of backgrounds and experience levels. Let’s dive in.What Did I Learn from Building LLM Applications in 2024? — Part 2So you’ve built a prototype of an LLM-based app that looks promising. What’s next? Satwiki De offers a comprehensive roadmap based on the takeaways she’s accumulated over a year of experimentation—the main thrust being to “develop your AI-enabled application keeping the business objectives in mind.”Why Generative-AI Apps’ Quality Often Sucks and What to Do About itHaving observed countless enterprise AI initiatives deliver underwhelming results (if not fail altogether) Dr. Marcel Müller focuses on the importance of robust assessment, and shows “how we can qualitatively and quantitatively evaluate generative AI applications in the context of concrete business processes.”Designing, Building &amp; Deploying an AI Chat App from Scratch (Part 1)If you’re ready to roll up your sleeves (real or proverbial) and start implementing an AI chat app, Joris Baan’s patient guide is a great resource to help you stay on the right track. Part 1 outlines the microservices architecture and local development needs you’ll want to think about, while part 2 moves on to a detailed discussion of cloud deployment and scaling.Photo by Krišjānis Kazaks on UnsplashLearn to Build Advanced AI Image ApplicationsMoving beyond chatbots, Ida Silfverskiöld explores the possibilities that visual generative-AI tools open up for real-world businesses and professionals—in this case, an interior design app based entirely on open-source models and frameworks.How to Build an AI Agent for Data Analytics Without Writing SQLWhy not harness the power of AI to streamline common data-analytics workflows that would typically require numerous SQL queries? Chengzhi Zhao aims to accomplish just that with the aid of an AI agent, and shows how you can build one yourself with LangChain and DuckDB.AI-Powered Information Extraction and MatchmakingFor another promising use case where an AI app can help you and your team save time and become more efficient, don’t miss Umair Ali Khan’s patient tutorial on building AI-based match-making tools (in this case, helping job seekers find positions that fit their skills and interests).Branching out into the world beyond AI apps, we’ve selected a few more recommended reads we thought you’d enjoy—from a beginner-friendly intro to LLMs to an in-depth analysis of data strategies.If your less tech-savvy colleagues could use a clear and accessible primer on what LLMs are and how they work, just send them Carolina Bento’s top-notch introduction.For a more advanced exploration of LLMs’ inner workings, head right over to Jaemin Han’s fascinating look at models’ shortcomings in generating and interpreting ASCII art—and the surprising security risks the latter might suggest.Quick and focused, Clara Chong’s guide to variable scoping explains why this particular aspect of your code can have far-reaching consequences for your data science workflows.What rules should we have in place to protect humans from unknowingly interacting with AI profiles on social platforms? James Barney unpacks the fallout from the recent controversy around Meta’s (now-suspended) experiment.The rise of code-generating chatbots leads Murtaza Ali to wonder if the age of human-written programming tutorials is coming to an end. (Spoiler alert / sigh of relief: not just yet!)If you’d like to make sense of the gap you observe between between predictions and real-world outcomes, Hennie de Harder’s new post walks us through the basics of prediction probabilities, calibration, and how to interpret these numbers in a practical context.In the mood for a deep dive? Jens Linden, PhD just shared the latest installment in his Demystify Data Strategy series, focusing this time on the common pitfalls of data and AI strategies—and how organizations can avoid them.To round out this week’s lineup, we invite you to learn about MicroPython and its potential relevance for data scientists; Sarah Lea’s concise guide will get you up to speed on all the essential details.Thank you for supporting the work of our authors! As we mentioned above, we love publishing articles from new authors, so if you’ve recently written an interesting project walkthrough, tutorial, or theoretical reflection on any of our core topics, don’t hesitate to share it with us.Until the next Variable,TDS TeamBuilding Successful AI Apps: The Dos and Don’ts was originally published in TDS Archive on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2025-01-23T14:32:22Z",
        "updated": "2025-01-23T14:32:22Z",
        "authors": [
            "TDS Editors"
        ],
        "link": "https://medium.com/data-science/building-successful-ai-apps-the-dos-and-donts-3e0fa027efe9?source=rss-7e12c71dfa81------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/67d71ddf6614",
        "title": "Charts, Dashboards, Maps, and More: Data Visualization in the Spotlight",
        "abstract": "Feeling inspired to write your first TDS post? We’re always open to contributions from new authors.Buzzwords and trends come and go, but the core task of telling compelling stories with data remains one of the main pillars in data scientists’ daily workflow. For practitioners who’d like to up their visualization game, this week we’re highlighting some of our best recent articles on creating powerful, effective, and sleek deliverables.Our selection tackles the topic from multiple angles, so whether you’re interested in chart optimization, geospatial aids, or interactive dashboards, we’re sure you’ll find something here to inspire you and help you expand your current skill set. Happy tinkering!How to Make Extremely Beautiful Charts with PythonWhy settle for “meh…” when “wow!” is within reach? Ari Joury, PhD challenges us to stop settling for mediocre visualizations, shows how a few smart and focused tweaks can transform your average chart into something you can truly be proud of, and offers more than a few concrete examples (and code snippets) to make these suggestions both concrete and actionable.5 Essential Tips to Build Business Dashboards Stakeholders Love“You might be wondering, isn’t dashboarding just creating a bunch of charts? Well, Yes and No.” Yu Dong zooms in on an often-undervalued aspect of data professionals’ work, and explains why it’s time to reevaluate and streamline your approach to dashboard creation.Awesome Plotly with Code Series (Part 8): How to Balance Dominant Bar Chart CategoriesFor the past few months, Jose Parreño’s excellent series has addressed some of the main pain points of designing functional, easy-to-digest charts. His latest installment takes a close, pragmatic look at a common scenario where one category threatens to throw an entire bar chart off-balance.Photo by Wenhao Ruan on UnsplashStep-by-Step Guide for Building Bump Charts in PlotlyReady to move on from bar charts into more advanced and custom formats? Don’t miss Amanda Iglesias Moreno’s Plotly-based tutorial, which introduces a complete workflow for creating a bump chart, a more specialized visualization that is “designed to explore changes in a ranking over time” and allows us to “quickly identify trends and detect elements at the top or bottom of the ranking.”Easy Map Boundary Extraction with GeoPandasWorking with geospatial data can be very rewarding—not to mention essential in many industries—but it can also get tricky and occasionally unwieldy. Lee Vaughan’s latest Python guide brings clarity and practicality to a very common use case: extracting, measuring, and plotting country borders.A new year often brings with it a rush of excellent new writing, and so far 2025 has not disappointed on that front. Here are several recent standouts on a wide range of topics, from hands-on AI projects to the history of GPT models.What would it take for a supervised-learning model to solve a Rubik’s Cube? Daniel Warfield’s latest deep dive walks us through a fun and enlightening project.As more and more AI-generated content floods contemporary media and online spaces, Stephanie Kirmer wonders how we should assess its cultural impact—and navigate its (many) pitfalls.For a solid understanding of cutting-edge LLMs, a trip down memory lane (all the way back to the prehistoric era of 2019–2020) might be a good idea—and Shirley Li’s thorough and accessible retrospective of GPT-2 and GPT-3 offers us precisely that.If you’re in the mood for something more hands-on this week, we highly recommend Maeda Hanafi, PhD’s new guide to customizing your fine-tuning code with Hugging Face’s Transformers library.Are we approaching the point where AI might be able to reliably determine if something is funny? Vineet Upadhya’s latest article reviews recent research into this fascinating topic, including a new framework for computational humor detection.Establishing causality is a common goal for data practitioners, but that doesn’t make it any less difficult to pull off. For his debut TDS post, Rémy Garnier presents a versatile method that you can apply across a wide range of use cases.After sharing a well-received list of AI projects you can experiment with over a weekend, Shaw Talebi fleshes out the details of one of his proposed ideas: an AI-powered resume optimizer.Thank you for supporting the work of our authors! As we mentioned above, we love publishing articles from new authors, so if you’ve recently written an interesting project walkthrough, tutorial, or theoretical reflection on any of our core topics, don’t hesitate to share it with us.Until the next Variable,TDS TeamCharts, Dashboards, Maps, and More: Data Visualization in the Spotlight was originally published in TDS Archive on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2025-01-16T14:31:58Z",
        "updated": "2025-01-16T14:31:58Z",
        "authors": [
            "TDS Editors"
        ],
        "link": "https://medium.com/data-science/charts-dashboards-maps-and-more-data-visualization-in-the-spotlight-67d71ddf6614?source=rss-7e12c71dfa81------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/445f684b01dc",
        "title": "LLM Evaluation, Parallel Computing, Demand Forecasting, and Other Hands-On Data Science Approaches",
        "abstract": "Feeling inspired to write your first TDS post? We’re always open to contributions from new authors.As we all settle into the sometimes hectic rhythm of a new year, we hope you’ve been enjoying the excitement of kicking off projects, learning about new topics, and exploring your next career moves. We’re definitely seeing a flurry of activity among our authors—both longstanding contributors and recent additions—and are thrilled to share all the great work they’ve been cooking up over the holidays.Our lineup of top-notch reads this week has a distinctly actionable, hands-on flavor to it—after all, what better way to harness all this energy than by tinkering with some datasets, models, and code? Whether you’re interested in learning more about cutting-edge evaluation methods or building agentic-AI tools, we’ve got you covered with a diverse selection of tutorials and practical overviews. Ready to dive in?Paradigm Shifts of Eval in the Age of LLMs Is it time to reevaluate the way we approach evaluations? Lili Jiang believes it is: “I’ve come to recognize that LLMs requires some subtle, conceptually simple, yet important changes in the way we think about evaluation.” Her latest article offers high-level insights into what a new paradigm might look like.The Next Frontier in LLM Accuracy Staying thematically close to LLM optimization, Mariya Mansurova’s new deep dive unpacks in great detail several methods we can use to increase models’ accuracy, and zooms in on advanced fine-tuning techniques.Photo by Vishal Banik on UnsplashHow to Build a Graph RAG App Ready to roll up your sleeves and dig deep into some code? Steve Hedden’s thorough tutorial on creating your first graph RAG app is a great option for anyone who’s interested in this trending topic but needs guidance and context to ensure they’re starting off on the right foot.Multi-Agentic RAG with Hugging Face Code Agents Agent-based systems gained enormous steam (and buzz) last year, and it doesn’t seem like that’s about to change in 2025. Curious to learn more about them? Gabriele Sgroi, PhD’s patient, step-by-step guide may be long, but it remains accessible and clear as it outlines the process of leveraging a “small” LLM to power a multi-agentic system—and produce good results, even on consumer-grade hardware.Demand Forecasting with Darts: A TutorialLLMs may be grabbing much of our collective attention these days, but business-focused workflows remain the bread and butter of industry data scientists. Sandra E.G.’s debut TDS article provides a robust, hands-on introduction to one such essential task: demand forecasting in the context of retail sales.Distributed Parallel Computing Made Easy with Ray It’s crucial for data and ML practitioners to experiment with new tools and frameworks, as seemingly small improvements can accumulate into major cost and efficiency benefits. Betty LD walks us through her recent foray into the AI-focused Ray library for distributed data processing, and demonstrates its power through the use case of scalable offline batch inference.If you’re ready to branch out into other topics this week, we’re here to help—whether your interests lie at the intersection of music and AI, quantum computing, or linear algebra (among others), we hope you explore some of these excellent articles:Presenting her team’s cutting-edge research, Tula Masterman explains how we can leverage the hidden state from an intermediate Transformer layer for efficient and robust content safety and prompt injection classification.Music-centered AI tools continue to make strides; Max Hilsdorf devotes his latest exploration of the topic to mono-to-stereo upmixing, a technique for enriching or improving our music-listening experience.With seven first-author publications under her belt, Malak Sadek is well-positioned to offer concrete insights for other researchers who’d like to grow their publishing footprint.Continuing his ongoing series on key linear algebra concepts, Rohit Pandey recently shared a new, comprehensive explainer that looks at the inner workings of orthonormal matrices.Quantum computing has been a hot topic for some time now, though discussions of its promise can sometimes feel almost sci-fi-adjacent. Here to help us make sense of the field and where things stand at the moment is Sara A. Metwalli, whose primer comes right as the UN has declared 2025 the international year of quantum science and technology.While we’re on the topic, let’s round out this week’s selection with Benjamin Assel’s nuanced discussion of error-rate measurement in IBM quantum processors, which also includes code examples, using Qiskit.Thank you for supporting the work of our authors! As we mentioned above, we love publishing articles from new authors, so if you’ve recently written an interesting project walkthrough, tutorial, or theoretical reflection on any of our core topics, don’t hesitate to share it with us.Until the next Variable,TDS TeamLLM Evaluation, Parallel Computing, Demand Forecasting, and Other Hands-On Data Science Approaches was originally published in TDS Archive on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2025-01-09T14:31:38Z",
        "updated": "2025-01-09T14:31:38Z",
        "authors": [
            "TDS Editors"
        ],
        "link": "https://medium.com/data-science/llm-evaluation-parallel-computing-demand-forecasting-and-other-hands-on-data-science-approaches-445f684b01dc?source=rss-7e12c71dfa81------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/1469b3d45348",
        "title": "Start a New Year of Learning on the Right Foot",
        "abstract": "Feeling inspired to write your first TDS post? We’re always open to contributions from new authors.Happy new year! Welcome back to the Variable!The ink has barely dried on our 2024 highlights roundup (it’s never too late to browse it, of course), and here we are, ready to dive headfirst into a fresh year of learning, growth, and exploration.We have a cherished tradition of devoting the first edition of the year to our most inspiring—and accessible—resources for early-stage data science and machine learning professionals (we really do!). We continue it this year with a selection of top-notch recent articles geared at beginner-level learners and job seekers. For the rest of our readers, we’re thrilled to kick things off with a trio of excellent posts from industry veterans who reflect on the current state of data science and AI, and share their opinionated, bold predictions for what the year ahead might look like. Let’s get started!2025: Ready, Set, Go!Top 10 Data &amp; AI Trends for 2025As the co-founder and CEO of Monte Carlo, few people are as well-positioned as Barr Moses to observe industry shifts and emerging trends—so her annual forecast should definitely be on your to-read list.2024 in Review: What I Got Right, Where I Was Wrong, and Bolder Predictions for 2025After a year dominated by RAG, AI agents, and knowledge graphs, what tools and workflows should we keep track of in the coming year? Leonie Monigatti shares a thorough recap of 2024—and a sneak peek at her well-tuned crystal ball for the year to come.Ten Predictions for Data Science and AI in 2025From Jason Tamara Widjaja, here’s another set of insightful reflections: “My predictions for 2025 attempt to provide a view of the tensions of AI, taking an unpopular but balanced view as someone whose work depends not on selling AI, but on implementing AI well — and living through the consequences of our decisions.”Photo by Annie Spratt on UnsplashData science and machine learning, step by step by stepThe Essential Guide to R and Python Libraries for Data VisualizationWith or without AI, charts and plots aren’t going anywhere anytime soon. Sarah Lea maps out the key libraries in which current and aspiring data scientists should gain fluency.Roadmap to Becoming a Data Scientist, Part 2: Software EngineeringProgramming isn’t going anywhere in 2025, either. Vyacheslav Efimov’s guide outlines the coding essentials that will lead you to data science success.Missing Data in Time-Series: Machine Learning TechniquesOne constant trait of real-world data: it’s messy! Learn how to navigate the chaos by following along Sara Nóbrega’s primer on handling missing data.Causality — Mental Hygiene for Data ScienceTaking a few steps back from the more nitty-gritty aspects of data science work, Eyal Kazin’s recent deep dive constitutes a “gentle intro” to the intricate art of detecting, interpreting, and applying causality.Master Machine Learning: 4 Classification Models Made SimpleFor anyone who enjoys structure and clarity above all else, Leo Anello’s (extremely) thorough, 15-step tutorial on classification models would be a perfect starting point from which to expand your ML know-how.2024 Survival Guide for Machine Learning Engineer InterviewsWhether you’re already applying for your first MLE job or contemplating it as one of your goals for the year, don’t miss Mengliu Zhao’s “survival guide,” aimed specifically at junior-level practitioners.Machine Learning Basics I Look for in Data Scientist InterviewsTackling the occasionally opaque hiring process from the other end of the table, Farzad Nobar created a helpful resource to help job applicants zoom in on the topics that really matter to employers.100 Years of (eXplainable) AIBeyond the whats and hows of day-to-day work, there are also the whys: why did this model produce these outputs? Sofya Lipnitskaya’s explainer unpacks the history of AI explainability in the context of the recent rise of LLMs.How to Build a General-Purpose LLM AgentTo end on a more hands-on note—and to satisfy the curiosity of all of you who’ve heard the buzz around AI agents—we highly recommend Maya Murad’s step-by-step guide, which can form “the groundwork for designing your own custom agentic architecture” down the line.Thank you for supporting the work of our authors! As we mentioned above, we love publishing articles from new authors; if contributing to TDS in 2025 is one of your new year’s resolutions—or even if you’ve just recently written an interesting project walkthrough, tutorial, or theoretical reflection on any of our core topics—don’t hesitate to share it with us.Until the next Variable,TDS TeamStart a New Year of Learning on the Right Foot was originally published in TDS Archive on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2025-01-02T14:31:56Z",
        "updated": "2025-01-02T14:31:56Z",
        "authors": [
            "TDS Editors"
        ],
        "link": "https://medium.com/data-science/start-a-new-year-of-learning-on-the-right-foot-1469b3d45348?source=rss-7e12c71dfa81------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/2c0979b4d595",
        "title": "2024 Highlights: The AI and Data Science Articles That Made a Splash",
        "abstract": "Feeling inspired to write your first TDS post before the end of 2024? We’re always open to contributions from new authors.And just like that, 2024 is (almost) in the books. It was a year of exciting transitions — both for the TDS team and, in many meaningful ways, for the data science, machine learning, and AI communities at large. We’d like to thank all of you—readers, authors, and followers—for your support, and for keeping us busy and engaged with your excellent contributions and comments.Unlike in 2023, when a single event (ChatGPT’s launch just weeks before the beginning of the year) stopped everyone in their tracks and shaped conversations for months on end, this year we experienced a more cumulative and fragmented sense of transformation. Practitioners across industry and academia experimented with new tools and worked hard to find innovative ways to benefit from the rapid rise of LLMs; at the same time, they also had to navigate a challenging job market and a world where AI’s footprint inches ever closer to their own everyday workflows.Photo by Oskars Sylwan on UnsplashTo help you make sense of these developments, we published more than 3,500 articles this past year, including hundreds from first-time contributors. Our authors have an incredible knack for injecting their unique perspective into any topic they cover—from big questions and timely topics to more focused technical challenges—and we’re proud of every post we published in 2024.Within this massive creative output, some articles manage to resonate particularly well with our readers, and we’re dedicating our final Variable edition to these: our most-read, -discussed, and -shared posts of the year. As you might expect, they cover a lot of ground, so we’ve decided to arrange them following the major themes we’ve detected this year: learning and building from scratch, RAG and AI agents, career growth, and breakthroughs and innovation.We hope you enjoy exploring our 2024 highlights, and we wish you a relaxing end of the year — see you in January!Learning and Building from ScratchThe most reliably popular type of TDS post is the one that teaches readers how to do or study something interesting and productive on their own, and with minimal prerequisites. This year is no exception—our three most-read articles of 2024 fall under this category.5 AI Projects You Can Build This Weekend (with Python)From beginner-friendly to advanced project ideas, Shaw Talebi demonstrates that anyone can get hands-on with AI.Understanding LLMs from Scratch Using Middle School MathHow do LLMs work? Rohit Patel offered one of the most accessible and engaging explainers you’ll ever find on this topic.How to Learn AI on Your Own (A Self-Study Guide)For the self-starters out there, Thu Vu put together a streamlined roadmap for studying the fundamental building blocks of AI.The Math Behind Neural NetworksTo understand neural networks, “the backbone of modern AI,” Cristian Leo guides us deep into their underlying mathematical principles.Text Embeddings: Comprehensive GuideEmbeddings make the magic of LLMs possible, and Mariya Mansurova’s thorough introduction makes it clear how and why they’ve become so important.How I Studied LLMs in Two Weeks: A Comprehensive RoadmapAnother excellent learning resource came to us from Hesam Sheikh, who walked us through an intensive—but accessible— curriculum to master the basics (and then some) of large language models.RAG and AI AgentsOnce the initial excitement surrounding LLMs settled (a bit), data and ML professionals realized that these powerful models aren’t all that useful out of the box. Retrieval-augmented generation and agentic AI rose to prominence in the past year as the two leading approaches that bridge the gap between the models’ potential and real-world value; they also ended up being our most covered technical topics in recent months.Intro to LLM Agents with LangChain: When RAG Is Not EnoughBack in March—and quite ahead of the curve—Alex Honchar published the definitive beginners’ guide to working with agents.Using LangChain ReAct Agents for Answering Multi-Hop Questions in RAG SystemsShowing us how agents and RAG can complement each other, Dr. Varshita Sher’s tutorial addresses the common need of answering complex queries on internal documents.17 (Advanced) RAG Techniques to Turn Your LLM App Prototype into a Production-Ready SolutionBuilding a rudimentary RAG pipeline is one thing; optimizing it so that it can actually work in a business context is another. Dominik Polzer put together a comprehensive guide to the methods you can leverage to achieving that lofty goal.12 RAG Pain Points and Proposed SolutionsOn a similar troubleshooting beat, Wenqi Glantz outlines a dozen streamlined approaches for tackling some of the most common challenges practitioners face when implementing RAG.Choosing Between LLM Agent FrameworksIt can be tough to make informed choices in an ecosystem where both major and emerging players release new tools every day. Aparna Dhinakaran is here to help with sharp insights on the tradeoffs to keep in mind.Career GrowthData science and machine learning career paths continue to evolve, and the need to adapt to this changing terrain can generate nontrivial amounts of stress for many professionals, whether they’re deep into their career or are just starting out. We love publishing personal reflections on this topic when they also offer readers pragmatic advice—here are four that stood out to us (and to our readers).What 10 Years at Uber, Meta, and Startups Taught Me About Data AnalyticsFrom the importance of storytelling and business acumen to the limitations of metrics, Torsten Walbaum generously consolidated lessons based on a decade of work into actionable insights.What I Learned in my First 3 Months as a Freelance Data ScientistCareer switches are always tricky, and moving away from the structure of working at a company to the world of self-employment comes with its own set of challenges—and, as CJ Sullivan shows, with great opportunities for learning and growth.How I Became A Data Scientist — No CS Degree, No BootcampFor anyone just taking their first steps in the field, Egor Howell’s candid account of his path into data science is a must-read.I Spent $96k To Become a Data Scientist. Here Are 5 Crucial Lessons All Beginners Must KnowOffering a different perspective on entering the discipline, Khouloud El Alami offers practical tips on managing your data science education so that you set yourself on the right path.Breakthroughs and InnovationStaying up-to-date with cutting-edge research and new tools can feel overwhelming at times, which is why we have a particular soft spot for top-notch paper walkthroughs and primers on emerging libraries and models. Here are three such articles that particularly resonated with our audience.A New Coefficient of Correlation“What if you were told there exists a new way to measure the relationship between two variables just like correlation except possibly better”? So starts Tim Sumner’s explainer on a groundbreaking 2020 paper.Intro to DSPy: Goodbye Prompting, Hello Programming!In another exciting year for open-source tools, one of the standout new arrivals was DSPy, which aims to open up LLMs for programmers and make it easier to build modular AI solutions. Leonie Monigatti’s hands-on introduction is the perfect place to start exploring its possibilities.Kolmogorov-Arnold Networks: The Latest Advance in Neural Networks, Simply ExplainedKANs, “promising alternatives of Multi-Layer Perceptrons (MLPs),” made a splashy entrance last spring; Theo Wolf made their ramifications and potential benefits for ML practitioners evident with this accessible primer.Thank you for supporting the work of our authors in 2024! If writing for TDS is one of your goals for 2025, why not get started now? Don’t hesitate to share your work with us.Until the next Variable, coming your way in the first week of January,TDS Team2024 Highlights: The AI and Data Science Articles That Made a Splash was originally published in TDS Archive on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2024-12-19T14:31:47Z",
        "updated": "2024-12-19T14:31:47Z",
        "authors": [
            "TDS Editors"
        ],
        "link": "https://medium.com/data-science/2024-highlights-the-ai-and-data-science-articles-that-made-a-splash-2c0979b4d595?source=rss-7e12c71dfa81------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/40e7793abffb",
        "title": "The Economics of Artificial Intelligence, Causal Tools, ChatGPT’s Impact, and Other Holiday Reads",
        "abstract": "Feeling inspired to write your first TDS post before the end of 2024? We’re always open to contributions from new authors.Our guiding principle is that it’s never a bad time to learn new things, but we also know that different moments call for different types of learning. Here at TDS, we’ve traditionally published lots of hands-on, roll-up-your-sleeves guides and tutorials as soon as we kick off a new year—and we’re sure that will be the case come January 2025, too.For now, as we enter the peak of the holiday season, we wanted to highlight some of our best recent articles that call for a bit more reflection and a slower pace of processing: stories you can savor as you lounge on a comfy armchair, say, rather than while typing code away on your laptop (though you can do that too, of course; we won’t hold it against you!).From the cultural impact of AI-generated content to a Bayesian analysis of dogs’ pooping habits (yes, you’ve read that right), we hope you enjoy this lineup of thought-provoking, engaging articles. And stay tuned: we can’t wait to share our 2024 highlights with you in next week’s final-edition-of-the-year Variable.The Economics of Artificial Intelligence — What Does Automation Mean for Workers? In his comprehensive analysis of AI’s effect on the workforce, Isaac Tham introduces a powerful framework: “AI augments or automates labor based on its performance relative to workers in a given task. If AI is better than labour, labour is automated, but if labour is better than AI, AI augments labour.” He goes on to unpack the stakes, risks, and potential benefits of AI’s rapidly growing footprint.The Cultural Impact of AI Generated Content: Part 1 Business implications take up much of the space in conversations around AI, but as Stephanie Kirmer stresses, we shouldn’t ignore the potentially seismic shifts AI-generated content causes in the cultural sphere, too: “It would be silly to expect our ways of thinking to not change as a result of these experiences, and I worry very much that the change we’re undergoing is not for the better.”ChatGPT: Two Years Later November 2022, when OpenAI launched the chatbot that would change everything (or at least… a lot of things), feels at once like two days and two decades ago. To help us make sense of our post-ChatGPT world, Julián Peller presents a panoramic overview of the past two years, a period of monumental transition within the “generative-AI revolution.”The Name That Broke ChatGPT: Who is David Mayer? For anyone who enjoys their explorations of AI’s inner workings with a generous dose of intrigue and mystery, Cassie Kozyrkov’s latest article fits the bill: it tackles some of the thorniest questions around LLM-based tools (privacy, bias, and prompt hacking, to name a few) through the example of one elusive name.Overcoming Security Challenges in Protecting Shared Generative AI Environments Approaching the problem of security in AI products from a different angle, Han HELOIR, Ph.D. zooms in on the particular challenges of multi-tenancy—the increasingly common situation when different groups of users (like multiple teams within a company) rely on the same data and LLM resources.Photo by Crystal Kay on UnsplashUnderstanding DDPG: The Algorithm That Solves Continuous Action Control Challenges Why not take the time this holiday season to expand your knowledge of deep reinforcement learning algorithms? Sirine Bhouri’s debut TDS article walks us through the theory and architecture behind the Deep Deterministic Policy Gradient (DDPG) algorithm, tests its performance, and examines its potential applications in bioengineering.LLM Routing — Intuitively and Exhaustively ExplainedWith thousands of large language models to choose from, how should practitioners decide which ones to choose for a given task? Daniel Warfield’s accessible deep dive into LLM routing explains how this “advanced inferencing technique” streamlines this process and how the different components it relies on complement each other.The Intuition behind Concordance Index — Survival Analysis Understanding and preventing churn remains one of the most common goals for industry-embedded data scientists. Antonieta Mastrogiuseppe provides a thorough primer on the underlying math of survival analysis, and the key role the concordance index plays in assessing a model’s accuracy.Dog Poop Compass Can a 5-year-old Cavalier King Charles Spaniel teach us important lessons in Bayesian statistics? It turns out the answer is yes — as long as you follow along Dima Sergeev’s gripping account of his attempts to detect patterns in his dog’s “bathroom” rituals.Causality — Mental Hygiene for Data ScienceTo round out our lineup this week, we invite you to dig into Eyal Kazin’s thoughtful reflection on causal tools—and when (and whether) to use them. Based on his recent PyData Global conference lecture, this article balances a big-picture analysis of causal inference with the nitty-gritty factors that shape the ways we apply causal thinking in day-to-day workflows.Thank you for supporting the work of our authors! As we mentioned above, we love publishing articles from new authors, so if you’ve recently written an interesting project walkthrough, tutorial, or theoretical reflection on any of our core topics, don’t hesitate to share it with us.Until the next Variable,TDS TeamThe Economics of Artificial Intelligence, Causal Tools, ChatGPT’s Impact, and Other Holiday Reads was originally published in TDS Archive on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2024-12-12T14:32:14Z",
        "updated": "2024-12-12T14:32:14Z",
        "authors": [
            "TDS Editors"
        ],
        "link": "https://medium.com/data-science/the-economics-of-artificial-intelligence-causal-tools-chatgpts-impact-and-other-holiday-reads-40e7793abffb?source=rss-7e12c71dfa81------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/9d8aac763d10",
        "title": "How to Transition Into Data Science-and Within Data Science",
        "abstract": "How to Transition Into Data Science—and Within Data ScienceFeeling inspired to write your first TDS post? We’re always open to contributions from new authors.With January just around the corner, we’re about to enter prime career-moves season: that exciting time of the year when many data and machine learning professionals assess their career growth and explore new opportunities, and newcomers to the field plan the next steps towards landing their first job. (It’s also when companies tend to ramp up their hiring after the end-of-year lull.)All this energy often comes with nontrivial amounts of uncertainty, stress, and the occasional moment of self-doubt. To help you calmly chart your own path and avoid unnecessary second-guessing (of yourself as well as of hiring teams, colleagues, and others), we put together a special edition of the Variable focused on career transitions for both new and current practitioners.We never miss a chance to celebrate data scientists’ diverse professional and academic backgrounds, and the lineup of articles we’re presenting here reflects that range, too. Whether you’re thinking about a switch to management, are about to jump into your first startup job, or are in the midst of transitioning to data science from a totally different discipline, you’ll find some concrete, experience-based insights to learn from.Rewiring My Career: How I Transitioned from Electrical Engineering to Data EngineeringWhen your goal is to jump across discipline lines, one of the toughest challenges is learning how to translate existing skills and knowledge and make their value apparent to prospective employers. Loizos Loizou’s debut TDS article offers a detailed account of the author’s successful repositioning from a trained electrical engineer to a data engineer—a change that is far more substantive than the title alone suggests.Why STEM Is Important for Any Data ScientistA background in the so-called hard sciences doesn’t always map directly onto data-focused job descriptions. As Radmila M. explains, however, the benefits of applying your hard-earned STEM expertise once you’ve moved on to data science are many — and can manifest themselves in unexpected moments when traditional problem-solving approaches fail to produce the desired outcome.From Data Scientist to Data Manager: My First 3 Months Leading a TeamAfter nearly seven years as a data scientist, Yu Dong took on a new challenge recently and stepped into a management role for the first time. In a thoughtful new post, Yu reflects on “what has changed, what I’ve enjoyed, and what’s been challenging.”Photo by The Nix Company on UnsplashAre You Sure You Want to Become a Data Science Manager?Tackling the management-track conundrum from a different angle, Jose Parreño encourages anyone who’s considering a move away from an individual contributor role to think deeply about their motivations and goals, and to make an informed decision based on a realistic understanding of what becoming a manager actually entails.Roadmap to Becoming a Data Scientist, Part 1: MathsFor aspiring data professionals who are still years away from debating their fit for a manager role, one of the perennial pain points remains the level and amount of math they need to master in order to start their journey on the right foot. Vyacheslav Efimov provides concrete pointers on what you should learn — and how to get started.GenAI is Reshaping Data Science TeamsSetting yourself up for success doesn’t involve a fixed formula; in fields as dynamic as data science and machine learning, the very definition of your role can evolve from one month to the next. This has been especially true in the past couple of years, as generative-AI tools and LLMs have transformed core workflows across industries. Anna Via wrote a focused synthesis of the challenges and opportunities this rapid change presents, and what data teams—and individuals within them—can do to stay nimble and adapt quickly.How to Hire at Early-Stage Startups It may sound counterintuitive that arriving at a new job with advanced educational credentials can sometimes make you less effective, but that’s precisely the point Claudia Ng drives home in her latest article. While she writes with hiring managers in mind, her insights are particularly valuable for data science PhDs who can adjust their mindset accordingly, and prevent potentially mismatched expectations.So It’s Your First Year in AI; Here’s What to ExpectCongratulations: you’ve landed your dream role at a buzzy AI startup. Now what? Based on his own personal experiences, Michael Zakhary seeks to demystify what the job might entail and to “offer a glimpse into the daily life of an ML engineer — whether you’re working in a small, agile team or part of a larger, more structured organization.”Thank you for supporting the work of our authors! As we mentioned above, we love publishing articles from new authors, so if you’ve recently written an interesting project walkthrough, tutorial, or theoretical reflection on any of our core topics, don’t hesitate to share it with us.Until the next Variable,TDS TeamHow to Transition Into Data Science-and Within Data Science was originally published in TDS Archive on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2024-12-05T14:31:01Z",
        "updated": "2024-12-05T14:31:01Z",
        "authors": [
            "TDS Editors"
        ],
        "link": "https://medium.com/data-science/how-to-transition-into-data-science-and-within-data-science-9d8aac763d10?source=rss-7e12c71dfa81------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/a8bafb49f0b7",
        "title": "Autonomous Agent Ecosystems, Data Integration, Open Source LLMs, and Other November Must-Reads",
        "abstract": "Agent Ecosystems, Data Integration, Open Source LLMs, and Other November Must-ReadsFeeling inspired to write your first TDS post? We’re always open to contributions from new authors.Welcome to the penultimate monthly recap of 2024 — could we really be this close to the end of the year?! We’re sure that you, like us, are hard at work tying up loose ends and making a final push on your various projects. We have quite a few of those on our end, and one exciting update we’re thrilled to share with our community already is that TDS is now active on Bluesky. If you’re one of the many recent arrivals to the platform (or have been thinking about taking the plunge), we encourage you to follow our account.What else is on our mind? All the fantastic articles our authors have published in recent weeks, inspiring our readers to learn new skills and explore emerging topics in data science and AI. Our monthly highlights cover a lot of ground—as they usually do—and provide multiple accessible entryways into timely technical topics, from knowledge graphs to RAG evaluation. Let’s dive in.Monthly HighlightsAgentic Mesh: The Future of Generative AI-Enabled Autonomous Agent Ecosystems What will it take for autonomous agents to find each other, collaborate, interact, and transact in a safe, efficient, and trusted fashion? Eric Broda presents his exciting vision for the agentic mesh, a framework that will act as the seamless connecting tissue for AI agents.Building Knowledge Graphs with LLM Graph Transformer For anyone in the mood for a hands-on deep dive, Tomaz Bratanic’s latest technical guide walks us through the nitty-gritty details of LangChain’s implementation of graph construction with LLMs.Why ETL-Zero? Understanding the shift in Data Integration “Instead of requiring the explicit extraction, transformation and loading of data in separate steps, as is traditionally the case, data should flow seamlessly between different systems.” Sarah Lea introduces a novel approach for creating a simplified ETL process with Python.Photo by Jacky Watt on UnsplashEconomics of Hosting Open Source LLMs As LLM usage has skyrocketed in the past year or so, practitioners have increasingly asked themselves what the most efficient way to deploy these models might be. Ida Silfverskiöld offers a detailed breakdown of the various factors to consider and how different providers stack up when it comes to processing time, cold start delays, and CPU, memory, and GPU costs.How I Improved My Productivity as a Data Scientist with Two Small Habits Sometimes, minor changes to your daily routine can have as much of an impact as a total workflow overhaul. Case in point: Philippe Ostiguy, M. Sc.’s new post, where we learn about two seemingly non-work-related habits around rest and mental strength that have given Philippe’s productivity a major boost.A 6-Month Detailed Plan to Build Your Junior Data Science Portfolio Whether you’re a freshly minted data scientist or a more seasoned professional looking for a new role, Sabrine Bendimerad’s blueprint for crafting a successful portfolio will give you concrete ideas and a realistic timeline for getting the job done.How to Reduce Python Runtime for Demanding Tasks Everyone wants their code to run faster, but hitting a plateau is all but inevitable when dealing with particularly heavy workloads. Still, as Jiayan Yin shows in her highly actionable post, there might still be GPU optimization options you haven’t taken advantage of to speed up your Python code.How to Create a RAG Evaluation Dataset From Documents As Dr. Leon Eversberg explains in his recent tutorial, “by uploading PDF files and storing them in a vector database, we can retrieve this knowledge via a vector similarity search and then insert the retrieved text into the LLM prompt as additional context.” The result? A robust approach for evaluating RAG workflows and a reduced chance for hallucinations.Our latest cohort of new authorsEvery month, we’re thrilled to see a fresh group of authors join TDS, each sharing their own unique voice, knowledge, and experience with our community. If you’re looking for new writers to explore and follow, just browse the work of our latest additions, including Jessica S, Tanner McRae, Ed Sandoval, Robert Corwin, Eric Colson, Joseph Ben, Marcus K. Elwin, Ro Isachenko, Michael Zakhary, Haim Barad, Elisa Yao, Mohamad Hamza, Eric Silberstein, Lorenzo Mezzini, David Teather, Diego Penilla, Daniel Klitzke, Iheb Rachdi, Aaron Beckley, Andrea Rosales, Bohumir Buso, Loizos Loizou, Omri Eliyahu Levy, Ohad Eytan, Julián Peller, Yan Georget, James Barney, Dima Sergeev, Pere Martra, and Gizem Kaya, among others.Thank you for supporting the work of our authors! We love publishing articles from new authors, so if you’ve recently written an interesting project walkthrough, tutorial, or theoretical reflection on any of our core topics, don’t hesitate to share it with us.Until the next Variable,TDS TeamAutonomous Agent Ecosystems, Data Integration, Open Source LLMs, and Other November Must-Reads was originally published in TDS Archive on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2024-11-28T14:32:16Z",
        "updated": "2024-11-28T14:32:16Z",
        "authors": [
            "TDS Editors"
        ],
        "link": "https://medium.com/data-science/autonomous-agent-ecosystems-data-integration-open-source-llms-and-other-november-must-reads-a8bafb49f0b7?source=rss-7e12c71dfa81------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/c93d766c86ec",
        "title": "Getting Started with Multimodal AI, One-Hot Encoding, and Other Beginner-Friendly Guides",
        "abstract": "Getting Started with Multimodal AI, CPUs and GPUs, One-Hot Encoding, and Other Beginner-Friendly GuidesFeeling inspired to write your first TDS post? We’re always open to contributions from new authors.Taking the first step towards mastering a new topic is always a bit daunting—sometimes it’s even very daunting! It doesn’t matter if you’re learning about algorithms for the first time, dipping your toes into the exciting world of LLMs, or have just been tasked with revamping your team’s data stack: taking on a challenge with little or no prior experience requires nontrivial amounts of courage and grit.The calm and nuanced perspective of more seasoned practitioners can go a long way, too — which is where our authors excel. This week, we’ve gathered some of our standout recent contributions that are tailored specifically to the needs of early-stage learners attempting to expand their skill set. Let’s roll up our sleeves and get started!From Parallel Computing Principles to Programming for CPU and GPU ArchitecturesFor freshly minted data scientists and ML engineers, few areas are more crucial to understand than memory fundamentals and parallel execution. Shreya Shukla’s thorough and accessible guide is the perfect resource to get a firm footing in this topic, focusing on how to write code for both CPU and GPU architectures to accomplish fundamental tasks like vector-matrix multiplication.Multimodal Models — LLMs That Can See and HearIf you’re feeling confident in your knowledge of LLM basics, why not take the next step and explore multimodal models, which can take in (and in some cases, generate) multiple forms of data—from images to code and audio? Shaw Talebi’s primer, the first part of a new series, offers a solid foundation from which to build your practical know-how.Boosting Algorithms in Machine Learning, Part II: Gradient BoostingWhether you’ve only recently started your ML journey or have been at it for so long that a refresher might be useful, it’s never a bad idea to firm up your knowledge of the basics. Gurjinder Kaur’s ongoing exploration of boosting algorithms is a great case in point, presenting accessible, easy-to-digest breakdowns of some of the most powerful models out there—in this case, gradient boosting.Photo by Taria Camerino on UnsplashNLP Illustrated, Part 1: Text EncodingAnother new project we’re thrilled to share with our readers? Shreya Rao’s just-launched series of illustrated guides to core concepts in natural language processing, the very technology powering many of the fancy chatbots and AI apps that have made a splash in recent years. Part one zooms in on an essential step in just about any NLP workflow: turning textual data into numerical inputs via text encoding.Decoding One-Hot Encoding: A Beginner’s Guide to Categorical DataIf you’re looking to learn about another form of data transformation, don’t miss Vyacheslav Efimov’s clear and concise introduction to one-hot encoding, “one of the most fundamental techniques used for data preprocessing,” turning categorical features into numerical vectors.Excel Spreadsheets Are Dead for Big Data. Companies Need More Python Instead.One type of transition that is often even more difficult than learning a new topic is switching to a new tool or workflow—especially when the one you’re moving away from fits squarely within your comfort zone. As Ari Joury, PhD explains, however, sometimes a temporary sacrifice of speed and ease of use is worth it, as in the case of adopting Python-based data tools instead of Excel spreadsheets.Ready to venture out into other topics and challenges this week? We hope so—we’ve published some excellent articles recently on LLM apps, Python-generated art, AI ethics, and more:After building LLM-based applications this past year, Satwiki De shares practical insights on how the process diverges from traditional product-development norms.In his latest article, Robert Lange focuses on recent advances in neural-network training, and examines various methods of distributed training, such as data-parallel training and gossip-based averaging.Translating data analysis into valuable business decisions remains a perennial challenge for data professionals. Tessa Xie presents a fresh perspective on this problem—as well as several pragmatic recommendations.Anyone in the mood for a math deep dive should head right over to Reza Bagheri’s latest explainer, which walks us through the inner workings of the all-important softmax function.Having been disappointed by the outputs of generative-AI tools, Anna Gordun Peiro attempts to create Mondrian-inspired artwork using nothing but Python, and documents her process in an easy-to-follow tutorial.When you work with time series data, it’s essential to know whether your outlier treatment has been effective. Sara Nóbrega devotes her latest post to a detailed discussion of the various approaches you can use to evaluate the treatment’s impact.What does it take to create AI ethics and governance frameworks that function at scale? Jason Tamara Widjaja unpacks the challenges of bridging common organizational and implementation gaps.Writing at the intersection of music and AI, Jon Flynn walks us through some of the recent developments in this growing field, and zooms in on the Qwen2-Audio model, which is trained to transcribe musical inputs into sheet music.Thank you for supporting the work of our authors! As we mentioned above, we love publishing articles from new authors, so if you’ve recently written an interesting project walkthrough, tutorial, or theoretical reflection on any of our core topics, don’t hesitate to share it with us.Until the next Variable,TDS TeamGetting Started with Multimodal AI, One-Hot Encoding, and Other Beginner-Friendly Guides was originally published in TDS Archive on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2024-11-21T15:14:35Z",
        "updated": "2024-11-21T15:14:35Z",
        "authors": [
            "TDS Editors"
        ],
        "link": "https://medium.com/data-science/getting-started-with-multimodal-ai-one-hot-encoding-and-other-beginner-friendly-guides-c93d766c86ec?source=rss-7e12c71dfa81------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/9b2a78053bec",
        "title": "Remove Paywalls from Search Results",
        "abstract": "“For the curious mind, there is more free content available on the internet than ever, and yet as internet users, we run into paywalls more often than ever before. So we built the FreeInternetPlugin.com to give people the option to choose what type of content is allowed and not allowed in their search results.”Try it out — Install Now Discuss on ProductHuntAnd open-sourced on GithubTry it out — Install Now Discuss on ProductHuntAnd open-sourced on GithubRemove Paywalls from Search Results was originally published in HackerNoon.com on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2021-05-14T13:35:17Z",
        "updated": "2021-05-14T13:35:17Z",
        "authors": [
            "#BlackLivesMatter"
        ],
        "link": "https://medium.com/hackernoon/remove-paywalls-from-search-results-9b2a78053bec?source=rss-4a8a924edf41------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/12d4868f079d",
        "title": "Meet Tech Experts",
        "abstract": "Introducing Hacker Noon ExpertsOn Demand Video Calls with the Smartest People in TechBook a Call With The First Hacker Noon ExpertsDiscussion on ProductHuntRead more Technology Stories on HackerNoon.comMeet Tech Experts was originally published in HackerNoon.com on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2020-12-22T07:29:44Z",
        "updated": "2020-12-22T07:29:44Z",
        "authors": [
            "#BlackLivesMatter"
        ],
        "link": "https://medium.com/hackernoon/meet-tech-experts-12d4868f079d?source=rss-4a8a924edf41------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/ec0ed54e0d9a",
        "title": "Tech News Discussion: Facebook vs. The Government, and Bitcoin Cash Reserves",
        "abstract": "Tune in: Tech News Discussion: Facebook vs. The Government, and Bitcoin Cash ReservesIn this Credder podcast episode, we discuss breaking news around the FTC’s antitrust lawsuit to break up Facebook, the use of bitcoin for startup and big tech cash reserves, social media censorship, the potential to open source news feed algorithms, Section 230, and innovative payment solutions for content creators.Also check out HackerNoon’s recent podcast appearance on the Growth Manifesto:Read more Technology Stories on HackerNoon.com",
        "published": "2020-12-15T20:13:53Z",
        "updated": "2020-12-15T20:13:53Z",
        "authors": [
            "#BlackLivesMatter"
        ],
        "link": "https://medium.com/@hackernoon/tech-news-discussion-facebook-vs-the-government-and-bitcoin-cash-reserves-ec0ed54e0d9a?source=rss-4a8a924edf41------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/77b1e06d55ee",
        "title": "Seven Tech Stories",
        "abstract": "Learning AI If You Suck at Math — Part Eight — The Musician in the MachineThe Long Game in the Space Industry: Why the Need to Transcend Requires Time and EffortCherry-picking Your Teammate’s Locally Uncommitted Changes in GitSaaS IPOs and a Zoominfo S1 mini teardownHow to Perform Emotion detection in Text via PythonFrom Fortnite to Fashion: How Travis Scott Creates Cultural RelevanceHow to Avoid Awkward Virtual EventsRead more Technology Stories on HackerNoon.com.Seven Tech Stories was originally published in HackerNoon.com on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2020-10-07T16:25:24Z",
        "updated": "2020-10-07T16:25:24Z",
        "authors": [
            "#BlackLivesMatter"
        ],
        "link": "https://medium.com/hackernoon/seven-tech-stories-77b1e06d55ee?source=rss-4a8a924edf41------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/80dec6147b26",
        "title": "Ask Me Anything with Brian Wallace, Founder and President of NowSourcing",
        "abstract": "✨ Ask Me Anything with Brian Wallace, Founder and President of NowSourcing, an industry leading infographic design agency based in Louisville, KY and Cincinnati, OH which works with companies that range from startups to Fortune 500s.🌎 Brian also runs #LinkedInLocal events nationwide, and hosts the Next Action Podcast. Brian has been named a Google Small Business Advisor for 2016-present and joined the SXSW Advisory Board in 2019.💎 Ask Him Anything now and join for the answers on September 9th, 10 am EST.https://community.hackernoon.com/t/ask-me-anything-with-brian-wallace-founder-and-president-of-nowsourcing/55516#ama #hackernoon #hackernoonAMA #noonies #nooniestechAsk Me Anything with Brian Wallace, Founder and President of NowSourcing was originally published in HackerNoon.com on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2020-09-08T16:37:31Z",
        "updated": "2020-09-08T16:37:31Z",
        "authors": [
            "#BlackLivesMatter"
        ],
        "link": "https://medium.com/hackernoon/ask-me-anything-with-brian-wallace-founder-and-president-of-nowsourcing-80dec6147b26?source=rss-4a8a924edf41------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/788b1c46ce7d",
        "title": "Five Tech Stories",
        "abstract": "I Built a Mental Health App While Stuck in My Tiny Studio in NYCBitcoin Sharpe Ratio: The Risk And Reward of Investing In CryptocurrenciesPicking the Right Metrics: The Ultimate Guide for Product ManagersThe Sectors Most at Risk from Remote Work and How to Secure ThemHow to Optimize Videos for Youtube’s Machine Learning AlgorithmsRead more Technology Stories on HackerNoon.com.Five Tech Stories was originally published in HackerNoon.com on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2020-09-08T12:11:02Z",
        "updated": "2020-09-08T12:11:02Z",
        "authors": [
            "#BlackLivesMatter"
        ],
        "link": "https://medium.com/hackernoon/five-tech-stories-788b1c46ce7d?source=rss-4a8a924edf41------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/771d06f08eaa",
        "title": "Four Tech Stories",
        "abstract": "“The Abandonment of Clean Algos is the Suicide of Mainstream Social Media” — Minds CEO Bill OttmanHow To Rename Your Git Repositories From “Master” to “Main”Protecting the Japanese Elderly from Scams Using AIThe 4-Hour a Month Crypto Investor: How to Make Money When You’re Short on TimeMore on HackerNoon.comFour Tech Stories was originally published in HackerNoon.com on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2020-09-07T19:01:09Z",
        "updated": "2020-09-07T19:01:09Z",
        "authors": [
            "#BlackLivesMatter"
        ],
        "link": "https://medium.com/hackernoon/four-tech-stories-771d06f08eaa?source=rss-4a8a924edf41------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/69ff663c0819",
        "title": "Emoji Reactions",
        "abstract": "New Inline Emoji Reactions on Hacker NoonThis work was done in collaboration with Mozilla’s Fix the Internet Initiative. Readers now have a low-friction tool to give writers specific feedback on their words. This creates a feedback loop that writers can learn from and improve their content over time.Try out inline emoji reactions by visiting any story on Hacker Noon.P.S. e27 went under the hood, covering how we built it.💚 Emoji Reactions 💡 was originally published in HackerNoon.com on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2020-09-04T10:46:01Z",
        "updated": "2020-09-04T10:46:01Z",
        "authors": [
            "#BlackLivesMatter"
        ],
        "link": "https://medium.com/hackernoon/emoji-reactions-69ff663c0819?source=rss-4a8a924edf41------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/c743fa38dbd0",
        "title": "PAYWALL FREE: Starting Up, Python Updates, and My Approach to Not Understanding",
        "abstract": "Get Published on Hacker Noon.Some Top Tech Stories from Hacker Noon:How I Started My Own Business at 19What Are The New Features in Python 3.9?The State of Infrastructure Security Amidst this PandemicInsights Into the Global Robotics MarketHow to Hack a Huge Career in Tech with PR Expert &amp; Founder Sarah EvansMy Approach To Not UnderstandingFrom Issac Asimov to DecalogueDid You Miss Bitcoin’s Most Revolutionary Feature Too?Learning When and When Not to Leverage AI in Your ProductsThe One Piece Of Advice All Founders Need To HearGet Published on Hacker Noon.PAYWALL FREE: Starting Up, Python Updates, and My Approach to Not Understanding was originally published in HackerNoon.com on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2020-09-02T20:18:04Z",
        "updated": "2020-09-02T20:18:04Z",
        "authors": [
            "#BlackLivesMatter"
        ],
        "link": "https://medium.com/hackernoon/paywall-free-starting-up-python-updates-and-my-approach-to-not-understanding-c743fa38dbd0?source=rss-4a8a924edf41------2",
        "source_category": "medium"
    },
    {
        "id": "https://medium.com/p/ab9739170114",
        "title": "Ask Me Anything with Terence Lee and Joe Kutner of Heroku and Cloud Native Buildpacks, on July…",
        "abstract": "Ask Me Anything with Terence Lee and Joe Kutner of Heroku and Cloud Native Buildpacks, on July 28th, 5 pm ESTTerence Lee and Joe Kutner are founding members of the Cloud Native Buildpacks project. Ask them anything at 5 pm EST on July 28th!In their own words:we are Terence Lee and Joe Kutner, both working on the Platform Engineering team at Salesforce Heroku. We are also founding members of the Cloud Native Buildpacks project, and today we will answer any questions you have around Buildpacks, what they are, use cases, and how we use them here at Salesforce Heroku. If you are new to the topic and want to have some context before the AMA you can take a look at the following resources:Cloud Native Buildpacks 11 WebsiteBuildpacks Go Cloud Native 1Turn Your Code into Docker Images with Cloud Native Buildpacks 3Building Docker images with Cloud Native Buildpacks (Podcast)Intro: Cloud Native Buildpacks — Terence Lee &amp; Joe Kutner, Salesforce Heroku 1 (Video)Thank you for joining us on July 28th, at 2 pm PDT / 5 pm EDT.Join the AMA.Ask Me Anything with Terence Lee and Joe Kutner of Heroku and Cloud Native Buildpacks, on July… was originally published in HackerNoon.com on Medium, where people are continuing the conversation by highlighting and responding to this story.",
        "published": "2020-07-26T11:15:50Z",
        "updated": "2020-07-26T11:15:50Z",
        "authors": [
            "#BlackLivesMatter"
        ],
        "link": "https://medium.com/hackernoon/ask-me-anything-with-terence-lee-and-joe-kutner-of-heroku-and-cloud-native-buildpacks-on-july-ab9739170114?source=rss-4a8a924edf41------2",
        "source_category": "medium"
    }
]